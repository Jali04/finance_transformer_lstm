{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "0815ad64-f59a-4d0c-931c-066a8ffbbf67",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === REPORT GENERATOR ===\n",
                "# 07_report.ipynb\n",
                "#\n",
                "# Funktion:\n",
                "# Dieses Notebook ist der Abschluss der Pipeline. Es aggregiert Ergebnisse aus allen\n",
                "# vorherigen Schritten (Training, Evaluation, Costed Backtest) und erstellt:\n",
                "# 1. Einen menschenlesbaren Markdown-Bericht (REPORT_block7.md).\n",
                "# 2. Ein JSON-File mit allen KPIs für maschinelle Weiterverarbeitung.\n",
                "# 3. Einen Vergleich mit einfachen Benchmarks (LogReg, MACD), um die Leistung einzuordnen.\n",
                "\n",
                "import os, json, yaml, re\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "import numpy as np\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "aabe6ed9-b8f0-4843-9051-3352546c50c2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === HELFER: RUN FINDEN ===\n",
                "# Wir müssen wissen, welches Experiment wir auswerten sollen.\n",
                "# Dazu suchen wir den neuesten Ordner, der zu unserer aktuellen Konfiguration passt.\n",
                "\n",
                "def jread(p: Path):\n",
                "    # Hilfsfunktion zum sicheren Laden von JSON\n",
                "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
                "        return json.load(f)\n",
                "\n",
                "def latest_lstm_run(results_dir: Path,\n",
                "                    lookback: int = None,\n",
                "                    horizon: int = None,\n",
                "                    eps_mode: str = None,\n",
                "                    epsilon: float = None,\n",
                "                    strict: bool = False) -> Path | None:\n",
                "    # Liste aller LSTM-Runs holen, neueste zuerst\n",
                "    runs = sorted(results_dir.glob(\"*_lstm\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
                "    if not runs:\n",
                "        return None\n",
                "\n",
                "    # Filter-Logik: Wir prüfen, ob die Config des Runs zu unseren Anforderungen passt\n",
                "    def matches(run: Path) -> bool:\n",
                "        try:\n",
                "            cfg = jread(run / \"config.json\")\n",
                "        except Exception:\n",
                "            return False # Defekte Config ignorieren\n",
                "            \n",
                "        # Parameter vergleichen (falls gefordert)\n",
                "        ok_lb = (lookback is None) or (int(cfg.get(\"lookback\", -1)) == lookback)\n",
                "        \n",
                "        # Dateinamen parsen, falls Parameter nicht direkt in Config stehen\n",
                "        tc = str(cfg.get(\"train_csv\", \"\"))\n",
                "        mH = re.search(r\"_cls_h(\\d+)_\", tc)\n",
                "        mE = re.search(r\"_(abs|rel)([\\dp]+)\\.csv$\", tc)\n",
                "        \n",
                "        ok_h = True if horizon is None else (int(cfg.get(\"horizon\", -1)) == horizon or (mH and int(mH.group(1)) == horizon))\n",
                "        ok_m = True if eps_mode is None else ((mE and mE.group(1) == eps_mode))\n",
                "        \n",
                "        # Epsilon Check (komplizierter wegen Fließkomma-Vergleich)\n",
                "        ok_e = True\n",
                "        if epsilon is not None:\n",
                "            if mE:\n",
                "                ok_e = float(mE.group(2).replace(\"p\",\".\")) == float(epsilon)\n",
                "            else:\n",
                "                ok_e = float(cfg.get(\"epsilon\", 1e9)) == float(epsilon)\n",
                "                \n",
                "        return ok_lb and ok_h and ok_m and ok_e\n",
                "\n",
                "    # Suchen\n",
                "    matches_list = [r for r in runs if matches(r)]\n",
                "    if matches_list:\n",
                "        return matches_list[0]\n",
                "    \n",
                "    # Wenn strict=False, geben wir zur Not einfach den allerneuesten Run zurück\n",
                "    return None if strict else runs[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "2829a180-c092-4e26-9e97-b2be4b71c411",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === CONFIG LADEN ===\n",
                "ROOT = Path(\"..\").resolve()\n",
                "with open(ROOT / \"config.json\", \"r\") as f:\n",
                "    C = json.load(f)\n",
                "\n",
                "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\")).resolve()\n",
                "LOOKBACK    = int(C[\"lookback\"])\n",
                "FEATURESET  = C.get(\"featureset\", \"v2\")\n",
                "\n",
                "# Wir laden auch die Feature-Definitionen, um Label-Infos zu bekommen\n",
                "HORIZON = MODE = EPS = None\n",
                "yml = ROOT / f\"data/features_{FEATURESET}.yml\"\n",
                "if yml.exists():\n",
                "    meta = yaml.safe_load(open(yml, \"r\")) or {}\n",
                "    lab = meta.get(\"label\", {})\n",
                "    HORIZON = int(lab.get(\"horizon\", 0)) or None\n",
                "    MODE    = str(lab.get(\"mode\", \"\")) or None\n",
                "    EPS     = float(lab.get(\"epsilon\", 0.0)) or None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "b898427f-16bc-44dd-ba5e-5f2abd283aea",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Aktives RUN_DIR: C:\\Users\\jacin\\DL_PROJECT\\finance_transformer_lstm\\LSTM\\results\\2026-01-03_21-02-35_lstm\n"
                    ]
                }
            ],
            "source": [
                "# === RUN-VERZEICHNIS BESTIMMEN ===\n",
                "# Man kann RUN_DIR auch per Environment-Variable erzwingen (für Pipeline-Automation)\n",
                "run_override = os.getenv(\"RUN_DIR\", \"\").strip() or None\n",
                "\n",
                "if run_override:\n",
                "    RUN_DIR = Path(run_override).resolve()\n",
                "else:\n",
                "    RUN_DIR = latest_lstm_run(RESULTS_DIR, lookback=LOOKBACK, horizon=HORIZON, eps_mode=MODE, epsilon=EPS, strict=False)\n",
                "\n",
                "if RUN_DIR is None or not RUN_DIR.exists():\n",
                "    raise SystemExit(\"Kein *_lstm Run gefunden. Bitte Block 3/4/6 vorher einmal ausführen.\")\n",
                "\n",
                "print(\"Aktives RUN_DIR:\", RUN_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "23f9e4fd-dafa-468d-8049-f2437e2e2cb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === ARTEFAKTE EINLESEN ===\n",
                "# Wir laden 'evaluation.json', das in den vorherigen Blocks Stück für Stück gefüllt wurde.\n",
                "ev_path = RUN_DIR / \"evaluation.json\"\n",
                "if not ev_path.exists():\n",
                "    raise SystemExit(f\"evaluation.json fehlt in {RUN_DIR}. Wurden Block 4 und 6 ausgeführt?\")\n",
                "\n",
                "ev    = jread(ev_path)\n",
                "cfg   = ev.get(\"config\", {})\n",
                "metrics = (ev.get(\"metrics\", {}) or {}).get(\"test\", {})\n",
                "thr_sel = ev.get(\"threshold_selection\", {})\n",
                "calib   = ev.get(\"calibration\", {})\n",
                "backtest_gross = ev.get(\"backtest\", {})\n",
                "\n",
                "# Fallback: Label-Parameter, falls oben nicht gefunden\n",
                "if HORIZON is None:\n",
                "    HORIZON = int(((ev.get(\"label_resolved_from\") or {}).get(\"horizon\")) or cfg.get(\"horizon\"))\n",
                "if MODE is None:\n",
                "    MODE = (ev.get(\"label_resolved_from\") or {}).get(\"mode\") or cfg.get(\"epsilon_mode\")\n",
                "if EPS is None:\n",
                "    EPS = float((ev.get(\"label_resolved_from\") or {}).get(\"epsilon\") or cfg.get(\"epsilon\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "0407107f-a03e-4891-856e-c16726338a5a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === KOSTEN-DATEN LADEN ===\n",
                "# Aus Block 6: Sensitivitäts-Ergebnisse\n",
                "sens_path = RUN_DIR / \"cost_sensitivity.csv\"\n",
                "sens_df = pd.read_csv(sens_path) if sens_path.exists() else None\n",
                "\n",
                "MAIN_RT = 15.0  # Unser Standard-Szenario für den Bericht: 15 bps Roundtrip\n",
                "MAIN_SLIP_PER_LEG = 2.0\n",
                "\n",
                "cost_pick = {}\n",
                "if sens_df is not None and not sens_df.empty:\n",
                "    # Wir filtern auf das realistische Modell \"Entry@t+1\"\n",
                "    df_t1_exact = sens_df[sens_df[\"model\"] == \"Entry@t+1\"]\n",
                "    df_t1_prefix = sens_df[sens_df[\"model\"].astype(str).str.startswith(\"Entry@t+1\")]\n",
                "    df_t1 = df_t1_exact if not df_t1_exact.empty else df_t1_prefix\n",
                "\n",
                "    if df_t1.empty:\n",
                "        df_t1 = sens_df.copy() # Falls Filter fehlschlägt, nimm alles\n",
                "\n",
                "    # Wir suchen die Zeile, die den 15bps am nächsten kommt\n",
                "    df_t1[\"rt_diff\"] = (df_t1[\"roundtrip_bps\"] - MAIN_RT).abs()\n",
                "    row = df_t1.sort_values([\"rt_diff\", \"roundtrip_bps\"]).iloc[0].to_dict()\n",
                "\n",
                "    # Struktur für den Report\n",
                "    cost_pick = dict(\n",
                "        model=row[\"model\"], \n",
                "        roundtrip_bps=float(row[\"roundtrip_bps\"]),\n",
                "        trades=int(row.get(\"trades\", 0)), \n",
                "        exposure=float(row.get(\"exposure\", np.nan)),\n",
                "        turnover=float(row.get(\"turnover\", np.nan)),\n",
                "        CAGR=float(row[\"CAGR\"]), \n",
                "        Sharpe=float(row[\"Sharpe\"]), \n",
                "        MaxDD=float(row[\"MaxDD\"]),\n",
                "        final_equity=float(row[\"final_equity\"]),\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "69024c17-9f6c-4017-97de-5b7c461aff18",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === BASELINES BERECHNEN ===\n",
                "# Wir berechnen einfache Benchmarks, um zu sehen, ob das komplexe LSTM überhaupt Mehrwert liefert.\n",
                "# WICHTIG: Das muss auf exakt denselben Daten (Test-Split) passieren!\n",
                "\n",
                "TRAIN_CSV = Path(cfg.get(\"train_csv\", \"\"))\n",
                "features_list = cfg.get(\"features\", None)\n",
                "\n",
                "if not features_list:\n",
                "    # Falls keine Feature-Liste im Config, aus YAML holen\n",
                "    yml = ROOT / f\"data/features_{FEATURESET}.yml\"\n",
                "    if yml.exists():\n",
                "        meta = yaml.safe_load(open(yml, \"r\")) or {}\n",
                "        features_list = meta.get(\"features\", [])\n",
                "\n",
                "# Originaldaten laden und splitten\n",
                "df_all = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
                "X_all = df_all[features_list].copy()\n",
                "y_all = df_all[\"target\"].astype(int).copy()\n",
                "\n",
                "# Zeitlicher Split (muss identisch zu Notebook 3 sein: 70/15/15)\n",
                "n = len(df_all)\n",
                "n_train = int(n * 0.70)\n",
                "n_val   = int(n * 0.15)\n",
                "n_test  = n - n_train - n_val\n",
                "\n",
                "X_train, y_train = X_all.iloc[:n_train],              y_all.iloc[:n_train]\n",
                "X_val,   y_val   = X_all.iloc[n_train:n_train+n_val], y_all.iloc[n_train:n_train+n_val]\n",
                "X_test,  y_test  = X_all.iloc[n_train+n_val:],        y_all.iloc[n_train+n_val:]\n",
                "\n",
                "# Preprocessing und Scaling\n",
                "LB = int(cfg.get(\"lookback\", LOOKBACK))\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import average_precision_score\n",
                "\n",
                "# Scaler nur auf Train fitten!\n",
                "scaler = StandardScaler().fit(X_train)\n",
                "Xtr_s = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
                "Xva_s = pd.DataFrame(scaler.transform(X_val),   index=X_val.index,   columns=X_val.columns)\n",
                "Xte_s = pd.DataFrame(scaler.transform(X_test),  index=X_test.index,  columns=X_test.columns)\n",
                "\n",
                "# Da LSTMs die ersten LB-1 Zeilen verlieren, passen wir den Vergleich hier auch an\n",
                "# (Obwohl LogReg theoretisch alles nutzen könnte, für fairen Vergleich cutten wir)\n",
                "tail = slice(LB-1, None)\n",
                "ytr_tail, yva_tail, yte_tail = y_train.iloc[tail], y_val.iloc[tail], y_test.iloc[tail]\n",
                "Xtr_tail, Xva_tail, Xte_tail = Xtr_s.iloc[tail],   Xva_s.iloc[tail],   Xte_s.iloc[tail]\n",
                "\n",
                "# 1. Baseline: Always-Up / Buy-And-Hold (Rate der positiven Labels)\n",
                "pos_rate_test = float(yte_tail.mean())\n",
                "auprc_always_up = pos_rate_test\n",
                "\n",
                "# 2. Baseline: Logistische Regression (Lineares Referenz-Modell)\n",
                "logit = LogisticRegression(max_iter=200)\n",
                "logit.fit(Xtr_tail, ytr_tail)\n",
                "proba_lr = logit.predict_proba(Xte_tail)[:,1]\n",
                "auprc_lr = float(average_precision_score(yte_tail, proba_lr))\n",
                "\n",
                "# 3. Baseline: Simpler MACD Indikator (falls vorhanden)\n",
                "# Wir testen, ob ein einfacher Indikator besser ist als das komplexe Modell\n",
                "if \"macd_diff\" in df_all.columns:\n",
                "    macd_diff = df_all.loc[Xte_tail.index, \"macd_diff\"].astype(float).fillna(0.0)\n",
                "    auprc_macd = float(average_precision_score(yte_tail, macd_diff.values))\n",
                "else:\n",
                "    auprc_macd = 0.0\n",
                "\n",
                "baselines_tbl = pd.DataFrame([\n",
                "    {\"baseline\": \"Always-Up (Prior)\", \"auprc\": auprc_always_up, \"pos_rate\": pos_rate_test,\n",
                "     \"lift_factor\": 1.0},\n",
                "    {\"baseline\": \"Logistic Regression\", \"auprc\": auprc_lr, \"pos_rate\": pos_rate_test,\n",
                "     \"lift_factor\": (auprc_lr / max(pos_rate_test, 1e-12))},\n",
                "    {\"baseline\": \"Simple MACD\", \"auprc\": auprc_macd, \"pos_rate\": pos_rate_test,\n",
                "     \"lift_factor\": (auprc_macd / max(pos_rate_test, 1e-12))}\n",
                "])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "57a3d6ad-a1c5-4a7e-83d1-bf5de33dc062",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === REPORT DATEN ZUSAMMENFÜHREN ===\n",
                "# Wir bauen ein großes Dictionary 'kpis', in dem alle wichtigen Infos gesammelt sind.\n",
                "\n",
                "kpis = {\n",
                "    \"run_dir\": str(RUN_DIR),\n",
                "    \"generated_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
                "    \"data\": {\n",
                "        \"ticker\": cfg.get(\"ticker\"),\n",
                "        \"interval\": cfg.get(\"interval\"),\n",
                "        \"period\": [cfg.get(\"start\"), cfg.get(\"end\")],\n",
                "        \"horizon\": cfg.get(\"horizon\"),\n",
                "        \"lookback\": cfg.get(\"lookback\"),\n",
                "        \"featureset\": cfg.get(\"featureset\"),\n",
                "        \"features_used\": ev.get(\"features_used\"),\n",
                "    },\n",
                "    \"label\": ev.get(\"label_resolved_from\"),\n",
                "    \"calibration\": {\n",
                "        \"chosen\": calib.get(\"chosen\"),\n",
                "        \"val_brier\": calib.get(\"val_brier\"),\n",
                "        \"test_brier\": calib.get(\"test_brier\"),\n",
                "        \"note\": \"Kalibrierung auf Validation Set, Test Set Werte nur zur Info.\"\n",
                "    },\n",
                "    \"threshold\": {\n",
                "        \"strategy\": thr_sel.get(\"strategy\"),\n",
                "        \"threshold\": thr_sel.get(\"threshold\"),\n",
                "        \"val_mcc\": thr_sel.get(\"val_mcc\"),\n",
                "        \"test_pred_pos_rate\": thr_sel.get(\"test_pred_pos_rate\"),\n",
                "    },\n",
                "    \"classification_test\": {\n",
                "        \"roc_auc\": metrics.get(\"roc_auc\"),\n",
                "        \"auprc\": metrics.get(\"auprc\"),\n",
                "        \"brier\": metrics.get(\"brier\"),\n",
                "        \"balanced_accuracy\": metrics.get(\"balanced_accuracy\"),\n",
                "        \"mcc\": metrics.get(\"mcc\"),\n",
                "    },\n",
                "    \"backtest_gross\": backtest_gross,     # Backtest Ergebniss ohne Kosten\n",
                "    \"backtest_cost_pick\": cost_pick,      # Ergebnisse mit Kosten (Realistisch)\n",
                "    \"baselines\": baselines_tbl.to_dict(orient=\"records\")\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "40e11499-04f4-45e6-9659-437cd867ed45",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ REPORT geschrieben: C:\\Users\\jacin\\DL_PROJECT\\finance_transformer_lstm\\LSTM\\results\\2026-01-03_21-02-35_lstm\\REPORT_block7.md\n"
                    ]
                }
            ],
            "source": [
                "# === REPORT GENERIEREN (MARKDOWN) ===\n",
                "# Wir schreiben die gesammelten Daten in eine sauber formatierte Markdown-Datei.\n",
                "\n",
                "fig_dir = RUN_DIR / \"figures\"\n",
                "# Dateipfade zu den Bildern, die wir einbinden wollen\n",
                "figs = {\n",
                "    \"roc\": fig_dir / \"roc_test.png\",\n",
                "    \"pr\":  fig_dir / \"pr_test.png\",\n",
                "    \"equity_cost\":  fig_dir / \"equity_costed.png\",\n",
                "}\n",
                "\n",
                "def _rel(p: Path) -> str:\n",
                "    # Hilfsfunktion für relative Pfade (damit der Report portabel ist)\n",
                "    return str(p.relative_to(RUN_DIR)) if p.exists() else str(p)\n",
                "\n",
                "report_md = RUN_DIR / \"REPORT_block7.md\"\n",
                "lines = []\n",
                "\n",
                "# Header\n",
                "lines.append(f\"# Block 7 – Abschluss-Report\\n\")\n",
                "lines.append(f\"- **Generiert:** {kpis['generated_utc']}\")\n",
                "lines.append(f\"- **Run:** `{RUN_DIR.name}`\")\n",
                "lines.append(f\"- **Modell-Setup:** Ticker `{kpis['data']['ticker']}` | Horizon `{kpis['data']['horizon']}` | Lookback `{kpis['data']['lookback']}`\")\n",
                "lines.append(\"---\")\n",
                "\n",
                "# Klassifikations-Metrics\n",
                "lines.append(\"## 1. Klassifikations-Leistung (Test Set)\")\n",
                "m = kpis[\"classification_test\"]\n",
                "lines.append(f\"Wir bewerten, wie gut das Modell Wahrscheinlichkeiten vorhersagt.\")\n",
                "lines.append(f\"- **AUROC**: `{m['roc_auc']:.3f}` (Fläche unter ROC Kurve, 0.5 = Zufall)\")\n",
                "lines.append(f\"- **AUPRC**: `{m['auprc']:.3f}` (Fläche unter Precision-Recall Kurve, Basisrate ≈ {kpis['baselines'][0]['pos_rate']:.2f})\")\n",
                "lines.append(f\"- **MCC**: `{m['mcc']:.3f}` (Matthews Correlation Coefficient)\")\n",
                "\n",
                "lines.append(f\"\\n### Grafiken\\n\")\n",
                "lines.append(f\"![ROC]({_rel(figs['roc'])})  \\n![PR]({_rel(figs['pr'])})\\n\")\n",
                "\n",
                "# Backtests\n",
                "lines.append(\"## 2. Finanzielle Performance (Backtest)\")\n",
                "cp = kpis[\"backtest_cost_pick\"]\n",
                "if cp:\n",
                "    lines.append(f\"Simuliertes Handelsergebnis mit **{cp['roundtrip_bps']}** bps Roundtrip-Kosten und T+1 Entry:\")\n",
                "    lines.append(f\"- **CAGR**: `{cp['CAGR']:.2%}` (Jährliche Rendite)\")\n",
                "    lines.append(f\"- **Sharpe Ratio**: `{cp['Sharpe']:.2f}` (Risikoadjustierte Rendite)\")\n",
                "    lines.append(f\"- **Max Drawdown**: `{cp['MaxDD']:.2%}` (Maximaler zwischenzeitlicher Verlust)\")\n",
                "    lines.append(f\"- **Endkapital**: `{cp['final_equity']:.2f}` (Start = 1.00)\")\n",
                "    lines.append(f\"\\n![Equity Trace]({_rel(figs['equity_cost'])})\\n\")\n",
                "else:\n",
                "    lines.append(\"*(Keine Kostendaten gefunden)*\\n\")\n",
                "\n",
                "# Baselines\n",
                "lines.append(\"## 3. Vergleich mit Benchmarks\")\n",
                "lines.append(\"Ist das Modell besser als simple Methoden? (Lift Factor > 1.0)\")\n",
                "lines.append(\"| Baseline | AUPRC | Lift Factor |\")\n",
                "lines.append(\"|---|---|---|\")\n",
                "for b in kpis[\"baselines\"]:\n",
                "    lines.append(f\"| {b['baseline']} | {b['auprc']:.3f} | {b['lift_factor']:.2f}x |\")\n",
                "\n",
                "lines.append(\"\\n---\")\n",
                "lines.append(\"**Ende des Berichts.**\")\n",
                "\n",
                "# Datei schreiben\n",
                "report_md.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
                "print(\"✓ REPORT geschrieben:\", report_md)\n",
                "\n",
                "# JSON Dump für spätere Analyse\n",
                "(RUN_DIR / \"REPORT_block7_kpis.json\").write_text(json.dumps(kpis, indent=2), encoding=\"utf-8\")\n",
                "\n",
                "# CSV Zeile für Sammel-Auswertungen\n",
                "try:\n",
                "    kpi_rows = {\n",
                "        \"roc_auc\": metrics.get(\"roc_auc\"), \n",
                "        \"auprc\": metrics.get(\"auprc\"), \n",
                "        \"mcc\": metrics.get(\"mcc\"),\n",
                "        \"cost_CAGR\": cp.get(\"CAGR\") if cp else None,\n",
                "        \"cost_Sharpe\": cp.get(\"Sharpe\") if cp else None,\n",
                "    }\n",
                "    pd.DataFrame([kpi_rows]).to_csv(RUN_DIR / \"kpis_block7.csv\", index=False)\n",
                "except Exception as e:\n",
                "    print(\"Fehler beim CSV-Export:\", e)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
