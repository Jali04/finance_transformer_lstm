{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0815ad64-f59a-4d0c-931c-066a8ffbbf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === REPORT GENERATOR ===\n",
    "# 07_report.ipynb\n",
    "#\n",
    "# Funktion:\n",
    "# Dieses Notebook ist der Abschluss der Pipeline. Es aggregiert Ergebnisse aus allen\n",
    "# vorherigen Schritten (Training, Evaluation, Costed Backtest) und erstellt:\n",
    "# 1. Einen menschenlesbaren Markdown-Bericht (REPORT_block7.md).\n",
    "# 2. Ein JSON-File mit allen KPIs f\u00fcr maschinelle Weiterverarbeitung (z.B. Dashboards).\n",
    "# 3. Einen Vergleich mit einfachen Benchmarks (LogReg, MACD), um die Leistung einzuordnen.\n",
    "\n",
    "import os, json, yaml, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabe6ed9-b8f0-4843-9051-3352546c50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HELFER: RUN FINDEN ===\n",
    "# Wir m\u00fcssen wissen, welches Experiment wir auswerten sollen.\n",
    "# Dazu suchen wir den neuesten Ordner, der zu unserer aktuellen Konfiguration passt.\n",
    "\n",
    "def jread(p: Path):\n",
    "    # Hilfsfunktion zum sicheren Laden von JSON\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def latest_lstm_run(results_dir: Path,\n",
    "                    lookback: int = None,\n",
    "                    horizon: int = None,\n",
    "                    eps_mode: str = None,\n",
    "                    epsilon: float = None,\n",
    "                    strict: bool = False) -> Path | None:\n",
    "    # Liste aller LSTM-Runs holen, neueste zuerst (reverse sortiert nach mtime)\n",
    "    runs = sorted(results_dir.glob(\"*_lstm\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not runs:\n",
    "        return None\n",
    "\n",
    "    # Filter-Logik: Wir pr\u00fcfen, ob die Config des Runs zu unseren Anforderungen passt\n",
    "    def matches(run: Path) -> bool:\n",
    "        try:\n",
    "            cfg = jread(run / \"config.json\")\n",
    "        except Exception:\n",
    "            return False # Defekte Config ignorieren\n",
    "            \n",
    "        # Parameter vergleichen (falls gefordert)\n",
    "        ok_lb = (lookback is None) or (int(cfg.get(\"lookback\", -1)) == lookback)\n",
    "        \n",
    "        # Dateinamen der Trainings-CSV parsen, da Parameter oft dort codiert sind\n",
    "        tc = str(cfg.get(\"train_csv\", \"\"))\n",
    "        mH = re.search(r\"_cls_h(\\d+)_\", tc)\n",
    "        mE = re.search(r\"_(abs|rel)([\\dp]+)\\.csv$\", tc)\n",
    "        \n",
    "        # Horizont pr\u00fcfen\n",
    "        ok_h = True if horizon is None else (int(cfg.get(\"horizon\", -1)) == horizon or (mH and int(mH.group(1)) == horizon))\n",
    "        \n",
    "        # Mode pr\u00fcfen\n",
    "        ok_m = True if eps_mode is None else ((mE and mE.group(1) == eps_mode))\n",
    "        \n",
    "        # Epsilon Check (komplizierter wegen Flie\u00dfkomma-Vergleich und 'p' statt '.')\n",
    "        ok_e = True\n",
    "        if epsilon is not None:\n",
    "            if mE:\n",
    "                # Epsilon aus Dateinamen extrahieren (z.B. 0p0005 -> 0.0005)\n",
    "                ok_e = float(mE.group(2).replace(\"p\",\".\")) == float(epsilon)\n",
    "            else:\n",
    "                ok_e = float(cfg.get(\"epsilon\", 1e9)) == float(epsilon)\n",
    "                \n",
    "        return ok_lb and ok_h and ok_m and ok_e\n",
    "\n",
    "    # Runs durchsuchen\n",
    "    matches_list = [r for r in runs if matches(r)]\n",
    "    if matches_list:\n",
    "        return matches_list[0]\n",
    "    \n",
    "    # Wenn strict=False, geben wir zur Not einfach den allerneuesten Run zur\u00fcck,\n",
    "    # auch wenn er nicht perfekt passt (Fallback).\n",
    "    return None if strict else runs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2829a180-c092-4e26-9e97-b2be4b71c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG LADEN ===\n",
    "# Wir laden die zentrale Projekt-Konfiguration.\n",
    "ROOT = Path(\"..\").resolve()\n",
    "with open(ROOT / \"config.json\", \"r\") as f:\n",
    "    C = json.load(f)\n",
    "\n",
    "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\")).resolve()\n",
    "LOOKBACK    = int(C[\"lookback\"])\n",
    "FEATURESET  = C.get(\"featureset\", \"v2\")\n",
    "\n",
    "# Wir laden auch die Feature-Definitionen (YAML), um Label-Infos (Horizont, Epsilon) zu bekommen.\n",
    "# Diese sind wichtig, um den richtigen Run zu finden.\n",
    "HORIZON = MODE = EPS = None\n",
    "yml = ROOT / f\"data/features_{FEATURESET}.yml\"\n",
    "if yml.exists():\n",
    "    meta = yaml.safe_load(open(yml, \"r\")) or {}\n",
    "    lab = meta.get(\"label\", {})\n",
    "    HORIZON = int(lab.get(\"horizon\", 0)) or None\n",
    "    MODE    = str(lab.get(\"mode\", \"\")) or None\n",
    "    EPS     = float(lab.get(\"epsilon\", 0.0)) or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b898427f-16bc-44dd-ba5e-5f2abd283aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktives RUN_DIR: C:\\Users\\jacin\\DL_PROJECT\\finance_transformer_lstm\\LSTM\\results\\2026-01-03_21-02-35_lstm\n"
     ]
    }
   ],
   "source": [
    "# === RUN-VERZEICHNIS BESTIMMEN ===\n",
    "# Man kann RUN_DIR auch per Environment-Variable erzwingen (f\u00fcr Pipeline-Automation wichtig!)\n",
    "run_override = os.getenv(\"RUN_DIR\", \"\").strip() or None\n",
    "\n",
    "if run_override:\n",
    "    RUN_DIR = Path(run_override).resolve()\n",
    "else:\n",
    "    # Automatische Suche nach dem passenden neuesten Run\n",
    "    RUN_DIR = latest_lstm_run(RESULTS_DIR, lookback=LOOKBACK, horizon=HORIZON, eps_mode=MODE, epsilon=EPS, strict=False)\n",
    "\n",
    "if RUN_DIR is None or not RUN_DIR.exists():\n",
    "    raise SystemExit(\"Kein *_lstm Run gefunden. Bitte Block 3/4/6 vorher einmal ausf\u00fchren.\")\n",
    "\n",
    "print(\"Aktives RUN_DIR:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f9e4fd-dafa-468d-8049-f2437e2e2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ARTEFAKTE EINLESEN ===\n",
    "# Wir laden 'evaluation.json', das in den vorherigen Blocks St\u00fcck f\u00fcr St\u00fcck gef\u00fcllt wurde.\n",
    "# Es enth\u00e4lt Metriken, Configs, Backtest-Ergebnisse etc.\n",
    "ev_path = RUN_DIR / \"evaluation.json\"\n",
    "if not ev_path.exists():\n",
    "    raise SystemExit(f\"evaluation.json fehlt in {RUN_DIR}. Wurden Block 4 und 6 ausgef\u00fchrt?\")\n",
    "\n",
    "ev    = jread(ev_path)\n",
    "cfg   = ev.get(\"config\", {})\n",
    "metrics = (ev.get(\"metrics\", {}) or {}).get(\"test\", {})\n",
    "thr_sel = ev.get(\"threshold_selection\", {})\n",
    "calib   = ev.get(\"calibration\", {})\n",
    "backtest_gross = ev.get(\"backtest\", {}) # Backtest ohne Kosten (aus Block 4)\n",
    "\n",
    "# Fallback: Label-Parameter, falls oben nicht gefunden, aus dem Evaluation-File holen\n",
    "if HORIZON is None:\n",
    "    HORIZON = int(((ev.get(\"label_resolved_from\") or {}).get(\"horizon\")) or cfg.get(\"horizon\"))\n",
    "if MODE is None:\n",
    "    MODE = (ev.get(\"label_resolved_from\") or {}).get(\"mode\") or cfg.get(\"epsilon_mode\")\n",
    "if EPS is None:\n",
    "    EPS = float((ev.get(\"label_resolved_from\") or {}).get(\"epsilon\") or cfg.get(\"epsilon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0407107f-a03e-4891-856e-c16726338a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === KOSTEN-DATEN LADEN ===\n",
    "# Aus Block 6: Sensitivit\u00e4ts-Ergebnisse laden.\n",
    "sens_path = RUN_DIR / \"cost_sensitivity.csv\"\n",
    "sens_df = pd.read_csv(sens_path) if sens_path.exists() else None\n",
    "\n",
    "MAIN_RT = 15.0  # Unser Standard-Szenario f\u00fcr den Bericht: 15 bps Roundtrip\n",
    "MAIN_SLIP_PER_LEG = 2.0\n",
    "\n",
    "cost_pick = {}\n",
    "if sens_df is not None and not sens_df.empty:\n",
    "    # Wir filtern auf das realistische Modell \"Entry@t+1\"\n",
    "    df_t1_exact = sens_df[sens_df[\"model\"] == \"Entry@t+1\"]\n",
    "    # Fallback f\u00fcr Namensvariationen (starts with)\n",
    "    df_t1_prefix = sens_df[sens_df[\"model\"].astype(str).str.startswith(\"Entry@t+1\")]\n",
    "    df_t1 = df_t1_exact if not df_t1_exact.empty else df_t1_prefix\n",
    "\n",
    "    if df_t1.empty:\n",
    "        df_t1 = sens_df.copy() # Falls Filter fehlschl\u00e4gt, nimm alles (Notl\u00f6sung)\n",
    "\n",
    "    # Wir suchen die Zeile, die den 15bps (MAIN_RT) am n\u00e4chsten kommt\n",
    "    df_t1[\"rt_diff\"] = (df_t1[\"roundtrip_bps\"] - MAIN_RT).abs()\n",
    "    row = df_t1.sort_values([\"rt_diff\", \"roundtrip_bps\"]).iloc[0].to_dict()\n",
    "\n",
    "    # Struktur f\u00fcr den Report aufbereiten\n",
    "    cost_pick = dict(\n",
    "        model=row[\"model\"], \n",
    "        roundtrip_bps=float(row[\"roundtrip_bps\"]),\n",
    "        trades=int(row.get(\"trades\", 0)), \n",
    "        exposure=float(row.get(\"exposure\", np.nan)),\n",
    "        turnover=float(row.get(\"turnover\", np.nan)),\n",
    "        CAGR=float(row[\"CAGR\"]), \n",
    "        Sharpe=float(row[\"Sharpe\"]), \n",
    "        MaxDD=float(row[\"MaxDD\"]),\n",
    "        final_equity=float(row[\"final_equity\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69024c17-9f6c-4017-97de-5b7c461aff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASELINES BERECHNEN ===\n",
    "# Wir berechnen einfache Benchmarks, um zu sehen, ob das komplexe LSTM \u00fcberhaupt Mehrwert liefert.\n",
    "# WICHTIG: Das muss auf exakt denselben Daten (Test-Split) passieren!\n",
    "\n",
    "TRAIN_CSV = Path(cfg.get(\"train_csv\", \"\"))\n",
    "features_list = cfg.get(\"features\", None)\n",
    "\n",
    "if not features_list:\n",
    "    # Falls keine Feature-Liste im Config, aus YAML holen\n",
    "    yml = ROOT / f\"data/features_{FEATURESET}.yml\"\n",
    "    if yml.exists():\n",
    "        meta = yaml.safe_load(open(yml, \"r\")) or {}\n",
    "        features_list = meta.get(\"features\", [])\n",
    "\n",
    "# Originaldaten laden und splitten\n",
    "df_all = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
    "X_all = df_all[features_list].copy()\n",
    "y_all = df_all[\"target\"].astype(int).copy()\n",
    "\n",
    "# Zeitlicher Split (muss identisch zu Notebook 3 sein: 70/15/15)\n",
    "n = len(df_all)\n",
    "n_train = int(n * 0.70)\n",
    "n_val   = int(n * 0.15)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "# Splits erstellen\n",
    "X_train, y_train = X_all.iloc[:n_train],              y_all.iloc[:n_train]\n",
    "X_val,   y_val   = X_all.iloc[n_train:n_train+n_val], y_all.iloc[n_train:n_train+n_val]\n",
    "X_test,  y_test  = X_all.iloc[n_train+n_val:],        y_all.iloc[n_train+n_val:]\n",
    "\n",
    "# Preprocessing und Scaling (f\u00fcr LogReg)\n",
    "LB = int(cfg.get(\"lookback\", LOOKBACK))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Scaler nur auf Train fitten! (Data Leakage vermeiden)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "Xtr_s = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "Xva_s = pd.DataFrame(scaler.transform(X_val),   index=X_val.index,   columns=X_val.columns)\n",
    "Xte_s = pd.DataFrame(scaler.transform(X_test),  index=X_test.index,  columns=X_test.columns)\n",
    "\n",
    "# Da LSTMs die ersten LB-1 Zeilen verlieren (Lookback), passen wir den Vergleich hier auch an.\n",
    "# Wir schneiden die ersten LB-1 Zeilen ab, um denselben Test-Zeitraum zu haben.\n",
    "tail = slice(LB-1, None)\n",
    "ytr_tail, yva_tail, yte_tail = y_train.iloc[tail], y_val.iloc[tail], y_test.iloc[tail]\n",
    "Xtr_tail, Xva_tail, Xte_tail = Xtr_s.iloc[tail],   Xva_s.iloc[tail],   Xte_s.iloc[tail]\n",
    "\n",
    "# 1. Baseline: Always-Up / Buy-And-Hold (Rate der positiven Labels)\n",
    "# Simpelste Annahme: Der Markt geht immer hoch.\n",
    "pos_rate_test = float(yte_tail.mean())\n",
    "auprc_always_up = pos_rate_test\n",
    "\n",
    "# 2. Baseline: Logistische Regression (Lineares Referenz-Modell)\n",
    "# Das hilft zu pr\u00fcfen, ob die Nicht-Linearit\u00e4t des LSTM wirklich n\u00f6tig ist.\n",
    "logit = LogisticRegression(max_iter=200)\n",
    "logit.fit(Xtr_tail, ytr_tail)\n",
    "proba_lr = logit.predict_proba(Xte_tail)[:,1]\n",
    "auprc_lr = float(average_precision_score(yte_tail, proba_lr))\n",
    "\n",
    "# 3. Baseline: Simpler MACD Indikator (falls vorhanden)\n",
    "# Wir testen, ob ein einfacher technischer Indikator besser ist als das komplexe Modell.\n",
    "if \"macd_diff\" in df_all.columns:\n",
    "    macd_diff = df_all.loc[Xte_tail.index, \"macd_diff\"].astype(float).fillna(0.0)\n",
    "    auprc_macd = float(average_precision_score(yte_tail, macd_diff.values))\n",
    "else:\n",
    "    auprc_macd = 0.0\n",
    "\n",
    "# Tabelle zusammenstellen: Lift Factor zeigt Verbesserung gegen\u00fcber Zufall (Always-Up)\n",
    "baselines_tbl = pd.DataFrame([\n",
    "    {\"baseline\": \"Always-Up (Prior)\", \"auprc\": auprc_always_up, \"pos_rate\": pos_rate_test,\n",
    "     \"lift_factor\": 1.0},\n",
    "    {\"baseline\": \"Logistic Regression\", \"auprc\": auprc_lr, \"pos_rate\": pos_rate_test,\n",
    "     \"lift_factor\": (auprc_lr / max(pos_rate_test, 1e-12))},\n",
    "    {\"baseline\": \"Simple MACD\", \"auprc\": auprc_macd, \"pos_rate\": pos_rate_test,\n",
    "     \"lift_factor\": (auprc_macd / max(pos_rate_test, 1e-12))}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a3d6ad-a1c5-4a7e-83d1-bf5de33dc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === REPORT DATEN ZUSAMMENF\u00dcHREN ===\n",
    "# Wir bauen ein gro\u00dfes Dictionary 'kpis', in dem alle wichtigen Infos gesammelt sind.\n",
    "# Das dient als Datenquelle f\u00fcr den Markdown-Report und das JSON-Export-File.\n",
    "\n",
    "kpis = {\n",
    "    \"run_dir\": str(RUN_DIR),\n",
    "    \"generated_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"data\": {\n",
    "        \"ticker\": cfg.get(\"ticker\"),\n",
    "        \"interval\": cfg.get(\"interval\"),\n",
    "        \"period\": [cfg.get(\"start\"), cfg.get(\"end\")],\n",
    "        \"horizon\": cfg.get(\"horizon\"),\n",
    "        \"lookback\": cfg.get(\"lookback\"),\n",
    "        \"featureset\": cfg.get(\"featureset\"),\n",
    "        \"features_used\": ev.get(\"features_used\"),\n",
    "    },\n",
    "    \"label\": ev.get(\"label_resolved_from\"), # Informationen zum Label (Target)\n",
    "    \"calibration\": {\n",
    "        \"chosen\": calib.get(\"chosen\"),\n",
    "        \"val_brier\": calib.get(\"val_brier\"),\n",
    "        \"test_brier\": calib.get(\"test_brier\"),\n",
    "        \"note\": \"Kalibrierung auf Validation Set, Test Set Werte nur zur Info.\"\n",
    "    },\n",
    "    \"threshold\": {\n",
    "        \"strategy\": thr_sel.get(\"strategy\"),\n",
    "        \"threshold\": thr_sel.get(\"threshold\"),\n",
    "        \"val_mcc\": thr_sel.get(\"val_mcc\"),\n",
    "        \"test_pred_pos_rate\": thr_sel.get(\"test_pred_pos_rate\"),\n",
    "    },\n",
    "    \"classification_test\": {\n",
    "        \"roc_auc\": metrics.get(\"roc_auc\"),\n",
    "        \"auprc\": metrics.get(\"auprc\"),\n",
    "        \"brier\": metrics.get(\"brier\"),\n",
    "        \"balanced_accuracy\": metrics.get(\"balanced_accuracy\"),\n",
    "        \"mcc\": metrics.get(\"mcc\"),\n",
    "    },\n",
    "    \"backtest_gross\": backtest_gross,     # Backtest Ergebnis ohne Kosten\n",
    "    \"backtest_cost_pick\": cost_pick,      # Ergebnisse mit Kosten (Realistisch, 15bps)\n",
    "    \"baselines\": baselines_tbl.to_dict(orient=\"records\") # Baseline-Vergleich\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e11499-04f4-45e6-9659-437cd867ed45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 REPORT geschrieben: C:\\Users\\jacin\\DL_PROJECT\\finance_transformer_lstm\\LSTM\\results\\2026-01-03_21-02-35_lstm\\REPORT_block7.md\n"
     ]
    }
   ],
   "source": [
    "# === REPORT GENERIEREN (MARKDOWN) ===\n",
    "# Wir schreiben die gesammelten Daten in eine sauber formatierte Markdown-Datei.\n",
    "# Diese Datei kann direkt im Browser oder in IDEs sch\u00f6n gelesen werden.\n",
    "\n",
    "fig_dir = RUN_DIR / \"figures\"\n",
    "# Dateipfade zu den Bildern, die wir einbinden wollen\n",
    "figs = {\n",
    "    \"roc\": fig_dir / \"roc_test.png\",\n",
    "    \"pr\":  fig_dir / \"pr_test.png\",\n",
    "    \"equity_cost\":  fig_dir / \"equity_costed.png\",\n",
    "}\n",
    "\n",
    "def _rel(p: Path) -> str:\n",
    "    # Hilfsfunktion f\u00fcr relative Pfade (damit der Report portabel ist, z.B. wenn man den Ordner verschiebt)\n",
    "    return str(p.relative_to(RUN_DIR)) if p.exists() else str(p)\n",
    "\n",
    "report_md = RUN_DIR / \"REPORT_block7.md\"\n",
    "lines = []\n",
    "\n",
    "# Header\n",
    "lines.append(f\"# Block 7 \u2013 Abschluss-Report\\n\")\n",
    "lines.append(f\"- **Generiert:** {kpis['generated_utc']}\")\n",
    "lines.append(f\"- **Run:** `{RUN_DIR.name}`\")\n",
    "lines.append(f\"- **Modell-Setup:** Ticker `{kpis['data']['ticker']}` | Horizon `{kpis['data']['horizon']}` | Lookback `{kpis['data']['lookback']}`\")\n",
    "lines.append(\"---\")\n",
    "\n",
    "# Klassifikations-Metrics (Vorhersage-Qualit\u00e4t)\n",
    "lines.append(\"## 1. Klassifikations-Leistung (Test Set)\")\n",
    "m = kpis[\"classification_test\"]\n",
    "lines.append(f\"Wir bewerten, wie gut das Modell Wahrscheinlichkeiten vorhersagt.\")\n",
    "lines.append(f\"- **AUROC**: `{m['roc_auc']:.3f}` (Fl\u00e4che unter ROC Kurve, 0.5 = Zufall)\")\n",
    "lines.append(f\"- **AUPRC**: `{m['auprc']:.3f}` (Fl\u00e4che unter Precision-Recall Kurve, Basisrate \u2248 {kpis['baselines'][0]['pos_rate']:.2f})\")\n",
    "lines.append(f\"- **MCC**: `{m['mcc']:.3f}` (Matthews Correlation Coefficient, >0 ist besser als Zufall)\")\n",
    "\n",
    "lines.append(f\"\\n### Grafiken\\n\")\n",
    "lines.append(f\"![ROC]({_rel(figs['roc'])})  \\n![PR]({_rel(figs['pr'])})\\n\")\n",
    "\n",
    "# Backtests (Finanzielle Performance)\n",
    "lines.append(\"## 2. Finanzielle Performance (Backtest)\")\n",
    "cp = kpis[\"backtest_cost_pick\"]\n",
    "if cp:\n",
    "    lines.append(f\"Simuliertes Handelsergebnis mit **{cp['roundtrip_bps']}** bps Roundtrip-Kosten und T+1 Entry:\")\n",
    "    lines.append(f\"- **CAGR**: `{cp['CAGR']:.2%}` (J\u00e4hrliche Rendite)\")\n",
    "    lines.append(f\"- **Sharpe Ratio**: `{cp['Sharpe']:.2f}` (Risikoadjustierte Rendite)\")\n",
    "    lines.append(f\"- **Max Drawdown**: `{cp['MaxDD']:.2%}` (Maximaler zwischenzeitlicher Verlust)\")\n",
    "    lines.append(f\"- **Endkapital**: `{cp['final_equity']:.2f}` (Start = 1.00)\")\n",
    "    lines.append(f\"\\n![Equity Trace]({_rel(figs['equity_cost'])})\\n\")\n",
    "else:\n",
    "    lines.append(\"*(Keine Kostendaten gefunden)*\\n\")\n",
    "\n",
    "# Baselines (Vergleichswerte)\n",
    "lines.append(\"## 3. Vergleich mit Benchmarks\")\n",
    "lines.append(\"Ist das Modell besser als simple Methoden? (Lift Factor > 1.0)\")\n",
    "lines.append(\"| Baseline | AUPRC | Lift Factor |\")\n",
    "lines.append(\"|---|---|---|\")\n",
    "for b in kpis[\"baselines\"]:\n",
    "    lines.append(f\"| {b['baseline']} | {b['auprc']:.3f} | {b['lift_factor']:.2f}x |\")\n",
    "\n",
    "lines.append(\"\\n---\")\n",
    "lines.append(\"**Ende des Berichts.**\")\n",
    "\n",
    "# Datei schreiben\n",
    "report_md.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "print(\"\u2713 REPORT geschrieben:\", report_md)\n",
    "\n",
    "# JSON Dump f\u00fcr sp\u00e4tere Analyse (Maschinen-lesbar)\n",
    "(RUN_DIR / \"REPORT_block7_kpis.json\").write_text(json.dumps(kpis, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# CSV Zeile f\u00fcr Sammel-Auswertungen (wenn man viele Experimente vergleichen will)\n",
    "try:\n",
    "    kpi_rows = {\n",
    "        \"roc_auc\": metrics.get(\"roc_auc\"), \n",
    "        \"auprc\": metrics.get(\"auprc\"), \n",
    "        \"mcc\": metrics.get(\"mcc\"),\n",
    "        \"cost_CAGR\": cp.get(\"CAGR\") if cp else None,\n",
    "        \"cost_Sharpe\": cp.get(\"Sharpe\") if cp else None,\n",
    "    }\n",
    "    pd.DataFrame([kpi_rows]).to_csv(RUN_DIR / \"kpis_block7.csv\", index=False)\n",
    "except Exception as e:\n",
    "    print(\"Fehler beim CSV-Export:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}