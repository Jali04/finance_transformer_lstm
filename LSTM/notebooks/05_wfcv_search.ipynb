{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9701daf5-1f64-4226-9ca2-aa40821fc6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.10.0\n",
      "\u2705 GPU DETECTED: 1 device(s)\n",
      "  [0] /physical_device:GPU:0\n",
      "      Compute Capability: (8, 9)\n",
      "\ud83d\ude80 Mixed Precision ENABLED (Float16 speedup active)\n"
     ]
    }
   ],
   "source": [
    "# === SYSTEM & IMPORTS ===\n",
    "# Block 5: Hyperparameter-Optimierung mit Walk-Forward Cross-Validation (WFCV)\n",
    "#\n",
    "# Ziel: Die besten Parameter f\u00fcr das Modell finden, ohne \"in die Zukunft\" zu schauen.\n",
    "# Methode: Wir trainieren auf [Vergangenheit] -> testen auf [Gegenwart].\n",
    "# Dann schieben wir das Fenster weiter: trainieren auf [Vergangenheit + Gegenwart] -> testen auf [Zukunft].\n",
    "\n",
    "import os, sys, json, time, logging, glob, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pfad zum Projekt-Root setzen\n",
    "ROOT = os.path.abspath(\"..\")\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "# TensorFlow-Logs unterdr\u00fccken (nur Fehler anzeigen), um Output sauber zu halten\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# GPU-Erkennung und Konfiguration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\u2705 GPU DETECTED: {len(gpus)} device(s)\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  [{i}] {gpu.name}\")\n",
    "        try:\n",
    "            # Optional: Details zur Rechenleistung der GPU abfragen\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"      Compute Capability: {details.get('compute_capability')}\")\n",
    "        except:\n",
    "            pass\n",
    "    # Mixed Precision aktivieren (nutzt float16 statt float32 wo m\u00f6glich -> Schneller auf modernen nvidia GPUs)\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print(\"\ud83d\ude80 Mixed Precision ENABLED (Float16 speedup active)\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Mixed Precision init failed: {e}\")\n",
    "else:\n",
    "    print(\"\u274c NO GPU DETECTED! Running on CPU (will be slow).\")\n",
    "    print(\"   Please check CUDA/cuDNN installation if you have an NVIDIA GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db180739-46e5-44c1-a93d-6e99e9c3ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WFCV_RUN_DIR: ..\\results\\2026-01-03_20-56-59_wfcv\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG & SETUP ===\n",
    "# Wir laden die zentrale Konfiguration\n",
    "with open(os.path.join(ROOT, \"config.json\"), \"r\") as f:\n",
    "    C = json.load(f)\n",
    "\n",
    "# Basis-Parameter \u00fcbernehmen\n",
    "TICKER   = C[\"ticker\"]       # Welches Asset?\n",
    "START    = C[\"start\"]        # Startdatum\n",
    "END      = C[\"end\"]          # Enddatum\n",
    "INTERVAL = C[\"interval\"]     # Intervall (z.B. 1d)\n",
    "HORIZON  = int(C[\"horizon\"]) # Vorhersage-Horizont\n",
    "LOOKBACK_DEFAULT = int(C[\"lookback\"]) # Standard-Lookback (falls wir ihn nicht variieren)\n",
    "BATCH    = int(C.get(\"batch\", 64))    # Batch-Gr\u00f6\u00dfe\n",
    "SEED     = int(C.get(\"seed\", 42))     # Random Seed\n",
    "FEATURESET = C.get(\"featureset\", \"v2\") # Feature-Set Name\n",
    "EPS_MODE   = C.get(\"epsilon_mode\", \"abs\")\n",
    "EPSILON    = float(C.get(\"epsilon\", 0.0005))\n",
    "\n",
    "# Ergebnis-Verzeichnis vorbereiten\n",
    "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\"))\n",
    "\n",
    "# Globalen Seed setzen\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# Neuer Ausgabe-Ordner mit Zeitstempel speziell f\u00fcr diesen WFCV-Lauf\n",
    "# Format: YYYY-MM-DD_HH-MM-SS_wfcv\n",
    "RUN_DIR = RESULTS_DIR / time.strftime(\"%Y-%m-%d_%H-%M-%S_wfcv\")\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Unterordner f\u00fcr Plots anlegen\n",
    "(RUN_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"WFCV_RUN_DIR:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55da04a-e118-4fc9-a5f2-339a4a1fbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FAST MODE ===\n",
    "# WFCV kann sehr lange dauern (Stunden bis Tage bei vielen Parametern).\n",
    "# F\u00fcr Debugging oder schnelle Tests nutzen wir den \"Fast Mode\".\n",
    "FAST = C.get(\"fast_wfcv\", False)\n",
    "\n",
    "# Einstellungen f\u00fcr \"Normal\" (Full Grid) und \"Fast\" (Reduziert)\n",
    "EPOCHS_GRID = 1   # Wie viele Epochen pro Grid-Point? 1 f\u00fcr Ultrafast, sonst z.B. 10.\n",
    "N_FOLDS = 2       # Wie viele Zeit-Folds? 2 ist Minimum.\n",
    "\n",
    "if FAST:\n",
    "    print(\"[INFO] Fast Mode ist AKTIV. Reduzierte Epochen und Folds.\")\n",
    "# Wenn Fast Mode aktiv ist, halten wir die Epochen niedrig.\n",
    "# Im echten Lauf w\u00fcrde man das in config.json steuern.\n",
    "EPOCHS_GRID = 1   # Set to 1 for Instant Mode (<1 min)\n",
    "N_FOLDS = 2       # Min Folds\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50b8c3e-f514-45e3-97c6-3f649cd47ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade TRAIN_CSV: ../data/AAPL_1d_2010-01-01_2026-01-01_cls_h1_abs0p0005.csv\n",
      "Label Positive Rate (gesamt): 0.514 | Datens\u00e4tze: 3991\n"
     ]
    }
   ],
   "source": [
    "# === DATEN LADEN ===\n",
    "# Wir m\u00fcssen die Features (X) und Labels (y) laden.\n",
    "import yaml\n",
    "\n",
    "# 1. Features-Metadaten laden, falls vorhanden (extrahiert Parameter aus YAML)\n",
    "yaml_path = f\"../data/features_{FEATURESET}.yml\"\n",
    "meta = {}\n",
    "label_h = label_mode = label_eps = None\n",
    "\n",
    "if os.path.exists(yaml_path):\n",
    "    with open(yaml_path, \"r\") as f:\n",
    "        meta = yaml.safe_load(f) or {}\n",
    "    lab = (meta or {}).get(\"label\", {})\n",
    "    label_h    = lab.get(\"horizon\")\n",
    "    label_mode = lab.get(\"mode\")\n",
    "    label_eps  = lab.get(\"epsilon\")\n",
    "\n",
    "# Hilfsfunktion zum Parsen des Dateinamens (Format: ..._cls_h5_abs0p0005.csv)\n",
    "def _parse_h_meps_from_name(path: str):\n",
    "    mH = re.search(r\"_cls_h(\\d+)_\", path)\n",
    "    me = re.search(r\"_(abs|rel)(\\d+p\\d+)\\.csv$\", path)\n",
    "    H  = int(mH.group(1)) if mH else None\n",
    "    md = me.group(1) if me else None\n",
    "    eps= float(me.group(2).replace(\"p\",\".\")) if me else None\n",
    "    return H, md, eps\n",
    "\n",
    "# Hilfsfunktion zur Suche passender Dateien im Ordner\n",
    "def _infer_from_existing_files(tkr, itv, start, end, mode_hint=None, eps_hint=None):\n",
    "    pat = f\"../data/{tkr}_{itv}_{start}_{end}_cls_h*_.csv\".replace(\"_ .csv\",\".csv\")\n",
    "    cands = sorted(glob.glob(pat), key=os.path.getmtime)\n",
    "    \n",
    "    if mode_hint and (eps_hint is not None):\n",
    "        tag = f\"{mode_hint}{str(eps_hint).replace('.','p')}\"\n",
    "        cands = [c for c in cands if c.endswith(f\"_{tag}.csv\")]\n",
    "        \n",
    "    if not cands:\n",
    "        return None\n",
    "    return _parse_h_meps_from_name(cands[-1])\n",
    "\n",
    "# Logik zur Bestimmung der Label-Parameter:\n",
    "# Versuch 1: Parameter aus YAML nehmen\n",
    "H_FOR_FILE    = int(label_h)    if label_h    is not None else None\n",
    "MODE_FOR_FILE = str(label_mode) if label_mode is not None else None\n",
    "EPS_FOR_FILE  = float(label_eps) if label_eps is not None else None\n",
    "\n",
    "# Versuch 2: Parameter aus Dateinamen erraten (falls in YAML nicht gefunden)\n",
    "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
    "    inferred = _infer_from_existing_files(TICKER, INTERVAL, START, END,\n",
    "                                          mode_hint=MODE_FOR_FILE, eps_hint=EPS_FOR_FILE)\n",
    "    if inferred is not None:\n",
    "        H_i, M_i, E_i = inferred\n",
    "        H_FOR_FILE    = H_FOR_FILE    if H_FOR_FILE    is not None else H_i\n",
    "        MODE_FOR_FILE = MODE_FOR_FILE if MODE_FOR_FILE is not None else M_i\n",
    "        EPS_FOR_FILE  = EPS_FOR_FILE  if EPS_FOR_FILE  is not None else E_i\n",
    "\n",
    "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
    "    raise RuntimeError(\"Label-Definition unklar. Bitte Block 2 pr\u00fcfen.\")\n",
    "\n",
    "# Dateipfad endg\u00fcltig zusammenbauen\n",
    "eps_tag   = f\"{MODE_FOR_FILE}{str(EPS_FOR_FILE).replace('.','p')}\"\n",
    "TRAIN_CSV = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{H_FOR_FILE}_{eps_tag}.csv\"\n",
    "\n",
    "# Existenz checken und laden\n",
    "if not os.path.exists(TRAIN_CSV):\n",
    "    # Fallback Suche nach leicht abweichenden Namen\n",
    "    pat = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h*_{eps_tag}.csv\"\n",
    "    candidates = sorted(glob.glob(pat), key=os.path.getmtime)\n",
    "    if candidates:\n",
    "        TRAIN_CSV = candidates[-1]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"CSV nicht gefunden: {TRAIN_CSV}\")\n",
    "\n",
    "print(\"Lade TRAIN_CSV:\", TRAIN_CSV)\n",
    "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "# Gesamten Feature-Pool bestimmen (alles au\u00dfer Label und OHLCV)\n",
    "OHLCV = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
    "if meta:\n",
    "    FEATURES_ALL = [c for c in meta.get(\"features\", []) if c in df.columns]\n",
    "else:\n",
    "    FEATURES_ALL = [c for c in df.columns if c not in (OHLCV | {\"target\"})]\n",
    "    \n",
    "assert len(FEATURES_ALL) > 0, \"Keine Features zum Optimieren gefunden.\"\n",
    "\n",
    "# Wir behalten den ganzen DataFrame im Speicher (X und y getrennt) f\u00fcr sp\u00e4tere Splits\n",
    "X_full = df[FEATURES_ALL].copy()\n",
    "y_full = df[\"target\"].astype(int).copy()\n",
    "\n",
    "print(\"Label Positive Rate (gesamt):\", round(y_full.mean(), 3), \"| Datens\u00e4tze:\", len(y_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408b2564-2d68-4706-838e-f3e1a499c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl generierter Folds: 2\n",
      "  Fold1: Train bis idx=1796, Val bis idx=2594 (Gr\u00f6\u00dfe Val: 798)\n"
     ]
    }
   ],
   "source": [
    "# === SPLITTING (Walk-Forward Logik) ===\n",
    "# Walk-Forward Validation bedeutet, dass sich das Zeitfenster schiebt.\n",
    "# Wir trainieren nie auf Daten, die in der Zukunft der Testdaten liegen.\n",
    "\n",
    "def make_wf_splits(n, n_folds=5, val_frac=0.20, min_train_frac=0.45):\n",
    "    # n: Anzahl Datenpunkte total\n",
    "    # val_frac: Wie viel % des aktuellen Fensters sind Validation?\n",
    "    # min_train_frac: Wie gro\u00df muss das Trainingset MINDESTENS sein?\n",
    "    \n",
    "    # L\u00e4nge des Validation-Sets berechnen\n",
    "    val_len   = max(60, int(round(n * val_frac)))\n",
    "    # Minimale Trainingsl\u00e4nge berechnen\n",
    "    min_train = max(200, int(round(n * min_train_frac)))\n",
    "    \n",
    "    # Der erste Split muss mindestens min_train + val_len gro\u00df sein\n",
    "    start_val_end = min_train + val_len\n",
    "    if start_val_end + 1 > n:\n",
    "        raise ValueError(f\"Dataset zu kurz f\u00fcr diese Split-Parameter: {n}\")\n",
    "    \n",
    "    # Wir verteilen die Endpunkte der Folds gleichm\u00e4\u00dfig \u00fcber die verbleibende Zeit\n",
    "    # Beispiel: Wenn noch 1000 Tage \u00fcbrig sind und wir 5 Folds wollen, endet jeder Fold 200 Tage sp\u00e4ter.\n",
    "    val_ends = np.linspace(start_val_end, n, num=n_folds, endpoint=True).astype(int)\n",
    "    val_ends = np.unique(val_ends)\n",
    "    \n",
    "    # Falls durch Rundung zu wenige Folds entstehen (bei kleinen Daten), fixieren wir Schritte\n",
    "    if len(val_ends) < n_folds:\n",
    "        step = max(1, (n - start_val_end) // n_folds)\n",
    "        val_ends = np.arange(start_val_end, start_val_end + step * n_folds, step)\n",
    "        val_ends = np.clip(val_ends, start_val_end, n)\n",
    "        \n",
    "    stops = []\n",
    "    for ve in val_ends[:n_folds]:\n",
    "        # Das Ende des Trainings ist 'val_len' vor dem Ende des Folds\n",
    "        te = int(ve - val_len)\n",
    "        # Training muss gro\u00df genug sein (Lookback beachten! wir brauchen Daten VOR dem ersten Fenster)\n",
    "        te = max(te, LOOKBACK_DEFAULT + 1)\n",
    "        \n",
    "        # Sicherheitschecks\n",
    "        if te <= 0 or ve <= te or ve > n:\n",
    "            continue\n",
    "            \n",
    "        # Wir speichern Slices: (Train-Bereich, Val-Bereich)\n",
    "        # Expanding Window: Train geht immer von 0 (Anfang) bis te (Ende Training)\n",
    "        stops.append((slice(0, te), slice(te, ve)))\n",
    "        \n",
    "    if len(stops) != n_folds:\n",
    "        # Warnung, falls wir nicht genug Folds generieren konnten (z.B. Daten zu kurz)\n",
    "        print(f\"[WARN] Konnte nur {len(stops)} von {n_folds} Folds generieren.\")\n",
    "        \n",
    "    return stops\n",
    "\n",
    "# Splits generieren\n",
    "splits = make_wf_splits(len(df), n_folds=N_FOLDS, val_frac=0.20, min_train_frac=0.45)\n",
    "print(\"Anzahl generierter Folds:\", len(splits))\n",
    "if len(splits) > 0:\n",
    "    tr_s, va_s = splits[0]\n",
    "    print(f\"  Fold1: Train bis idx={tr_s.stop}, Val bis idx={va_s.stop} (Gr\u00f6\u00dfe Val: {va_s.stop - va_s.start})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7faef79f-6bf2-4968-b014-0638ef49ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODELL-HELPER FUNKTIONEN ===\n",
    "# Funktionen zum Erstellen von Datasets und Modellen.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
    "from tensorflow.keras import layers, regularizers, callbacks, optimizers, models\n",
    "\n",
    "# Optimierte Dataset Funktion (ersetzt die manuelle make_windows Schleife)\n",
    "# Nutzt die internen C++ Funktionen von Keras f\u00fcr maximalen Speed.\n",
    "def make_dataset(X_df, y_ser, lookback, batch_size=64, shuffle=False, seed=42):\n",
    "    # Cast to float32/int32 explicitly for TF\n",
    "    data = X_df.values.astype(\"float32\")\n",
    "    targets = y_ser.values.astype(\"int32\")\n",
    "    \n",
    "    # timeseries_dataset_from_array erstellt automatisch Sliding Windows\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data,\n",
    "        targets=targets,\n",
    "        sequence_length=lookback,\n",
    "        sequence_stride=1,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=batch_size,\n",
    "        seed=seed,\n",
    "        start_index=0,\n",
    "        end_index=None\n",
    "    )\n",
    "    # Prefetch sorgt daf\u00fcr, dass die GPU nie warten muss (Daten werden vorgeladen)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Funktion zum Bauen des neuronalen Netzes (flexibel per Parameter)\n",
    "def build_model(n_features, width1=64, width2=32, dropout=0.10, lr=5e-4, use_gru=True):\n",
    "    rnn = layers.GRU if use_gru else layers.LSTM\n",
    "    m = models.Sequential([\n",
    "        # Input Shape: (None, n_features) -> 'None' steht f\u00fcr flexible Zeit-L\u00e4nge (Lookback)\n",
    "        layers.Input(shape=(None, n_features)),\n",
    "        # Erste RNN-Schicht (return_sequences=True f\u00fcr Stacked RNN)\n",
    "        rnn(width1, return_sequences=True, recurrent_dropout=dropout),\n",
    "        layers.LayerNormalization(),\n",
    "        # Zweite RNN-Schicht\n",
    "        rnn(width2, recurrent_dropout=dropout),\n",
    "        layers.LayerNormalization(),\n",
    "        # Dense Layer zur Verarbeitung\n",
    "        layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        # Output Layer (Sigmoid f\u00fcr Wahrscheinlichkeit 0-1)\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "    # Kompilieren mit Optimizer und Metriken\n",
    "    m.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                 tf.keras.metrics.AUC(name=\"auprc\", curve=\"PR\")]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "# Hilfsfunktion: MCC (Matthews Correlation Coefficient) f\u00fcr besten Threshold berechnen\n",
    "# Wir testen alle m\u00f6glichen Thresholds, um den besten MCC zu finden.\n",
    "def mcc_at_best_thr(y_true, y_prob):\n",
    "    ts = np.r_[0.0, np.unique(y_prob), 1.0] # Alle Wahrscheinlichkeiten als Kanddidaten\n",
    "    best = (-1.0, 0.5)\n",
    "    for t in ts:\n",
    "        yp = (y_prob >= t).astype(int)\n",
    "        m = matthews_corrcoef(y_true, yp)\n",
    "        if m > best[0]:\n",
    "            best = (float(m), float(t))\n",
    "    return best  # Returns (beste_mcc, bester_threshold)\n",
    "\n",
    "# Funktion f\u00fcr Training und Evaluation eines einzelnen Folds\n",
    "def fit_eval_fold_fast(ds_tr, ds_va, y_va, n_features, hp, epochs=EPOCHS_GRID):\n",
    "    # Session clearen, um RAM freizugeben\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Modell bauen mit \u00fcbergebenen Hyperparametern 'hp'\n",
    "    model = build_model(\n",
    "        n_features=n_features,\n",
    "        width1=hp[\"width1\"], width2=hp[\"width2\"],\n",
    "        dropout=hp[\"dropout\"], lr=hp[\"lr\"], use_gru=(hp[\"cell\"]==\"GRU\")\n",
    "    )\n",
    "\n",
    "    # Callbacks (Early Stopping, LR Reduction, NaN-Terminator)\n",
    "    cbs = [\n",
    "        callbacks.EarlyStopping(monitor=\"val_auprc\", mode=\"max\", patience=6, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_auprc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "        callbacks.TerminateOnNaN(),\n",
    "    ]\n",
    "\n",
    "    # Training starten (verbose=0 f\u00fcr Ruhe im Output)\n",
    "    hist = model.fit(ds_tr, validation_data=ds_va, epochs=epochs, verbose=0, callbacks=cbs)\n",
    "\n",
    "    # Evaluation auf Validation-Set\n",
    "    # Achtung: ds_va muss NICHT geshuffelt sein, damit die Reihenfolge zu y_va passt!\n",
    "    yva_proba = model.predict(ds_va, verbose=0).ravel()\n",
    "    \n",
    "    # Wichtig: y_va ist der Raw-Input. Wegen Windowing fehlen am Anfang 'lookback' Werte.\n",
    "    # Wir m\u00fcssen y_va am Ende beschneiden, damit es die gleiche L\u00e4nge wie yva_proba hat.\n",
    "    # (timeseries_dataset_from_array k\u00fcrzt am Anfang, wenn man start_index=0 l\u00e4sst)\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    mcc_val, thr_val = mcc_at_best_thr(y_va[-len(yva_proba):], yva_proba)\n",
    "    yva_true_clipped = y_va[-len(yva_proba):]\n",
    "    yva_pred_best = (yva_proba >= thr_val).astype(int)\n",
    "\n",
    "    # Dictionary mit Ergebnissen zur\u00fcckgeben\n",
    "    metrics = dict(\n",
    "        mcc=float(mcc_val),\n",
    "        thr_val=float(thr_val),\n",
    "        bal_acc=float(balanced_accuracy_score(yva_true_clipped, yva_pred_best)),\n",
    "        auprc=float(average_precision_score(yva_true_clipped, yva_proba)),\n",
    "        auroc=float(roc_auc_score(yva_true_clipped, yva_proba)),\n",
    "        epochs_trained=int(len(hist.history[\"loss\"]))\n",
    "    )\n",
    "    tf.keras.backend.clear_session()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337cbd2c-95af-41f5-9be0-dd323baf6fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\u00f6\u00dfe Suchraum:\n",
      "  HP-Kombinationen: 1\n",
      "  Lookback-Optionen: 1\n",
      "  Feature-Sets: 1\n",
      "  Folds pro Kombination: 2\n",
      "  -> Gesamte Training-Runs: 2\n"
     ]
    }
   ],
   "source": [
    "# === SUCH-GRIDS DEFINIEREN ===\n",
    "# Hier definieren wir den Suchraum f\u00fcr die Hyperparameter.\n",
    "\n",
    "# 1. Lookback: Wie weit schauen wir zur\u00fcck?\n",
    "# Wir testen nur 60 Tage (oder Default im Fast Mode), um Zeit zu sparen.\n",
    "LOOKBACK_GRID = [60] if not FAST else [LOOKBACK_DEFAULT]\n",
    "\n",
    "# 2. Modell-Architektur und Hyperparameter\n",
    "# Wir bauen eine Liste von Dictionaries (Grid Search).\n",
    "# Hier k\u00f6nnen wir Dimensionen, Dropout, Lernrate etc. variieren.\n",
    "HP_GRID = [\n",
    "    dict(width1=w1, width2=w2, dropout=dp, lr=lr, cell=cell)\n",
    "    for (w1, w2) in [(64,32)] # Netzgr\u00f6\u00dfe (nur eine Option)\n",
    "    for lr in [5e-4]          # Lernrate (nur eine Option)\n",
    "    for dp in [0.2]           # Dropout (nur 0.2 getestet)\n",
    "    for cell in [\"GRU\"]       # Zelltyp (nur GRU getestet)\n",
    "]\n",
    "\n",
    "# Falls FAST-Mode, \u00fcberschreiben wir das Grid mit nur einer Konfiguration\n",
    "if FAST:\n",
    "    HP_GRID = [dict(width1=32, width2=16, dropout=0.10, lr=5e-4, cell=\"GRU\")]\n",
    "\n",
    "# 3. Feature-Subsets: Welche Spalten nutzen wir?\n",
    "# Man kann testen, ob weniger Features (Rauschen entfernen) besser sind.\n",
    "FEATURE_SUBSETS = {\n",
    "    # Standard: Alle verf\u00fcgbaren Features\n",
    "    \"all\": FEATURES_ALL,\n",
    "    \n",
    "    # Optional: Experimentelle Subsets (auskommentiert f\u00fcr Speed)\n",
    "    # \"mom_only\": [c for c in FEATURES_ALL if ...],\n",
    "}\n",
    "\n",
    "print(\"Gr\u00f6\u00dfe Suchraum:\")\n",
    "print(f\"  HP-Kombinationen: {len(HP_GRID)}\")\n",
    "print(f\"  Lookback-Optionen: {len(LOOKBACK_GRID)}\")\n",
    "print(f\"  Feature-Sets: {len(FEATURE_SUBSETS)}\")\n",
    "print(f\"  Folds pro Kombination: {len(splits)}\")\n",
    "print(f\"  -> Gesamte Training-Runs: {len(HP_GRID) * len(LOOKBACK_GRID) * len(FEATURE_SUBSETS) * len(splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42408c93-f83a-4abc-ab98-1d0279037343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Suche ...\n",
      "\n",
      "[PREP] Generiere Datasets f\u00fcr Feat='all', LB=60 ...\n",
      "[all.. | LB=60 | GRU | Fold1] MCC=0.070 (Ep:1)\n",
      "[all.. | LB=60 | GRU | Fold2] MCC=0.054 (Ep:1)\n",
      "\n",
      "Completed. Time=32.2s\n"
     ]
    }
   ],
   "source": [
    "# === HAUPTSCHLEIFE: SUCHE DURCHF\u00dcHREN (OPTIMIERT) ===\n",
    "# Hier l\u00e4uft die eigentliche WFCV ab.\n",
    "from time import perf_counter\n",
    "\n",
    "print(\"Starte Suche ...\", flush=True)\n",
    "MAX_SECONDS = 60 * 60 * 2  # Zeitlimit: Max 2 Stunden\n",
    "\n",
    "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
    "records = []\n",
    "t0 = perf_counter()\n",
    "\n",
    "# Resume Check: Falls wir abgebrochen haben, laden wir bisherige Ergebnisse\n",
    "done_keys = set()\n",
    "if csv_path.exists():\n",
    "    try:\n",
    "        done_df = pd.read_csv(csv_path)\n",
    "        for _, r in done_df.iterrows():\n",
    "            # Erstelle einen eindeutigen Schl\u00fcssel f\u00fcr jeden erledigten Run\n",
    "            done_keys.add((r[\"features_used\"], int(r[\"lookback\"]),\n",
    "                           r[\"cell\"], int(r[\"width1\"]), int(r[\"width2\"]),\n",
    "                           float(r[\"dropout\"]), float(r[\"lr\"]), int(r[\"fold\"])))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "stop_time = t0 + MAX_SECONDS\n",
    "\n",
    "# \u00c4USSERE SCHLEIFEN (Daten-Dimensionen: Features & Lookback)\n",
    "# Wir iterieren erst \u00fcber Daten-Konfigurationen, um Datasets effizient vorzuberechnen.\n",
    "for feat_name, FEATS in FEATURE_SUBSETS.items():\n",
    "    if len(FEATS) == 0: continue\n",
    "\n",
    "    for lookback in LOOKBACK_GRID:\n",
    "        \n",
    "        # --- OPTIMIERUNG: Datasets vor den HP-Loops erstellen ---\n",
    "        # Wir bereiten die TF Datasets f\u00fcr diese (Feat, LB) Kombi EINMALIG vor.\n",
    "        # Das Skalieren und Windowing muss so nicht f\u00fcr jede HP-Kombi wiederholt werden.\n",
    "        fold_datasets = {}\n",
    "        \n",
    "        print(f\"\\n[PREP] Generiere Datasets f\u00fcr Feat='{feat_name}', LB={lookback} ...\")\n",
    "        \n",
    "        for fold_id, (tr_s, va_s) in enumerate(splits, start=1):\n",
    "            # 1. Slice Data: Daten f\u00fcr diesen Fold ausschneiden\n",
    "            X_tr, y_tr = X_full.iloc[tr_s][FEATS], y_full.iloc[tr_s]\n",
    "            X_va, y_va = X_full.iloc[va_s][FEATS], y_full.iloc[va_s]\n",
    "            \n",
    "            # 2. Scale: StandardScaler auf Train fitten, auf Val anwenden\n",
    "            scaler = StandardScaler()\n",
    "            X_tr_s = pd.DataFrame(scaler.fit_transform(X_tr), index=X_tr.index, columns=X_tr.columns)\n",
    "            X_va_s = pd.DataFrame(scaler.transform(X_va),     index=X_va.index, columns=X_va.columns)\n",
    "            \n",
    "            # 3. Make TF Datasets (using C++ generator for speed)\n",
    "            ds_tr = make_dataset(X_tr_s, y_tr, lookback, batch_size=BATCH, shuffle=True, seed=SEED)\n",
    "            ds_va = make_dataset(X_va_s, y_va, lookback, batch_size=BATCH, shuffle=False)\n",
    "            \n",
    "            # y_va (raw values) aufheben f\u00fcr Metrik-Berechnung.\n",
    "            # Wir m\u00fcssen beachten, dass das Dataset die ersten (lookback-1) Samples verschluckt.\n",
    "            y_va_aligned = y_va.values[lookback-1:] \n",
    "            \n",
    "            fold_datasets[fold_id] = (ds_tr, ds_va, y_va_aligned, len(FEATS))\n",
    "        \n",
    "        # --- INNERE SCHLEIFE: Hyperparameter ---\n",
    "        for hp in HP_GRID:\n",
    "            for fold_id in range(1, len(splits) + 1):\n",
    "                # Pr\u00fcfen, ob dieser Run schon erledigt ist\n",
    "                key = (feat_name, int(lookback),\n",
    "                       hp[\"cell\"], int(hp[\"width1\"]), int(hp[\"width2\"]),\n",
    "                       float(hp[\"dropout\"]), float(hp[\"lr\"]), int(fold_id))\n",
    "                \n",
    "                if key in done_keys:\n",
    "                    continue\n",
    "                \n",
    "                # Zeitlimit pr\u00fcfen\n",
    "                if perf_counter() > stop_time:\n",
    "                    break\n",
    "\n",
    "                # Vorberechnete Datasets holen\n",
    "                ds_tr, ds_va, y_va_true, n_feat = fold_datasets[fold_id]\n",
    "                \n",
    "                # Training & Eval durchf\u00fchren\n",
    "                mets = fit_eval_fold_fast(\n",
    "                    ds_tr, ds_va, y_va_true, n_feat,\n",
    "                    hp=hp, epochs=EPOCHS_GRID\n",
    "                )\n",
    "                \n",
    "                # Ergebnisse speichern\n",
    "                rec = {\n",
    "                    \"feature_set\": FEATURESET,\n",
    "                    \"features_used\": feat_name,\n",
    "                    \"n_features\": n_feat,\n",
    "                    \"lookback\": lookback,\n",
    "                    **hp,\n",
    "                    \"fold\": fold_id,\n",
    "                    **mets\n",
    "                }\n",
    "                records.append(rec)\n",
    "                # Sofort in CSV schreiben (Append Mode) als Backup\n",
    "                pd.DataFrame([rec]).to_csv(csv_path, mode='a', header=not os.path.exists(csv_path), index=False)\n",
    "                \n",
    "                # Kurze Log-Ausgabe\n",
    "                print(f\"[{feat_name[:5]}.. | LB={lookback} | {hp['cell']} | Fold{fold_id}] MCC={mets['mcc']:.3f} (Ep:{mets['epochs_trained']})\")\n",
    "\n",
    "            if perf_counter() > stop_time: break\n",
    "        if perf_counter() > stop_time: break\n",
    "    if perf_counter() > stop_time: \n",
    "        print(\"[INFO] Time limit reached.\")\n",
    "        break\n",
    "\n",
    "t1 = perf_counter()\n",
    "print(f\"\\nCompleted. Time={t1-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2afca6e0-5c56-45c9-93a0-066fc19b613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Konfigurationen:\n",
      "  features_used  lookback cell  dropout  mcc_mean   mcc_std\n",
      "0           all        60  GRU      0.2  0.061812  0.010962\n",
      "\n",
      "Best config saved to: ..\\results\\2026-01-03_20-56-59_wfcv\\best_config.json\n"
     ]
    }
   ],
   "source": [
    "# === ERGEBNIS-ANALYSE ===\n",
    "# Wir aggregieren die Ergebnisse aller Folds und suchen die beste Konfiguration.\n",
    "# Kriterium: Hoher Durchschnitts-MCC und geringe Standardabweichung (Stabilit\u00e4t).\n",
    "\n",
    "import pandas as pd, json, numpy as np\n",
    "\n",
    "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
    "if not csv_path.exists():\n",
    "    print(\"Keine Ergebnisse gefunden.\")\n",
    "else:\n",
    "    results = pd.read_csv(csv_path)\n",
    "\n",
    "    # Wir gruppieren nach allen Hyperparametern (au\u00dfer Fold)\n",
    "    agg_cols = [c for c in [\"feature_set\",\"features_used\",\"n_features\",\"lookback\",\n",
    "                            \"width1\",\"width2\",\"dropout\",\"lr\",\"cell\"] if c in results.columns]\n",
    "\n",
    "    # Aggregation berechnen: Mittelwert und Standardabweichung der Metriken\n",
    "    agg_dict = {\"mcc\": [\"mean\",\"std\"], \"auprc\": [\"mean\",\"std\"], \"auroc\": [\"mean\"]}\n",
    "    g = results.groupby(agg_cols).agg(agg_dict)\n",
    "\n",
    "    # Flache Spaltennamen erzeugen (MultiIndex entfernen, z.B. mcc_mean)\n",
    "    g.columns = [\n",
    "        \"_\".join([str(x) for x in col if str(x) != \"\"]).strip(\"_\")\n",
    "        for col in g.columns.to_flat_index()\n",
    "    ]\n",
    "    g = g.reset_index()\n",
    "\n",
    "    # Sortieren: Beste Konfiguration zuerst.\n",
    "    # Priorit\u00e4t: Hoher MCC Mean -> Hoher AUPRC Mean -> Niedrige MCC Std (Stabilit\u00e4t)\n",
    "    g = g.sort_values([\"mcc_mean\",\"auprc_mean\",\"mcc_std\"], ascending=[False, False, True])\n",
    "\n",
    "    # Speichern der aggregierten Tabelle\n",
    "    g.to_csv(RUN_DIR / \"wfcv_results_agg.csv\", index=False)\n",
    "    \n",
    "    # Top 5 speichern\n",
    "    top5 = g.head(5).copy()\n",
    "    top5.to_csv(RUN_DIR / \"wfcv_results_top5.csv\", index=False)\n",
    "    \n",
    "    print(\"Top 3 Konfigurationen:\")\n",
    "    print(top5.head(3)[[\"features_used\", \"lookback\", \"cell\", \"dropout\", \"mcc_mean\", \"mcc_std\"]])\n",
    "\n",
    "    # Die allerbeste Config extrahieren und als JSON speichern\n",
    "    # Diese Datei (\"best_config.json\") wird von Notebook 3 automatisch geladen.\n",
    "    best = top5.iloc[0].to_dict() if len(top5) else {}\n",
    "    with open(RUN_DIR / \"best_config.json\", \"w\") as f:\n",
    "        json.dump(best, f, indent=2)\n",
    "\n",
    "    print(\"\\nBest config saved to:\", RUN_DIR / \"best_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a79f67d2-a756-442f-8aaa-047113714208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# === VISUALISIERUNG: HEATMAPS ===\n",
    "# Wir plotten Heatmaps, um zu sehen, welche Paremeter-R\u00e4ume gut funktionieren.\n",
    "# Das hilft, Muster zu erkennen (z.B. \"l\u00e4ngere Lookbacks sind gut\").\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if csv_path.exists():\n",
    "    agg = pd.read_csv(RUN_DIR / \"wfcv_results_agg.csv\")\n",
    "    \n",
    "    # Wir pivotieren die Tabelle f\u00fcr die Heatmap\n",
    "    # Y-Achse: Lookback, X-Achse: Features/Cell etc.\n",
    "    pivot_index = \"lookback\" if \"lookback\" in agg.columns else agg.columns[0]\n",
    "    col_candidates = [\"features_used\", \"cell\", \"width1\"]\n",
    "    pivot_columns = [c for c in col_candidates if c in agg.columns]\n",
    "    \n",
    "    def _plot_grid(df: pd.DataFrame, value_col: str, fname: str):\n",
    "        if value_col not in df.columns: return\n",
    "        try:\n",
    "            # Pivot Table erstellen: Mittelwerte der Scores pro Grid-Punkt\n",
    "            pvt = df.pivot_table(index=pivot_index, columns=pivot_columns, values=value_col, aggfunc=\"mean\")\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            im = plt.imshow(pvt.values, aspect=\"auto\", cmap=\"viridis\")\n",
    "            plt.colorbar(im)\n",
    "            \n",
    "            # Achsen beschriften\n",
    "            plt.yticks(range(len(pvt.index)), pvt.index)\n",
    "            plt.xticks(range(len(pvt.columns)), pvt.columns, rotation=45, ha=\"right\")\n",
    "            \n",
    "            plt.xlabel(\" / \".join(pivot_columns)); plt.ylabel(pivot_index)\n",
    "            plt.title(fname.replace(\"_\", \" \").replace(\".png\", \"\"))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(RUN_DIR / \"plots\" / fname, dpi=160)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Konnte Plot {fname} nicht erstellen: {e}\")\n",
    "\n",
    "    _plot_grid(agg, \"mcc_mean\",   \"heatmap_mcc.png\")\n",
    "    _plot_grid(agg, \"auprc_mean\", \"heatmap_auprc.png\")\n",
    "    print(\"Heatmaps gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103d010d-c556-4942-a47e-dc5f79ae5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxplots gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# === VISUALISIERUNG: BOXPLOTS ===\n",
    "# Boxplots zeigen die Verteilung der Ergebnisse \u00fcber die Folds.\n",
    "# Das ist wichtig, um die Stabilit\u00e4t (Varianz) zu beurteilen.\n",
    "\n",
    "if csv_path.exists():\n",
    "    results = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Wir erstellen ein kurzes Label f\u00fcr jede Config (f\u00fcr die X-Achse)\n",
    "    def _short_label(r):\n",
    "        return f\"{r['features_used']}-{r['cell']}-lb{int(r['lookback'])}-dp{r['dropout']}\"\n",
    "    \n",
    "    results[\"config_label\"] = results.apply(_short_label, axis=1)\n",
    "\n",
    "    # Wir nehmen nur die Top 10 Configs f\u00fcr den Plot, sonst wird es unleserlich\n",
    "    top_labels = results.groupby(\"config_label\")[\"mcc\"].mean().sort_values(ascending=False).head(10).index\n",
    "    subset = results[results[\"config_label\"].isin(top_labels)]\n",
    "    \n",
    "    # Daten f\u00fcr Plot vorbereiten\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data = [grp[\"mcc\"].values for label, grp in subset.groupby(\"config_label\")]\n",
    "    labels = [label for label, grp in subset.groupby(\"config_label\")]\n",
    "    \n",
    "    # Sortierung im Plot nach Median (h\u00f6chster links)\n",
    "    medians = [np.median(d) for d in data]\n",
    "    sort_idx = np.argsort(medians)[::-1]\n",
    "    data = [data[i] for i in sort_idx]\n",
    "    labels = [labels[i] for i in sort_idx]\n",
    "\n",
    "    # Zeichnen\n",
    "    plt.boxplot(data, showmeans=True, meanline=True)\n",
    "    plt.xticks(range(1, len(labels)+1), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Top 10 Configs: MCC Varianz \u00fcber Folds\")\n",
    "    plt.ylabel(\"MCC Score\")\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RUN_DIR / \"plots\" / \"boxplots_top10_mcc.png\", dpi=160)\n",
    "    plt.close()\n",
    "    print(\"Boxplots gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5581e69e-5b28-4224-9026-2872d2313683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 5 abgeschlossen. Ergebnisse in: ..\\results\\2026-01-03_20-56-59_wfcv\n"
     ]
    }
   ],
   "source": [
    "# === INFO-DUMP ===\n",
    "# Metadaten speichern\n",
    "run_info = {\n",
    "    \"seed\": SEED,\n",
    "    \"epochs_grid\": EPOCHS_GRID,\n",
    "    \"n_folds\": N_FOLDS,\n",
    "    \"val_frac\": 0.20,\n",
    "    \"min_train_frac\": 0.45,\n",
    "    \"lookback_grid\": LOOKBACK_GRID,\n",
    "    \"hp_grid_size\": (len(HP_GRID) if not FAST else 1),\n",
    "    \"feature_subsets\": list(FEATURE_SUBSETS.keys()),\n",
    "    \"train_csv\": TRAIN_CSV,\n",
    "    \"label_resolution\": {\n",
    "        \"source\": \"yaml\" if os.path.exists(yaml_path) and (label_h is not None) else \"inferred_from_csv\",\n",
    "        \"yaml_path\": yaml_path\n",
    "    },\n",
    "    \"labels\": {\"horizon\": H_FOR_FILE, \"mode\": MODE_FOR_FILE, \"epsilon\": EPS_FOR_FILE}\n",
    "}\n",
    "with open(RUN_DIR / \"wfcv_run_info.json\", \"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(\"\\nBlock 5 abgeschlossen. Ergebnisse in:\", RUN_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}