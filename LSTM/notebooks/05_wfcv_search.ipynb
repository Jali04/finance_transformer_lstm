{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "9701daf5-1f64-4226-9ca2-aa40821fc6b3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SYSTEM & IMPORTS ===\n",
                "# Block 5: Das Herzstück der Optimierung.\n",
                "# Hier führen wir eine \"Walk-Forward Cross-Validation\" durch.\n",
                "# Das bedeutet: Wir testen das Modell auf mehreren Zeiträumen, um sicherzustellen,\n",
                "# dass es nicht nur zufällig in einem Monat gut war.\n",
                "import os, sys, json, time, logging, glob, re\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "ROOT = os.path.abspath(\"..\")\n",
                "if ROOT not in sys.path:\n",
                "    sys.path.insert(0, ROOT)\n",
                "\n",
                "# TensorFlow soll nicht so viele Warnungen ausgeben\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
                "import tensorflow as tf\n",
                "tf.get_logger().setLevel(logging.ERROR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "db180739-46e5-44c1-a93d-6e99e9c3ff9a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WFCV_RUN_DIR: ..\\results\\2026-01-01_18-18-53_wfcv\n"
                    ]
                }
            ],
            "source": [
                "# === CONFIG & SETUP ===\n",
                "with open(os.path.join(ROOT, \"config.json\"), \"r\") as f:\n",
                "    C = json.load(f)\n",
                "\n",
                "TICKER, START, END, INTERVAL = C[\"ticker\"], C[\"start\"], C[\"end\"], C[\"interval\"]\n",
                "HORIZON  = int(C[\"horizon\"])\n",
                "LOOKBACK_DEFAULT = int(C[\"lookback\"])\n",
                "BATCH    = int(C.get(\"batch\", 64))\n",
                "SEED     = int(C.get(\"seed\", 42))\n",
                "FEATURESET = C.get(\"featureset\", \"v2\")\n",
                "EPS_MODE   = C.get(\"epsilon_mode\", \"abs\")\n",
                "EPSILON    = float(C.get(\"epsilon\", 0.0005))\n",
                "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\"))\n",
                "\n",
                "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
                "\n",
                "# Eigener Ordner für diesen Suchlauf\n",
                "RUN_DIR = RESULTS_DIR / time.strftime(\"%Y-%m-%d_%H-%M-%S_wfcv\")\n",
                "(RUN_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
                "print(\"WFCV_RUN_DIR:\", RUN_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "c55da04a-e118-4fc9-a5f2-339a4a1fbcd7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === FAST MODE ===\n",
                "# Option für schnelles Debugging (weniger Epochen, weniger Folds)\n",
                "FAST = C.get(\"fast_wfcv\", False)\n",
                "EPOCHS_GRID = 60\n",
                "N_FOLDS = 5\n",
                "if FAST:\n",
                "    EPOCHS_GRID = 25\n",
                "    N_FOLDS = 3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "b50b8c3e-f514-45e3-97c6-3f649cd47ea7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded TRAIN_CSV: ../data/AAPL_1d_2010-01-01_2026-01-01_cls_h1_abs0p0005.csv\n",
                        "Label pos_rate: 0.514 | n: 3991\n"
                    ]
                }
            ],
            "source": [
                "# === DATEN LADEN ===\n",
                "# Ähnliche Logik wie in den anderen Notebooks, um robuste Dateipfade zu finden\n",
                "import yaml\n",
                "\n",
                "yaml_path = f\"../data/features_{FEATURESET}.yml\"\n",
                "meta = {}\n",
                "label_h = label_mode = label_eps = None\n",
                "\n",
                "if os.path.exists(yaml_path):\n",
                "    with open(yaml_path, \"r\") as f:\n",
                "        meta = yaml.safe_load(f) or {}\n",
                "    lab = (meta or {}).get(\"label\", {})\n",
                "    label_h    = lab.get(\"horizon\", None)\n",
                "    label_mode = lab.get(\"mode\", None)\n",
                "    label_eps  = lab.get(\"epsilon\", None)\n",
                "\n",
                "def _parse_h_meps_from_name(path: str):\n",
                "    mH = re.search(r\"_cls_h(\\d+)_\", path)\n",
                "    me = re.search(r\"_(abs|rel)(\\d+p\\d+)\\.csv$\", path)\n",
                "    H  = int(mH.group(1)) if mH else None\n",
                "    md = me.group(1) if me else None\n",
                "    eps= float(me.group(2).replace(\"p\",\".\")) if me else None\n",
                "    return H, md, eps\n",
                "\n",
                "def _infer_from_existing_files(tkr, itv, start, end, mode_hint=None, eps_hint=None):\n",
                "    pat = f\"../data/{tkr}_{itv}_{start}_{end}_cls_h*_.csv\".replace(\"_ .csv\",\".csv\")\n",
                "    cands = sorted(glob.glob(pat), key=os.path.getmtime)\n",
                "    if mode_hint and (eps_hint is not None):\n",
                "        tag = f\"{mode_hint}{str(eps_hint).replace('.','p')}\"\n",
                "        cands = [c for c in cands if c.endswith(f\"_{tag}.csv\")]\n",
                "    if not cands:\n",
                "        return None\n",
                "    return _parse_h_meps_from_name(cands[-1])\n",
                "\n",
                "# Parameter bestimmen\n",
                "H_FOR_FILE    = int(label_h)    if label_h    is not None else None\n",
                "MODE_FOR_FILE = str(label_mode) if label_mode is not None else None\n",
                "EPS_FOR_FILE  = float(label_eps) if label_eps is not None else None\n",
                "\n",
                "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
                "    inferred = _infer_from_existing_files(TICKER, INTERVAL, START, END,\n",
                "                                          mode_hint=MODE_FOR_FILE, eps_hint=EPS_FOR_FILE)\n",
                "    if inferred is not None:\n",
                "        H_i, M_i, E_i = inferred\n",
                "        H_FOR_FILE    = H_FOR_FILE    if H_FOR_FILE    is not None else H_i\n",
                "        MODE_FOR_FILE = MODE_FOR_FILE if MODE_FOR_FILE is not None else M_i\n",
                "        EPS_FOR_FILE  = EPS_FOR_FILE  if EPS_FOR_FILE  is not None else E_i\n",
                "\n",
                "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
                "    raise RuntimeError(\"Label-Definition unklar. Bitte Block 2 prüfen.\")\n",
                "\n",
                "# Dateipfad bauen\n",
                "eps_tag   = f\"{MODE_FOR_FILE}{str(EPS_FOR_FILE).replace('.','p')}\"\n",
                "TRAIN_CSV = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{H_FOR_FILE}_{eps_tag}.csv\"\n",
                "\n",
                "if not os.path.exists(TRAIN_CSV):\n",
                "    # Fallback Suche nach Datei\n",
                "    pat = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h*_{eps_tag}.csv\"\n",
                "    candidates = sorted(glob.glob(pat), key=os.path.getmtime)\n",
                "    if candidates:\n",
                "        TRAIN_CSV = candidates[-1]\n",
                "    else:\n",
                "        raise FileNotFoundError(f\"CSV nicht gefunden: {TRAIN_CSV}\")\n",
                "\n",
                "print(\"Loaded TRAIN_CSV:\", TRAIN_CSV)\n",
                "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
                "\n",
                "# Alle möglichen Features laden\n",
                "OHLCV = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
                "if meta:\n",
                "    FEATURES_ALL = [c for c in meta.get(\"features\", []) if c in df.columns]\n",
                "else:\n",
                "    FEATURES_ALL = [c for c in df.columns if c not in (OHLCV | {\"target\"})]\n",
                "assert len(FEATURES_ALL) > 0, \"Keine Features gefunden.\"\n",
                "\n",
                "X_full = df[FEATURES_ALL].copy()\n",
                "y_full = df[\"target\"].astype(int).copy()\n",
                "\n",
                "print(\"Label pos_rate:\", round(y_full.mean(), 3), \"| n:\", len(y_full))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "408b2564-2d68-4706-838e-f3e1a499c7ac",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Anzahl Folds: 5\n",
                        "  Fold0 sizes (train/val): 1796 798\n"
                    ]
                }
            ],
            "source": [
                "# === SPLITTING (Walk-Forward) ===\n",
                "# Funktion teilt die Zeitreihe in wachsende Fenster (Folds).\n",
                "# Fold 1: Trainier auf [0...T], Test auf [T...T+V]\n",
                "# Fold 2: Trainier auf [0...T+V], Test auf [T+V...T+2V]\n",
                "# ... usw.\n",
                "def make_wf_splits(n, n_folds=5, val_frac=0.20, min_train_frac=0.45):\n",
                "    val_len   = max(60, int(round(n * val_frac)))\n",
                "    min_train = max(200, int(round(n * min_train_frac)))\n",
                "    start_val_end = min_train + val_len\n",
                "    if start_val_end + 1 > n:\n",
                "        raise ValueError(f\"Dataset zu kurz: {n}\")\n",
                "    \n",
                "    # Endpunkte der Validation-Sets bestimmen\n",
                "    val_ends = np.linspace(start_val_end, n, num=n_folds, endpoint=True).astype(int)\n",
                "    val_ends = np.unique(val_ends)\n",
                "    \n",
                "    # Wenn zu wenige Punkte, dann fixe Schritte nehmen\n",
                "    if len(val_ends) < n_folds:\n",
                "        step = max(1, (n - start_val_end) // n_folds)\n",
                "        val_ends = np.arange(start_val_end, start_val_end + step * n_folds, step)\n",
                "        val_ends = np.clip(val_ends, start_val_end, n)\n",
                "        \n",
                "    stops = []\n",
                "    for ve in val_ends[:n_folds]:\n",
                "        te = int(ve - val_len)\n",
                "        te = max(te, LOOKBACK_DEFAULT + 1)\n",
                "        if te <= 0 or ve <= te or ve > n:\n",
                "            continue\n",
                "        # (Train-Slice, Valid-Slice)\n",
                "        stops.append((slice(0, te), slice(te, ve)))\n",
                "        \n",
                "    if len(stops) != n_folds:\n",
                "        raise RuntimeError(f\"Erzeugte nur {len(stops)} von {n_folds} Folds.\")\n",
                "    return stops\n",
                "\n",
                "n = len(df)\n",
                "splits = make_wf_splits(n, n_folds=N_FOLDS, val_frac=0.20, min_train_frac=0.45)\n",
                "print(\"Anzahl Folds:\", len(splits))\n",
                "if len(splits) > 0:\n",
                "    tr_s, va_s = splits[0]\n",
                "    print(\"  Fold0 sizes (train/val):\", tr_s.stop, va_s.stop - va_s.start)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "7faef79f-6bf2-4968-b014-0638ef49ff10",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === MODELL-HELPER ===\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
                "from tensorflow.keras import layers, regularizers, callbacks, optimizers, models\n",
                "\n",
                "# 3D-Daten für RNN bauen\n",
                "def make_windows(X_df, y_ser, lookback):\n",
                "    Xv = X_df.values.astype(np.float32)\n",
                "    yv = y_ser.values.astype(np.int32)\n",
                "    xs, ys = [], []\n",
                "    for i in range(lookback-1, len(X_df)):\n",
                "        xs.append(Xv[i - lookback + 1 : i + 1])\n",
                "        ys.append(yv[i])\n",
                "    return np.stack(xs, axis=0), np.array(ys)\n",
                "\n",
                "# Modellarchitektur generieren\n",
                "def build_model(n_features, width1=64, width2=32, dropout=0.10, lr=5e-4, use_gru=True):\n",
                "    rnn = layers.GRU if use_gru else layers.LSTM\n",
                "    m = models.Sequential([\n",
                "        layers.Input(shape=(None, n_features)),\n",
                "        rnn(width1, return_sequences=True, recurrent_dropout=dropout),\n",
                "        layers.LayerNormalization(),\n",
                "        rnn(width2, recurrent_dropout=dropout),\n",
                "        layers.LayerNormalization(),\n",
                "        layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-5)),\n",
                "        layers.Dense(1, activation=\"sigmoid\"),\n",
                "    ])\n",
                "    m.compile(\n",
                "        optimizer=optimizers.Adam(learning_rate=lr),\n",
                "        loss=\"binary_crossentropy\",\n",
                "        metrics=[tf.keras.metrics.AUC(name=\"auc\"),\n",
                "                 tf.keras.metrics.AUC(name=\"auprc\", curve=\"PR\")]\n",
                "    )\n",
                "    return m\n",
                "\n",
                "# MCC maximieren: Sucht besten Threshold auf Validation\n",
                "def mcc_at_best_thr(y_true, y_prob):\n",
                "    ts = np.r_[0.0, np.unique(y_prob), 1.0]\n",
                "    best = (-1.0, 0.5)\n",
                "    for t in ts:\n",
                "        m = matthews_corrcoef(y_true, (y_prob >= t).astype(int))\n",
                "        if m > best[0]:\n",
                "            best = (float(m), float(t))\n",
                "    return best  # (mcc, thr)\n",
                "\n",
                "# Training eines einzelnen Folds\n",
                "def fit_eval_fold(X_tr, y_tr, X_va, y_va, lookback, hp, seed=SEED, batch=BATCH, epochs=EPOCHS_GRID):\n",
                "    tf.keras.backend.clear_session()\n",
                "\n",
                "    # Scaler nur auf Train fitten!\n",
                "    scaler = StandardScaler()\n",
                "    X_tr_s = pd.DataFrame(scaler.fit_transform(X_tr), index=X_tr.index, columns=X_tr.columns)\n",
                "    X_va_s = pd.DataFrame(scaler.transform(X_va),     index=X_va.index, columns=X_va.columns)\n",
                "\n",
                "    Xtr_win, ytr = make_windows(X_tr_s, y_tr, lookback)\n",
                "    Xva_win, yva = make_windows(X_va_s, y_va, lookback)\n",
                "\n",
                "    # Datasets erstellen\n",
                "    ds_tr = tf.data.Dataset.from_tensor_slices((Xtr_win, ytr)) \\\n",
                "            .shuffle(len(Xtr_win), seed=seed) \\\n",
                "            .batch(batch).prefetch(tf.data.AUTOTUNE)\n",
                "    ds_va = tf.data.Dataset.from_tensor_slices((Xva_win, yva)) \\\n",
                "            .batch(batch).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "    tf.keras.utils.set_random_seed(seed)\n",
                "    model = build_model(\n",
                "        n_features=Xtr_win.shape[-1],\n",
                "        width1=hp[\"width1\"], width2=hp[\"width2\"],\n",
                "        dropout=hp[\"dropout\"], lr=hp[\"lr\"], use_gru=(hp[\"cell\"]==\"GRU\")\n",
                "    )\n",
                "\n",
                "    # Callbacks (Early Stopping, Learning Rate Reduction)\n",
                "    cbs = [\n",
                "        callbacks.EarlyStopping(monitor=\"val_auprc\", mode=\"max\", patience=6, restore_best_weights=True),\n",
                "        callbacks.ReduceLROnPlateau(monitor=\"val_auprc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5),\n",
                "        callbacks.TerminateOnNaN(),\n",
                "    ]\n",
                "\n",
                "    print(f\"  -> fit (epochs={epochs}, batch={batch}) ...\", flush=True)\n",
                "    hist = model.fit(ds_tr, validation_data=ds_va, epochs=epochs, verbose=1 if FAST else 0, callbacks=cbs)\n",
                "\n",
                "    # Evaluation\n",
                "    yva_proba = model.predict(ds_va, verbose=0).ravel()\n",
                "    mcc_val, thr_val = mcc_at_best_thr(yva, yva_proba)   # Beste Performance wählen\n",
                "    yva_pred_best = (yva_proba >= thr_val).astype(int)\n",
                "\n",
                "    metrics = dict(\n",
                "        mcc=float(mcc_val),\n",
                "        thr_val=float(thr_val),\n",
                "        bal_acc=float(balanced_accuracy_score(yva, yva_pred_best)),\n",
                "        auprc=float(average_precision_score(yva, yva_proba)),\n",
                "        auroc=float(roc_auc_score(yva, yva_proba)),\n",
                "        epochs_trained=int(len(hist.history[\"loss\"]))\n",
                "    )\n",
                "    tf.keras.backend.clear_session()\n",
                "    return metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "337cbd2c-95af-41f5-9be0-dd323baf6fdb",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sanity check vor der Suche:\n",
                        "  HP_GRID size: 24\n",
                        "  #splits: 5\n"
                    ]
                }
            ],
            "source": [
                "# === SUCH-GRIDS DEFINIEREN ===\n",
                "# Hier legen wir fest, welche Parameter-Kombinationen getestet werden sollen.\n",
                "LOOKBACK_GRID = [40, 60, 120] if not FAST else [LOOKBACK_DEFAULT]\n",
                "HP_GRID = [\n",
                "    dict(width1=w1, width2=w2, dropout=dp, lr=lr, cell=cell)\n",
                "    for (w1, w2) in [(32,16), (64,32)]\n",
                "    for lr in [5e-4, 3e-4]\n",
                "    for dp in [0.0, 0.1, 0.2]\n",
                "    for cell in [\"GRU\", \"LSTM\"]\n",
                "] if not FAST else [dict(width1=32, width2=16, dropout=0.10, lr=5e-4, cell=\"GRU\")]\n",
                "\n",
                "# Verschiedene Feature-Sets testen (z.B. nur Momentum? Oder auch Volatilität?)\n",
                "FEATURE_SUBSETS = {\n",
                "    \"all\": FEATURES_ALL,\n",
                "    \"mom_only\": [c for c in FEATURES_ALL\n",
                "                 if (\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"})],\n",
                "    \"mom+vol\": [c for c in FEATURES_ALL\n",
                "                if ((\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"}))\n",
                "                   or (c in {\"realized_vol_10\",\"vol_z_20\"})]\n",
                "}\n",
                "\n",
                "print(\"Sanity check vor der Suche:\")\n",
                "print(\"  HP_GRID size:\", len(HP_GRID))\n",
                "print(\"  #splits:\", len(splits))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "42408c93-f83a-4abc-ab98-1d0279037343",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starte Suche ...\n",
                        "  -> fit (epochs=25, batch=128) ...\n",
                        "[mom+vol | LB=60 | Fold1] MCC=0.120\n",
                        "  -> fit (epochs=25, batch=128) ...\n",
                        "[mom+vol | LB=60 | Fold2] MCC=0.096\n",
                        "  -> fit (epochs=25, batch=128) ...\n",
                        "[mom+vol | LB=60 | Fold3] MCC=0.119\n",
                        "\n",
                        "Suche fertig. Dauer=32.0s\n"
                    ]
                }
            ],
            "source": [
                "# === SUCHE DURCHFÜHREN ===\n",
                "from time import perf_counter\n",
                "\n",
                "print(\"Starte Suche ...\", flush=True)\n",
                "\n",
                "# Overrides für schnelle Tests innerhalb dieser Zelle\n",
                "FAST_PATCH     = True\n",
                "MAX_SECONDS    = 60 * 60  # Max 1 Stunde Laufzeit\n",
                "EPOCHS_FAST    = 25\n",
                "NFOLDS_FAST    = 3\n",
                "BATCH_FAST     = 128\n",
                "\n",
                "# Fallback Subsets für Fast Patch\n",
                "FEATURE_SUBSETS_FAST = {}\n",
                "if \"mom+vol\" in FEATURE_SUBSETS:\n",
                "    FEATURE_SUBSETS_FAST[\"mom+vol\"] = FEATURE_SUBSETS[\"mom+vol\"]\n",
                "elif \"mom_only\" in FEATURE_SUBSETS:\n",
                "    FEATURE_SUBSETS_FAST[\"mom_only\"] = FEATURE_SUBSETS[\"mom_only\"]\n",
                "else:\n",
                "    k0 = next(iter(FEATURE_SUBSETS.keys()))\n",
                "    FEATURE_SUBSETS_FAST[k0] = FEATURE_SUBSETS[k0]\n",
                "\n",
                "LOOKBACK_GRID_FAST = [60]\n",
                "HP_GRID_FAST = [dict(width1=32, width2=16, dropout=0.10, lr=5e-4, cell=\"GRU\")]\n",
                "\n",
                "# Patch anwenden\n",
                "if FAST_PATCH:\n",
                "    EPOCHS_GRID = EPOCHS_FAST\n",
                "    splits = splits[:min(NFOLDS_FAST, len(splits))]\n",
                "    FEATURE_SUBSETS = FEATURE_SUBSETS_FAST\n",
                "    LOOKBACK_GRID = LOOKBACK_GRID_FAST\n",
                "    HP_GRID = HP_GRID_FAST\n",
                "\n",
                "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
                "records = []\n",
                "total_combos = 0\n",
                "t0 = perf_counter()\n",
                "\n",
                "# Resume-Logic: Wenn CSV existiert, schon berechnete überspringen\n",
                "if csv_path.exists():\n",
                "    done_df = pd.read_csv(csv_path)\n",
                "    done_keys = {\n",
                "        (r[\"features_used\"], int(r[\"lookback\"]),\n",
                "         r[\"cell\"], int(r[\"width1\"]), int(r[\"width2\"]),\n",
                "         float(r[\"dropout\"]), float(r[\"lr\"]), int(r[\"fold\"]))\n",
                "        for _, r in done_df.iterrows()\n",
                "    }\n",
                "else:\n",
                "    done_df = pd.DataFrame()\n",
                "    done_keys = set()\n",
                "\n",
                "stop_time = t0 + MAX_SECONDS\n",
                "\n",
                "# HAUPTSCHLEIFE\n",
                "for feat_name, FEATS in FEATURE_SUBSETS.items():\n",
                "    if len(FEATS) == 0: continue\n",
                "\n",
                "    for lookback in LOOKBACK_GRID:\n",
                "        for hp in HP_GRID:\n",
                "            total_combos += 1\n",
                "            ran_folds = 0\n",
                "\n",
                "            for fold_id, (tr_s, va_s) in enumerate(splits, start=1):\n",
                "                key = (feat_name, int(lookback),\n",
                "                       hp[\"cell\"], int(hp[\"width1\"]), int(hp[\"width2\"]),\n",
                "                       float(hp[\"dropout\"]), float(hp[\"lr\"]), int(fold_id))\n",
                "                if key in done_keys:\n",
                "                    ran_folds += 1\n",
                "                    continue\n",
                "\n",
                "                # Zeitbudget check\n",
                "                if perf_counter() > stop_time:\n",
                "                    print(\"[INFO] Zeitbudget erreicht — speichere & beende.\")\n",
                "                    out_df = pd.concat([done_df, pd.DataFrame.from_records(records)], ignore_index=True) if csv_path.exists() else pd.DataFrame.from_records(records)\n",
                "                    out_df.to_csv(csv_path, index=False)\n",
                "                    raise SystemExit(0)\n",
                "\n",
                "                # Daten holen\n",
                "                X_tr, y_tr = X_full.iloc[tr_s][FEATS], y_full.iloc[tr_s]\n",
                "                X_va, y_va = X_full.iloc[va_s][FEATS], y_full.iloc[va_s]\n",
                "\n",
                "                # Training\n",
                "                mets = fit_eval_fold(X_tr, y_tr, X_va, y_va,\n",
                "                                     lookback=lookback, hp=hp,\n",
                "                                     epochs=EPOCHS_GRID, batch=BATCH_FAST)\n",
                "\n",
                "                ran_folds += 1\n",
                "                rec = {\n",
                "                    \"feature_set\": FEATURESET,\n",
                "                    \"features_used\": feat_name,\n",
                "                    \"n_features\": len(FEATS),\n",
                "                    \"lookback\": lookback,\n",
                "                    **hp,\n",
                "                    \"fold\": fold_id,\n",
                "                    **mets\n",
                "                }\n",
                "                records.append(rec)\n",
                "\n",
                "                # Inkrementell speichern\n",
                "                cur_df = pd.DataFrame.from_records(records)\n",
                "                out_df = pd.concat([done_df, cur_df], ignore_index=True)\n",
                "                out_df.to_csv(csv_path, index=False)\n",
                "                done_df = out_df\n",
                "\n",
                "                print(f\"[{feat_name} | LB={lookback} | Fold{fold_id}] MCC={mets['mcc']:.3f}\")\n",
                "\n",
                "t1 = perf_counter()\n",
                "print(f\"\\nSuche fertig. Dauer={t1-t0:.1f}s\")\n",
                "\n",
                "# Final speichern\n",
                "final_df = pd.read_csv(csv_path) if csv_path.exists() else pd.DataFrame.from_records(records)\n",
                "final_df.to_csv(csv_path, index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "2afca6e0-5c56-45c9-93a0-066fc19b613c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best config: {'feature_set': 'v2', 'features_used': 'mom+vol', 'n_features': 11, 'lookback': 60, 'width1': 32, 'width2': 16, 'dropout': 0.1, 'lr': 0.0005, 'cell': 'GRU', 'mcc_mean': 0.11192501701210433, 'mcc_std': 0.013737426269908473, 'auprc_mean': 0.5558566616457216, 'auprc_std': 0.02613838967568943, 'auroc_mean': 0.5252876312581236}\n"
                    ]
                }
            ],
            "source": [
                "# === ERGEBNIS-ANALYSE ===\n",
                "# Wir aggregieren die Ergebnisse aller Folds und suchen die beste Konfiguration\n",
                "import pandas as pd, json, numpy as np\n",
                "\n",
                "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
                "results = pd.read_csv(csv_path)\n",
                "\n",
                "# Duplikate entfernen\n",
                "key_cols = [\"features_used\",\"lookback\",\"cell\",\"width1\",\"width2\",\"dropout\",\"lr\",\"fold\"]\n",
                "present_keys = [c for c in key_cols if c in results.columns]\n",
                "results = results.drop_duplicates(subset=present_keys, keep=\"last\").reset_index(drop=True)\n",
                "\n",
                "# Gruppieren & Mittelwerte berechnen\n",
                "agg_cols = [c for c in [\"feature_set\",\"features_used\",\"n_features\",\"lookback\",\n",
                "                        \"width1\",\"width2\",\"dropout\",\"lr\",\"cell\"] if c in results.columns]\n",
                "\n",
                "agg_dict = {\"mcc\": [\"mean\",\"std\"], \"auprc\": [\"mean\",\"std\"], \"auroc\": [\"mean\"]}\n",
                "g = results.groupby(agg_cols).agg(agg_dict)\n",
                "\n",
                "# Flache Spaltennamen erzeugen\n",
                "g.columns = [\n",
                "    \"_\".join([str(x) for x in col if str(x) != \"\"]).strip(\"_\")\n",
                "    for col in g.columns.to_flat_index()\n",
                "]\n",
                "g = g.reset_index()\n",
                "\n",
                "# Sortieren: Beste Konfiguration zuerst (nach MCC Mean)\n",
                "g = g.sort_values([\"mcc_mean\",\"auprc_mean\",\"mcc_std\"], ascending=[False, False, True])\n",
                "\n",
                "# Speichern\n",
                "(g).to_csv(RUN_DIR / \"wfcv_results_agg.csv\", index=False)\n",
                "top5 = g.head(5).copy()\n",
                "top5.to_csv(RUN_DIR / \"wfcv_results_top5.csv\", index=False)\n",
                "\n",
                "# Beste Config als JSON exportieren (für Step 3: Training)\n",
                "best = top5.iloc[0].to_dict() if len(top5) else {}\n",
                "with open(RUN_DIR / \"best_config.json\", \"w\") as f:\n",
                "    json.dump(best, f, indent=2)\n",
                "\n",
                "print(\"Best config:\", best)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "a79f67d2-a756-442f-8aaa-047113714208",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Grids gespeichert.\n"
                    ]
                }
            ],
            "source": [
                "# === VISUALISIERUNG: HEATMAPS ===\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "agg_path = RUN_DIR / \"wfcv_results_agg.csv\"\n",
                "agg = pd.read_csv(agg_path)\n",
                "\n",
                "pivot_index = \"lookback\" if \"lookback\" in agg.columns else None\n",
                "col_candidates = [\"features_used\", \"cell\", \"width1\"]\n",
                "pivot_columns = [c for c in col_candidates if c in agg.columns]\n",
                "if not pivot_columns: pivot_columns = [agg.columns[0]]\n",
                "\n",
                "(RUN_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "def _plot_grid(df: pd.DataFrame, value_col: str, fname: str):\n",
                "    if value_col not in df.columns: return\n",
                "    idx = pivot_index or pivot_columns[0]\n",
                "    pvt = df.pivot_table(index=idx, columns=pivot_columns, values=value_col, aggfunc=\"mean\")\n",
                "\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    im = plt.imshow(pvt.values, aspect=\"auto\")\n",
                "    plt.colorbar(im)\n",
                "    plt.title(fname.replace(\"_\", \" \").replace(\".png\", \"\"))\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(RUN_DIR / \"plots\" / fname, dpi=160)\n",
                "    plt.close()\n",
                "\n",
                "_plot_grid(agg, \"mcc_mean\",   \"score_grid_mcc.png\")\n",
                "_plot_grid(agg, \"auprc_mean\", \"score_grid_auprc.png\")\n",
                "print(\"Grids gespeichert.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "103d010d-c556-4942-a47e-dc5f79ae5796",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === VISUALISIERUNG: BOXPLOTS ===\n",
                "# Zeigt die Streuung der Leistung über die verschiedenen Folds\n",
                "def _short_label(r):\n",
                "    return f\"{r['features_used']}-{r['cell']}-{int(r['width1'])}/{int(r['width2'])}-lb{int(r['lookback'])}-dp{r['dropout']}-lr{r['lr']}\"\n",
                "\n",
                "if not results.empty:\n",
                "    results[\"config_label\"] = results.apply(_short_label, axis=1)\n",
                "\n",
                "    # MCC Plot\n",
                "    plt.figure(figsize=(max(8, 0.35*len(results[\"config_label\"].unique())), 5))\n",
                "    data = [grp[\"mcc\"].values for _, grp in results.groupby(\"config_label\")]\n",
                "    labels = list(results.groupby(\"config_label\").groups.keys())\n",
                "    plt.boxplot(data, showmeans=True, meanline=True)\n",
                "    plt.xticks(range(1, len(labels)+1), labels, rotation=90)\n",
                "    plt.title(\"MCC über Folds (pro Konfiguration)\")\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(RUN_DIR / \"plots\" / \"boxplots_mcc.png\", dpi=160)\n",
                "    plt.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "5581e69e-5b28-4224-9026-2872d2313683",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Block 5 abgeschlossen. Ergebnisse in: ..\\results\\2026-01-01_18-18-53_wfcv\n"
                    ]
                }
            ],
            "source": [
                "# === INFO-DUMP ===\n",
                "# Speichert Metadaten über den Lauf\n",
                "run_info = {\n",
                "    \"seed\": SEED,\n",
                "    \"epochs_grid\": EPOCHS_GRID,\n",
                "    \"n_folds\": N_FOLDS,\n",
                "    \"val_frac\": 0.20,\n",
                "    \"min_train_frac\": 0.45,\n",
                "    \"lookback_grid\": LOOKBACK_GRID,\n",
                "    \"hp_grid_size\": (len(HP_GRID) if not FAST else 1),\n",
                "    \"feature_subsets\": list(FEATURE_SUBSETS.keys()),\n",
                "    \"train_csv\": TRAIN_CSV,\n",
                "    \"label_resolution\": {\n",
                "        \"source\": \"yaml\" if os.path.exists(yaml_path) and (label_h is not None) else \"inferred_from_csv\",\n",
                "        \"yaml_path\": yaml_path\n",
                "    },\n",
                "    \"labels\": {\"horizon\": H_FOR_FILE, \"mode\": MODE_FOR_FILE, \"epsilon\": EPS_FOR_FILE}\n",
                "}\n",
                "with open(RUN_DIR / \"wfcv_run_info.json\", \"w\") as f:\n",
                "    json.dump(run_info, f, indent=2)\n",
                "\n",
                "print(\"\\nBlock 5 abgeschlossen. Ergebnisse in:\", RUN_DIR)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
