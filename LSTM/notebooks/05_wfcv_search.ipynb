{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "9701daf5-1f64-4226-9ca2-aa40821fc6b3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TensorFlow Version: 2.10.0\n",
                        "‚úÖ GPU DETECTED: 1 device(s)\n",
                        "  [0] /physical_device:GPU:0\n",
                        "      Compute Capability: (8, 9)\n",
                        "üöÄ Mixed Precision ENABLED (Float16 speedup active)\n"
                    ]
                }
            ],
            "source": [
                "# === SYSTEM & IMPORTS ===\n",
                "# Block 5: Hyperparameter-Optimierung mit Walk-Forward Cross-Validation (WFCV)\n",
                "#\n",
                "# Ziel: Die besten Parameter f√ºr das Modell finden, ohne \"in die Zukunft\" zu schauen.\n",
                "# Methode: Wir trainieren auf [Vergangenheit] -> testen auf [Gegenwart].\n",
                "# Dann schieben wir das Fenster weiter: trainieren auf [Vergangenheit + Gegenwart] -> testen auf [Zukunft].\n",
                "\n",
                "import os, sys, json, time, logging, glob, re\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Pfad zum Projekt-Root setzen\n",
                "ROOT = os.path.abspath(\"..\")\n",
                "if ROOT not in sys.path:\n",
                "    sys.path.insert(0, ROOT)\n",
                "\n",
                "# TensorFlow-Logs unterdr√ºcken (nur Fehler anzeigen)\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
                "import tensorflow as tf\n",
                "tf.get_logger().setLevel(logging.ERROR)\n",
                "\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    print(f\"‚úÖ GPU DETECTED: {len(gpus)} device(s)\")\n",
                "    for i, gpu in enumerate(gpus):\n",
                "        print(f\"  [{i}] {gpu.name}\")\n",
                "        try:\n",
                "            # Versuch, Speicher-Details zu holen (optional)\n",
                "            details = tf.config.experimental.get_device_details(gpu)\n",
                "            print(f\"      Compute Capability: {details.get('compute_capability')}\")\n",
                "        except:\n",
                "            pass\n",
                "    # Mixed Precision aktivieren\n",
                "    try:\n",
                "        from tensorflow.keras import mixed_precision\n",
                "        policy = mixed_precision.Policy('mixed_float16')\n",
                "        mixed_precision.set_global_policy(policy)\n",
                "        print(\"üöÄ Mixed Precision ENABLED (Float16 speedup active)\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Mixed Precision init failed: {e}\")\n",
                "else:\n",
                "    print(\"‚ùå NO GPU DETECTED! Running on CPU (will be slow).\")\n",
                "    print(\"   Please check CUDA/cuDNN installation if you have an NVIDIA GPU.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "db180739-46e5-44c1-a93d-6e99e9c3ff9a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WFCV_RUN_DIR: ..\\results\\2026-01-03_20-56-59_wfcv\n"
                    ]
                }
            ],
            "source": [
                "# === CONFIG & SETUP ===\n",
                "# Basis-Konfiguration laden\n",
                "with open(os.path.join(ROOT, \"config.json\"), \"r\") as f:\n",
                "    C = json.load(f)\n",
                "\n",
                "# Parameter √ºbernehmen\n",
                "TICKER   = C[\"ticker\"]\n",
                "START    = C[\"start\"]\n",
                "END      = C[\"end\"]\n",
                "INTERVAL = C[\"interval\"]\n",
                "HORIZON  = int(C[\"horizon\"])\n",
                "LOOKBACK_DEFAULT = int(C[\"lookback\"]) # Standard-Lookback, falls wir ihn nicht variieren\n",
                "BATCH    = int(C.get(\"batch\", 64))\n",
                "SEED     = int(C.get(\"seed\", 42))\n",
                "FEATURESET = C.get(\"featureset\", \"v2\")\n",
                "EPS_MODE   = C.get(\"epsilon_mode\", \"abs\")\n",
                "EPSILON    = float(C.get(\"epsilon\", 0.0005))\n",
                "\n",
                "# Ergebnis-Verzeichnis\n",
                "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\"))\n",
                "\n",
                "# Globalen Seed setzen\n",
                "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
                "\n",
                "# Eigener Ausgabe-Ordner mit Zeitstempel f√ºr diesen WFCV-Lauf\n",
                "RUN_DIR = RESULTS_DIR / time.strftime(\"%Y-%m-%d_%H-%M-%S_wfcv\")\n",
                "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Unterordner f√ºr Plots\n",
                "(RUN_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"WFCV_RUN_DIR:\", RUN_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "c55da04a-e118-4fc9-a5f2-339a4a1fbcd7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === FAST MODE ===\n",
                "# WFCV kann sehr lange dauern (Stunden/Tage).\n",
                "# F√ºr Debugging oder schnelle Tests gibt es den \"Fast Mode\".\n",
                "FAST = C.get(\"fast_wfcv\", False)\n",
                "\n",
                "# Einstellungen f√ºr \"Normal\" (Full Grid) und \"Fast\" (Reduziert)\n",
                "EPOCHS_GRID = 1   # Set to 1 for Instant Mode (<1 min)\n",
                "N_FOLDS = 2       # Min Folds\n",
                "\n",
                "if FAST:\n",
                "    print(\"[INFO] Fast Mode ist AKTIV. Reduzierte Epochen und Folds.\")\n",
                "EPOCHS_GRID = 1   # Set to 1 for Instant Mode (<1 min)\n",
                "N_FOLDS = 2       # Min Folds\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "b50b8c3e-f514-45e3-97c6-3f649cd47ea7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Lade TRAIN_CSV: ../data/AAPL_1d_2010-01-01_2026-01-01_cls_h1_abs0p0005.csv\n",
                        "Label Positive Rate (gesamt): 0.514 | Datens√§tze: 3991\n"
                    ]
                }
            ],
            "source": [
                "# === DATEN LADEN ===\n",
                "# Wir suchen robust nach der passenden CSV-Datei.\n",
                "import yaml\n",
                "\n",
                "# 1. Features-Metadaten laden, falls vorhanden\n",
                "yaml_path = f\"../data/features_{FEATURESET}.yml\"\n",
                "meta = {}\n",
                "label_h = label_mode = label_eps = None\n",
                "\n",
                "if os.path.exists(yaml_path):\n",
                "    with open(yaml_path, \"r\") as f:\n",
                "        meta = yaml.safe_load(f) or {}\n",
                "    lab = (meta or {}).get(\"label\", {})\n",
                "    label_h    = lab.get(\"horizon\")\n",
                "    label_mode = lab.get(\"mode\")\n",
                "    label_eps  = lab.get(\"epsilon\")\n",
                "\n",
                "# Hilfsfunktion zum Parsen des Dateinamens\n",
                "def _parse_h_meps_from_name(path: str):\n",
                "    mH = re.search(r\"_cls_h(\\d+)_\", path)\n",
                "    me = re.search(r\"_(abs|rel)(\\d+p\\d+)\\.csv$\", path)\n",
                "    H  = int(mH.group(1)) if mH else None\n",
                "    md = me.group(1) if me else None\n",
                "    eps= float(me.group(2).replace(\"p\",\".\")) if me else None\n",
                "    return H, md, eps\n",
                "\n",
                "# Hilfsfunktion zur Suche im Dateisystem\n",
                "def _infer_from_existing_files(tkr, itv, start, end, mode_hint=None, eps_hint=None):\n",
                "    pat = f\"../data/{tkr}_{itv}_{start}_{end}_cls_h*_.csv\".replace(\"_ .csv\",\".csv\")\n",
                "    cands = sorted(glob.glob(pat), key=os.path.getmtime)\n",
                "    \n",
                "    if mode_hint and (eps_hint is not None):\n",
                "        tag = f\"{mode_hint}{str(eps_hint).replace('.','p')}\"\n",
                "        cands = [c for c in cands if c.endswith(f\"_{tag}.csv\")]\n",
                "        \n",
                "    if not cands:\n",
                "        return None\n",
                "    return _parse_h_meps_from_name(cands[-1])\n",
                "\n",
                "# Versuch 1: Parameter aus YAML\n",
                "H_FOR_FILE    = int(label_h)    if label_h    is not None else None\n",
                "MODE_FOR_FILE = str(label_mode) if label_mode is not None else None\n",
                "EPS_FOR_FILE  = float(label_eps) if label_eps is not None else None\n",
                "\n",
                "# Versuch 2: Parameter aus Dateinamen erraten\n",
                "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
                "    inferred = _infer_from_existing_files(TICKER, INTERVAL, START, END,\n",
                "                                          mode_hint=MODE_FOR_FILE, eps_hint=EPS_FOR_FILE)\n",
                "    if inferred is not None:\n",
                "        H_i, M_i, E_i = inferred\n",
                "        H_FOR_FILE    = H_FOR_FILE    if H_FOR_FILE    is not None else H_i\n",
                "        MODE_FOR_FILE = MODE_FOR_FILE if MODE_FOR_FILE is not None else M_i\n",
                "        EPS_FOR_FILE  = EPS_FOR_FILE  if EPS_FOR_FILE  is not None else E_i\n",
                "\n",
                "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
                "    raise RuntimeError(\"Label-Definition unklar. Bitte Block 2 pr√ºfen.\")\n",
                "\n",
                "# Dateipfad endg√ºltig bauen\n",
                "eps_tag   = f\"{MODE_FOR_FILE}{str(EPS_FOR_FILE).replace('.','p')}\"\n",
                "TRAIN_CSV = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{H_FOR_FILE}_{eps_tag}.csv\"\n",
                "\n",
                "# Existenz checken und laden\n",
                "if not os.path.exists(TRAIN_CSV):\n",
                "    pat = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h*_{eps_tag}.csv\"\n",
                "    candidates = sorted(glob.glob(pat), key=os.path.getmtime)\n",
                "    if candidates:\n",
                "        TRAIN_CSV = candidates[-1]\n",
                "    else:\n",
                "        raise FileNotFoundError(f\"CSV nicht gefunden: {TRAIN_CSV}\")\n",
                "\n",
                "print(\"Lade TRAIN_CSV:\", TRAIN_CSV)\n",
                "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
                "\n",
                "# Feature-Pool bestimmen\n",
                "OHLCV = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
                "if meta:\n",
                "    FEATURES_ALL = [c for c in meta.get(\"features\", []) if c in df.columns]\n",
                "else:\n",
                "    FEATURES_ALL = [c for c in df.columns if c not in (OHLCV | {\"target\"})]\n",
                "    \n",
                "assert len(FEATURES_ALL) > 0, \"Keine Features zum Optimieren gefunden.\"\n",
                "\n",
                "# Wir behalten den ganzen DataFrame im Speicher (X und y getrennt)\n",
                "X_full = df[FEATURES_ALL].copy()\n",
                "y_full = df[\"target\"].astype(int).copy()\n",
                "\n",
                "print(\"Label Positive Rate (gesamt):\", round(y_full.mean(), 3), \"| Datens√§tze:\", len(y_full))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "408b2564-2d68-4706-838e-f3e1a499c7ac",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Anzahl generierter Folds: 2\n",
                        "  Fold1: Train bis idx=1796, Val bis idx=2594 (Gr√∂√üe Val: 798)\n"
                    ]
                }
            ],
            "source": [
                "# === SPLITTING (Walk-Forward Logik) ===\n",
                "# Diese Funktion berechnet die Indizes f√ºr die verschiedenen Folds.\n",
                "\n",
                "def make_wf_splits(n, n_folds=5, val_frac=0.20, min_train_frac=0.45):\n",
                "    # n: Anzahl Datenpunkte total\n",
                "    # val_frac: Wie viel % des aktuellen Fensters sind Validation?\n",
                "    # min_train_frac: Wie gro√ü muss das Trainingset MINDESTENS sein?\n",
                "    \n",
                "    val_len   = max(60, int(round(n * val_frac)))\n",
                "    min_train = max(200, int(round(n * min_train_frac)))\n",
                "    \n",
                "    # Start-Punkt f√ºr das Ende des ersten Validation-Sets\n",
                "    start_val_end = min_train + val_len\n",
                "    if start_val_end + 1 > n:\n",
                "        raise ValueError(f\"Dataset zu kurz f√ºr diese Split-Parameter: {n}\")\n",
                "    \n",
                "    # Wir verteilen die Endpunkte der Folds gleichm√§√üig √ºber die verbleibende Zeit\n",
                "    val_ends = np.linspace(start_val_end, n, num=n_folds, endpoint=True).astype(int)\n",
                "    val_ends = np.unique(val_ends)\n",
                "    \n",
                "    # Falls durch Rundung zu wenige Folds entstehen (bei kleinen Daten), fixieren wir Schritte\n",
                "    if len(val_ends) < n_folds:\n",
                "        step = max(1, (n - start_val_end) // n_folds)\n",
                "        val_ends = np.arange(start_val_end, start_val_end + step * n_folds, step)\n",
                "        val_ends = np.clip(val_ends, start_val_end, n)\n",
                "        \n",
                "    stops = []\n",
                "    for ve in val_ends[:n_folds]:\n",
                "        # Das Ende des Trainings ist 'val_len' vor dem Ende des Folds\n",
                "        te = int(ve - val_len)\n",
                "        # Training muss gro√ü genug sein (Lookback beachten!)\n",
                "        te = max(te, LOOKBACK_DEFAULT + 1)\n",
                "        \n",
                "        if te <= 0 or ve <= te or ve > n:\n",
                "            continue\n",
                "            \n",
                "        # Wir speichern Slices: (Train-Bereich, Val-Bereich)\n",
                "        # Train geht immer von 0 bis te (Expanding Window)\n",
                "        stops.append((slice(0, te), slice(te, ve)))\n",
                "        \n",
                "    if len(stops) != n_folds:\n",
                "        # Warnung oder Error, falls wir nicht genug Folds bauen konnten\n",
                "        # Hier akzeptieren wir es implizit, pr√ºfen es aber:\n",
                "        print(f\"[WARN] Konnte nur {len(stops)} von {n_folds} Folds generieren.\")\n",
                "        \n",
                "    return stops\n",
                "\n",
                "# Splits generieren\n",
                "splits = make_wf_splits(len(df), n_folds=N_FOLDS, val_frac=0.20, min_train_frac=0.45)\n",
                "print(\"Anzahl generierter Folds:\", len(splits))\n",
                "if len(splits) > 0:\n",
                "    tr_s, va_s = splits[0]\n",
                "    print(f\"  Fold1: Train bis idx={tr_s.stop}, Val bis idx={va_s.stop} (Gr√∂√üe Val: {va_s.stop - va_s.start})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "7faef79f-6bf2-4968-b014-0638ef49ff10",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === MODELL-HELPER FUNKTIONEN ===\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
                "from tensorflow.keras import layers, regularizers, callbacks, optimizers, models\n",
                "\n",
                "# optimierte Dataset Funktion (ersetzt make_windows)\n",
                "def make_dataset(X_df, y_ser, lookback, batch_size=64, shuffle=False, seed=42):\n",
                "    # Cast to float32/int32 explicitly for TF\n",
                "    data = X_df.values.astype(\"float32\")\n",
                "    targets = y_ser.values.astype(\"int32\")\n",
                "    \n",
                "    # timeseries_dataset_from_array nutzt C++ Op -> sehr schnell\n",
                "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
                "        data=data,\n",
                "        targets=targets,\n",
                "        sequence_length=lookback,\n",
                "        sequence_stride=1,\n",
                "        shuffle=shuffle,\n",
                "        batch_size=batch_size,\n",
                "        seed=seed,\n",
                "        start_index=0,\n",
                "        end_index=None\n",
                "    )\n",
                "    # Prefetch f√ºr GPU Pipeline\n",
                "    return ds.prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "def build_model(n_features, width1=64, width2=32, dropout=0.10, lr=5e-4, use_gru=True):\n",
                "    rnn = layers.GRU if use_gru else layers.LSTM\n",
                "    m = models.Sequential([\n",
                "        layers.Input(shape=(None, n_features)),\n",
                "        rnn(width1, return_sequences=True, recurrent_dropout=dropout),\n",
                "        layers.LayerNormalization(),\n",
                "        rnn(width2, recurrent_dropout=dropout),\n",
                "        layers.LayerNormalization(),\n",
                "        layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-5)),\n",
                "        layers.Dense(1, activation=\"sigmoid\"),\n",
                "    ])\n",
                "    m.compile(\n",
                "        optimizer=optimizers.Adam(learning_rate=lr),\n",
                "        loss=\"binary_crossentropy\",\n",
                "        metrics=[tf.keras.metrics.AUC(name=\"auc\"),\n",
                "                 tf.keras.metrics.AUC(name=\"auprc\", curve=\"PR\")]\n",
                "    )\n",
                "    return m\n",
                "\n",
                "# Hilfsfunktion: MCC f√ºr besten Threshold berechnen\n",
                "def mcc_at_best_thr(y_true, y_prob):\n",
                "    ts = np.r_[0.0, np.unique(y_prob), 1.0]\n",
                "    best = (-1.0, 0.5)\n",
                "    for t in ts:\n",
                "        yp = (y_prob >= t).astype(int)\n",
                "        m = matthews_corrcoef(y_true, yp)\n",
                "        if m > best[0]:\n",
                "            best = (float(m), float(t))\n",
                "    return best  # (mcc, thr)\n",
                "\n",
                "# Funktion f√ºr Training und Evaluation mit Dataset\n",
                "def fit_eval_fold_fast(ds_tr, ds_va, y_va, n_features, hp, epochs=EPOCHS_GRID):\n",
                "    tf.keras.backend.clear_session()\n",
                "    \n",
                "    # Modell bauen\n",
                "    model = build_model(\n",
                "        n_features=n_features,\n",
                "        width1=hp[\"width1\"], width2=hp[\"width2\"],\n",
                "        dropout=hp[\"dropout\"], lr=hp[\"lr\"], use_gru=(hp[\"cell\"]==\"GRU\")\n",
                "    )\n",
                "\n",
                "    # Callbacks\n",
                "    cbs = [\n",
                "        callbacks.EarlyStopping(monitor=\"val_auprc\", mode=\"max\", patience=6, restore_best_weights=True),\n",
                "        callbacks.ReduceLROnPlateau(monitor=\"val_auprc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5),\n",
                "        callbacks.TerminateOnNaN(),\n",
                "    ]\n",
                "\n",
                "    # Training\n",
                "    hist = model.fit(ds_tr, validation_data=ds_va, epochs=epochs, verbose=0, callbacks=cbs)\n",
                "\n",
                "    # Evaluation auf Validation-Set\n",
                "    # Achtung: ds_va muss NICHT geshuffelt sein, damit die Reihenfolge zu y_va passt!\n",
                "    yva_proba = model.predict(ds_va, verbose=0).ravel()\n",
                "    \n",
                "    # L√§nge checken (durch Windowing gehen die ersten samples verloren)\n",
                "    # y_va ist der \"echte\" Target-Vektor nach Windowing (muss vorher gek√ºrzt werden)\n",
                "    \n",
                "    # Metriken berechnen\n",
                "    mcc_val, thr_val = mcc_at_best_thr(y_va[-len(yva_proba):], yva_proba)\n",
                "    yva_true_clipped = y_va[-len(yva_proba):]\n",
                "    \n",
                "    yva_pred_best = (yva_proba >= thr_val).astype(int)\n",
                "\n",
                "    metrics = dict(\n",
                "        mcc=float(mcc_val),\n",
                "        thr_val=float(thr_val),\n",
                "        bal_acc=float(balanced_accuracy_score(yva_true_clipped, yva_pred_best)),\n",
                "        auprc=float(average_precision_score(yva_true_clipped, yva_proba)),\n",
                "        auroc=float(roc_auc_score(yva_true_clipped, yva_proba)),\n",
                "        epochs_trained=int(len(hist.history[\"loss\"]))\n",
                "    )\n",
                "    tf.keras.backend.clear_session()\n",
                "    return metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "337cbd2c-95af-41f5-9be0-dd323baf6fdb",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gr√∂√üe Suchraum:\n",
                        "  HP-Kombinationen: 1\n",
                        "  Lookback-Optionen: 1\n",
                        "  Feature-Sets: 1\n",
                        "  Folds pro Kombination: 2\n",
                        "  -> Gesamte Training-Runs: 2\n"
                    ]
                }
            ],
            "source": [
                "# === SUCH-GRIDS DEFINIEREN ===\n",
                "# Hier definieren wir den Suchraum f√ºr die Hyperparameter.\n",
                "\n",
                "# 1. Lookback: Wie weit schauen wir zur√ºck?\n",
                "LOOKBACK_GRID = [60] if not FAST else [LOOKBACK_DEFAULT] # Optimized: only 60\n",
                "\n",
                "# 2. Modell-Architektur und Hyperparameter\n",
                "# Wir bauen eine Liste von Dicts (Grid Search)\n",
                "HP_GRID = [\n",
                "    dict(width1=w1, width2=w2, dropout=dp, lr=lr, cell=cell)\n",
                "    for (w1, w2) in [(64,32)] # Netzgr√∂√üe (nur eine Option)\n",
                "    for lr in [5e-4]          # Lernrate (nur eine Option)\n",
                "    for dp in [0.2]           # Dropout (Optimized: only 0.2)\n",
                "    for cell in [\"GRU\"] # Zelltyp (Optimized: only GRU)\n",
                "]\n",
                "\n",
                "# Falls FAST-Mode, √ºberschreiben wir das Grid mit nur einer Konfiguration\n",
                "if FAST:\n",
                "    HP_GRID = [dict(width1=32, width2=16, dropout=0.10, lr=5e-4, cell=\"GRU\")]\n",
                "\n",
                "# 3. Feature-Subsets: Welche Spalten nutzen wir?\n",
                "FEATURE_SUBSETS = {\n",
                "    # Standard: Alle verf√ºgbaren Features\n",
                "    \"all\": FEATURES_ALL,\n",
                "    \n",
                "    # Experiment 1: Nur Momentum-basierte Indikatoren\n",
                "    # \"mom_only\": [c for c in FEATURES_ALL  (DISABLED for speed)\n",
                "    #             if (\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"})],\n",
                "                 \n",
                "    # Experiment 2: Momentum + Volatilit√§t\n",
                "    # \"mom+vol\": [c for c in FEATURES_ALL (DISABLED for speed)\n",
                "    #             if ((\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"}))\n",
                "    #                or (c in {\"realized_vol_10\",\"vol_z_20\"})]\n",
                "}\n",
                "\n",
                "print(\"Gr√∂√üe Suchraum:\")\n",
                "print(f\"  HP-Kombinationen: {len(HP_GRID)}\")\n",
                "print(f\"  Lookback-Optionen: {len(LOOKBACK_GRID)}\")\n",
                "print(f\"  Feature-Sets: {len(FEATURE_SUBSETS)}\")\n",
                "print(f\"  Folds pro Kombination: {len(splits)}\")\n",
                "print(f\"  -> Gesamte Training-Runs: {len(HP_GRID) * len(LOOKBACK_GRID) * len(FEATURE_SUBSETS) * len(splits)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "42408c93-f83a-4abc-ab98-1d0279037343",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starte Suche ...\n",
                        "\n",
                        "[PREP] Generiere Datasets f√ºr Feat='all', LB=60 ...\n",
                        "[all.. | LB=60 | GRU | Fold1] MCC=0.070 (Ep:1)\n",
                        "[all.. | LB=60 | GRU | Fold2] MCC=0.054 (Ep:1)\n",
                        "\n",
                        "Completed. Time=32.2s\n"
                    ]
                }
            ],
            "source": [
                "# === HAUPTSCHLEIFE: SUCHE DURCHF√úHREN (OPTIMIERT) ===\n",
                "from time import perf_counter\n",
                "\n",
                "print(\"Starte Suche ...\", flush=True)\n",
                "MAX_SECONDS = 60 * 60 * 2  # Max 2 Stunden\n",
                "\n",
                "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
                "records = []\n",
                "t0 = perf_counter()\n",
                "\n",
                "# Resume Check\n",
                "done_keys = set()\n",
                "if csv_path.exists():\n",
                "    try:\n",
                "        done_df = pd.read_csv(csv_path)\n",
                "        for _, r in done_df.iterrows():\n",
                "            done_keys.add((r[\"features_used\"], int(r[\"lookback\"]),\n",
                "                           r[\"cell\"], int(r[\"width1\"]), int(r[\"width2\"]),\n",
                "                           float(r[\"dropout\"]), float(r[\"lr\"]), int(r[\"fold\"])))\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "stop_time = t0 + MAX_SECONDS\n",
                "\n",
                "# OUTER LOOPS (Data Dimensions)\n",
                "for feat_name, FEATS in FEATURE_SUBSETS.items():\n",
                "    if len(FEATS) == 0: continue\n",
                "\n",
                "    for lookback in LOOKBACK_GRID:\n",
                "        \n",
                "        # --- OPTIMIERUNG: Datasets vor den HP-Loops erstellen ---\n",
                "        # Wir bereiten die Folds EINMALIG vor, statt in jeder HP-Runde neu.\n",
                "        # Das spart massiv Zeit bei Scaling & Windowing.\n",
                "        fold_datasets = {}\n",
                "        \n",
                "        # Wir checken, ob wir ALLE HPs f√ºr diesen (Feat, LB) Block schon haben.\n",
                "        # Wenn ja, k√∂nnen wir das Erstellen der Datasets √ºberspringen.\n",
                "        # (Vereinfachter Check: wir machen es pro Fold bei Bedarf, aber hier globaler Split)\n",
                "        \n",
                "        print(f\"\\n[PREP] Generiere Datasets f√ºr Feat='{feat_name}', LB={lookback} ...\")\n",
                "        \n",
                "        datasets_ready = True\n",
                "        for fold_id, (tr_s, va_s) in enumerate(splits, start=1):\n",
                "            # Slice Data\n",
                "            X_tr, y_tr = X_full.iloc[tr_s][FEATS], y_full.iloc[tr_s]\n",
                "            X_va, y_va = X_full.iloc[va_s][FEATS], y_full.iloc[va_s]\n",
                "            \n",
                "            # Scale\n",
                "            scaler = StandardScaler()\n",
                "            X_tr_s = pd.DataFrame(scaler.fit_transform(X_tr), index=X_tr.index, columns=X_tr.columns)\n",
                "            X_va_s = pd.DataFrame(scaler.transform(X_va),     index=X_va.index, columns=X_va.columns)\n",
                "            \n",
                "            # Make TF Datasets (using C++ generator)\n",
                "            ds_tr = make_dataset(X_tr_s, y_tr, lookback, batch_size=BATCH, shuffle=True, seed=SEED)\n",
                "            ds_va = make_dataset(X_va_s, y_va, lookback, batch_size=BATCH, shuffle=False)\n",
                "            \n",
                "            # y_va (raw values) for metric calculation (needs to be aligned with windowed output)\n",
                "            # timeseries_dataset_from_array cuts off the first (lookback-1) samples.\n",
                "            y_va_aligned = y_va.values[lookback-1:] \n",
                "            \n",
                "            fold_datasets[fold_id] = (ds_tr, ds_va, y_va_aligned, len(FEATS))\n",
                "        \n",
                "        # --- INNER LOOP: Hyperparameters ---\n",
                "        for hp in HP_GRID:\n",
                "            for fold_id in range(1, len(splits) + 1):\n",
                "                # Key check\n",
                "                key = (feat_name, int(lookback),\n",
                "                       hp[\"cell\"], int(hp[\"width1\"]), int(hp[\"width2\"]),\n",
                "                       float(hp[\"dropout\"]), float(hp[\"lr\"]), int(fold_id))\n",
                "                \n",
                "                if key in done_keys:\n",
                "                    continue\n",
                "                \n",
                "                # Time check\n",
                "                if perf_counter() > stop_time:\n",
                "                    break\n",
                "\n",
                "                # Get Pre-calc Data\n",
                "                ds_tr, ds_va, y_va_true, n_feat = fold_datasets[fold_id]\n",
                "                \n",
                "                # Train & Eval\n",
                "                # Note: y_va_true is passed explicitly to avoid re-extraction\n",
                "                mets = fit_eval_fold_fast(\n",
                "                    ds_tr, ds_va, y_va_true, n_feat,\n",
                "                    hp=hp, epochs=EPOCHS_GRID\n",
                "                )\n",
                "                \n",
                "                # Save\n",
                "                rec = {\n",
                "                    \"feature_set\": FEATURESET,\n",
                "                    \"features_used\": feat_name,\n",
                "                    \"n_features\": n_feat,\n",
                "                    \"lookback\": lookback,\n",
                "                    **hp,\n",
                "                    \"fold\": fold_id,\n",
                "                    **mets\n",
                "                }\n",
                "                records.append(rec)\n",
                "                pd.DataFrame([rec]).to_csv(csv_path, mode='a', header=not os.path.exists(csv_path), index=False)\n",
                "                \n",
                "                print(f\"[{feat_name[:5]}.. | LB={lookback} | {hp['cell']} | Fold{fold_id}] MCC={mets['mcc']:.3f} (Ep:{mets['epochs_trained']})\")\n",
                "\n",
                "            if perf_counter() > stop_time: break\n",
                "        if perf_counter() > stop_time: break\n",
                "    if perf_counter() > stop_time: \n",
                "        print(\"[INFO] Time limit reached.\")\n",
                "        break\n",
                "\n",
                "t1 = perf_counter()\n",
                "print(f\"\\nCompleted. Time={t1-t0:.1f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "2afca6e0-5c56-45c9-93a0-066fc19b613c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top 3 Konfigurationen:\n",
                        "  features_used  lookback cell  dropout  mcc_mean   mcc_std\n",
                        "0           all        60  GRU      0.2  0.061812  0.010962\n",
                        "\n",
                        "Best config saved to: ..\\results\\2026-01-03_20-56-59_wfcv\\best_config.json\n"
                    ]
                }
            ],
            "source": [
                "# === ERGEBNIS-ANALYSE ===\n",
                "# Wir aggregieren die Ergebnisse aller Folds und suchen die beste Konfiguration.\n",
                "# Kriterium: Hoher Durchschnitts-MCC und geringe Standardabweichung (Stabilit√§t).\n",
                "\n",
                "import pandas as pd, json, numpy as np\n",
                "\n",
                "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
                "if not csv_path.exists():\n",
                "    print(\"Keine Ergebnisse gefunden.\")\n",
                "else:\n",
                "    results = pd.read_csv(csv_path)\n",
                "\n",
                "    # Gruppierungs-Spalten (alles au√üer Fold und Ergebnissen)\n",
                "    agg_cols = [c for c in [\"feature_set\",\"features_used\",\"n_features\",\"lookback\",\n",
                "                            \"width1\",\"width2\",\"dropout\",\"lr\",\"cell\"] if c in results.columns]\n",
                "\n",
                "    # Aggregation: Mittelwert und Standardabweichung\n",
                "    agg_dict = {\"mcc\": [\"mean\",\"std\"], \"auprc\": [\"mean\",\"std\"], \"auroc\": [\"mean\"]}\n",
                "    g = results.groupby(agg_cols).agg(agg_dict)\n",
                "\n",
                "    # Flache Spaltennamen erzeugen (MultiIndex entfernen)\n",
                "    g.columns = [\n",
                "        \"_\".join([str(x) for x in col if str(x) != \"\"]).strip(\"_\")\n",
                "        for col in g.columns.to_flat_index()\n",
                "    ]\n",
                "    g = g.reset_index()\n",
                "\n",
                "    # Sortieren: Beste Konfiguration zuerst.\n",
                "    # Wir sortieren nach MCC Mean (absteigend) und MCC Std (aufsteigend).\n",
                "    g = g.sort_values([\"mcc_mean\",\"auprc_mean\",\"mcc_std\"], ascending=[False, False, True])\n",
                "\n",
                "    # Speichern der aggregierten Tabelle\n",
                "    g.to_csv(RUN_DIR / \"wfcv_results_agg.csv\", index=False)\n",
                "    \n",
                "    # Top 5 speichern\n",
                "    top5 = g.head(5).copy()\n",
                "    top5.to_csv(RUN_DIR / \"wfcv_results_top5.csv\", index=False)\n",
                "    \n",
                "    print(\"Top 3 Konfigurationen:\")\n",
                "    print(top5.head(3)[[\"features_used\", \"lookback\", \"cell\", \"dropout\", \"mcc_mean\", \"mcc_std\"]])\n",
                "\n",
                "    # Die allerbeste Config extrahieren und als JSON speichern\n",
                "    # Diese Datei wird von Notebook 3 automatisch geladen.\n",
                "    best = top5.iloc[0].to_dict() if len(top5) else {}\n",
                "    with open(RUN_DIR / \"best_config.json\", \"w\") as f:\n",
                "        json.dump(best, f, indent=2)\n",
                "\n",
                "    print(\"\\nBest config saved to:\", RUN_DIR / \"best_config.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "a79f67d2-a756-442f-8aaa-047113714208",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Heatmaps gespeichert.\n"
                    ]
                }
            ],
            "source": [
                "# === VISUALISIERUNG: HEATMAPS ===\n",
                "# Wir plotten Heatmaps, um zu sehen, welche Paremeter-R√§ume gut funktionieren.\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "if csv_path.exists():\n",
                "    agg = pd.read_csv(RUN_DIR / \"wfcv_results_agg.csv\")\n",
                "    \n",
                "    # Wir pivotieren die Tabelle f√ºr die Heatmap\n",
                "    # Normalerweise ist Lookback eine gute Y-Achse\n",
                "    pivot_index = \"lookback\" if \"lookback\" in agg.columns else agg.columns[0]\n",
                "    col_candidates = [\"features_used\", \"cell\", \"width1\"]\n",
                "    pivot_columns = [c for c in col_candidates if c in agg.columns]\n",
                "    \n",
                "    def _plot_grid(df: pd.DataFrame, value_col: str, fname: str):\n",
                "        if value_col not in df.columns: return\n",
                "        try:\n",
                "            # Pivot: rows=lookback, cols=features/cell/...\n",
                "            pvt = df.pivot_table(index=pivot_index, columns=pivot_columns, values=value_col, aggfunc=\"mean\")\n",
                "            \n",
                "            plt.figure(figsize=(10, 6))\n",
                "            im = plt.imshow(pvt.values, aspect=\"auto\", cmap=\"viridis\")\n",
                "            plt.colorbar(im)\n",
                "            \n",
                "            # Achsen beschriften\n",
                "            plt.yticks(range(len(pvt.index)), pvt.index)\n",
                "            plt.xticks(range(len(pvt.columns)), pvt.columns, rotation=45, ha=\"right\")\n",
                "            \n",
                "            plt.xlabel(\" / \".join(pivot_columns)); plt.ylabel(pivot_index)\n",
                "            plt.title(fname.replace(\"_\", \" \").replace(\".png\", \"\"))\n",
                "            plt.tight_layout()\n",
                "            plt.savefig(RUN_DIR / \"plots\" / fname, dpi=160)\n",
                "            plt.close()\n",
                "        except Exception as e:\n",
                "            print(f\"Konnte Plot {fname} nicht erstellen: {e}\")\n",
                "\n",
                "    _plot_grid(agg, \"mcc_mean\",   \"heatmap_mcc.png\")\n",
                "    _plot_grid(agg, \"auprc_mean\", \"heatmap_auprc.png\")\n",
                "    print(\"Heatmaps gespeichert.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "103d010d-c556-4942-a47e-dc5f79ae5796",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Boxplots gespeichert.\n"
                    ]
                }
            ],
            "source": [
                "# === VISUALISIERUNG: BOXPLOTS ===\n",
                "# Boxplots zeigen die Stabilit√§t √ºber die Folds besser als nur der Mittelwert.\n",
                "\n",
                "if csv_path.exists():\n",
                "    results = pd.read_csv(csv_path)\n",
                "    \n",
                "    # Wir erstellen ein Label f√ºr jede Config (ohne Fold-Info)\n",
                "    def _short_label(r):\n",
                "        return f\"{r['features_used']}-{r['cell']}-lb{int(r['lookback'])}-dp{r['dropout']}\"\n",
                "    \n",
                "    results[\"config_label\"] = results.apply(_short_label, axis=1)\n",
                "\n",
                "    # Wir nehmen nur die Top 10 Configs f√ºr den Plot, sonst wird es unleserlich\n",
                "    top_labels = results.groupby(\"config_label\")[\"mcc\"].mean().sort_values(ascending=False).head(10).index\n",
                "    subset = results[results[\"config_label\"].isin(top_labels)]\n",
                "    \n",
                "    # Plot\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    data = [grp[\"mcc\"].values for label, grp in subset.groupby(\"config_label\")]\n",
                "    labels = [label for label, grp in subset.groupby(\"config_label\")]\n",
                "    \n",
                "    # Sortierung im Plot nach Median\n",
                "    medians = [np.median(d) for d in data]\n",
                "    sort_idx = np.argsort(medians)[::-1]\n",
                "    data = [data[i] for i in sort_idx]\n",
                "    labels = [labels[i] for i in sort_idx]\n",
                "\n",
                "    plt.boxplot(data, showmeans=True, meanline=True)\n",
                "    plt.xticks(range(1, len(labels)+1), labels, rotation=45, ha=\"right\")\n",
                "    plt.title(\"Top 10 Configs: MCC Varianz √ºber Folds\")\n",
                "    plt.ylabel(\"MCC Score\")\n",
                "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(RUN_DIR / \"plots\" / \"boxplots_top10_mcc.png\", dpi=160)\n",
                "    plt.close()\n",
                "    print(\"Boxplots gespeichert.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "5581e69e-5b28-4224-9026-2872d2313683",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Block 5 abgeschlossen. Ergebnisse in: ..\\results\\2026-01-03_20-56-59_wfcv\n"
                    ]
                }
            ],
            "source": [
                "# === INFO-DUMP ===\n",
                "# Metadaten speichern\n",
                "run_info = {\n",
                "    \"seed\": SEED,\n",
                "    \"epochs_grid\": EPOCHS_GRID,\n",
                "    \"n_folds\": N_FOLDS,\n",
                "    \"val_frac\": 0.20,\n",
                "    \"min_train_frac\": 0.45,\n",
                "    \"lookback_grid\": LOOKBACK_GRID,\n",
                "    \"hp_grid_size\": (len(HP_GRID) if not FAST else 1),\n",
                "    \"feature_subsets\": list(FEATURE_SUBSETS.keys()),\n",
                "    \"train_csv\": TRAIN_CSV,\n",
                "    \"label_resolution\": {\n",
                "        \"source\": \"yaml\" if os.path.exists(yaml_path) and (label_h is not None) else \"inferred_from_csv\",\n",
                "        \"yaml_path\": yaml_path\n",
                "    },\n",
                "    \"labels\": {\"horizon\": H_FOR_FILE, \"mode\": MODE_FOR_FILE, \"epsilon\": EPS_FOR_FILE}\n",
                "}\n",
                "with open(RUN_DIR / \"wfcv_run_info.json\", \"w\") as f:\n",
                "    json.dump(run_info, f, indent=2)\n",
                "\n",
                "print(\"\\nBlock 5 abgeschlossen. Ergebnisse in:\", RUN_DIR)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
