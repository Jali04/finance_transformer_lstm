{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "362a40d2-85eb-4eb7-9305-75a83695cb55",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SYSTEM & IMPORTS ===\n",
                "# Dieser Block importiert alle notwendigen Bibliotheken und prüft die Systemumgebung.\n",
                "\n",
                "# os: Betriebssystem-Funktionen (Pfade erstellen, Umgebungsvariablen lesen)\n",
                "import os\n",
                "# sys: System-Parameter (z.B. Python-Pfad)\n",
                "import sys\n",
                "# json: Konfigurationsdateien lesen/schreiben\n",
                "import json\n",
                "# time: Zeitstempel für Ordnernamen und Messungen\n",
                "import time\n",
                "# glob: Dateimuster-Suche (z.B. \"*.csv\" finden)\n",
                "import glob\n",
                "\n",
                "# Path: Objektorientierter Umgang mit Dateipfaden (moderner als os.path)\n",
                "from pathlib import Path\n",
                "\n",
                "# numpy (np): Schnelle numerische Operationen auf Arrays\n",
                "import numpy as np\n",
                "\n",
                "# pandas (pd): Tabellarische Datenverarbeitung (DataFrames)\n",
                "import pandas as pd\n",
                "\n",
                "# Wir setzen das Root-Verzeichnis auf den übergeordneten Ordner (..)\n",
                "ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "# Wir prüfen, ob ROOT bereits im Systempfad ist, und fügen es hinzu, falls nicht\n",
                "# damit Python unsere eigenen Module (z.B. im Parent-Ordner) findet.\n",
                "if ROOT not in sys.path:\n",
                "    sys.path.insert(0, ROOT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "f38e5857-6e38-4751-b8d6-b1910c535020",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RUN_DIR: ..\\results\\2026-01-07_21-29-55_lstm\n"
                    ]
                }
            ],
            "source": [
                "# === 0) KONFIGURATION LADEN ===\n",
                "# Wir laden die zentrale Steuerdatei 'config.json'. Das stellt sicher, dass alle Skripte \n",
                "# mit denselben Parametern (Ticker, Zeiträume etc.) arbeiten.\n",
                "\n",
                "# Öffnen der Konfigurationsdatei im Lesemodus\n",
                "with open(os.path.join(ROOT, \"config.json\"), \"r\") as f:\n",
                "    # Parsen des JSON-Inhalts in ein Python-Dictionary 'C'\n",
                "    C = json.load(f)\n",
                "\n",
                "# Wir extrahieren die Parameter in eigene Variablen für bessere Lesbarkeit.\n",
                "TICKER   = C[\"ticker\"]       # Börsenkürzel (z.B. AAPL)\n",
                "START    = C[\"start\"]        # Startdatum der Daten\n",
                "END      = C[\"end\"]          # Enddatum der Daten\n",
                "INTERVAL = C[\"interval\"]     # Daten-Intervall (z.B. 1d)\n",
                "\n",
                "# Modell-Parameter aus der Config:\n",
                "HORIZON  = int(C[\"horizon\"])  # Vorhersage-Horizont: Für wie viele Tage in die Zukunft sagen wir vorher?\n",
                "LOOKBACK = int(C[\"lookback\"]) # Rückblick: Wie viele vergangene Tage sieht das Modell als Input?\n",
                "BATCH    = int(C[\"batch\"])    # Batch-Größe: Anzahl der Samples pro Trainings-Schritt\n",
                "EPOCHS   = int(C[\"epochs\"])   # Epochen: Wie oft wird der gesamte Datensatz trainiert?\n",
                "SEED     = int(C.get(\"seed\", 42)) # Seed für Reproduzierbarkeit (Standard: 42)\n",
                "\n",
                "# Feature- und Label-Einstellungen:\n",
                "FEATURESET = C.get(\"featureset\", \"v2\") # Welches Variablenset nutzen wir? (Standard: v2)\n",
                "EPS_MODE   = C.get(\"epsilon_mode\", \"abs\") # Wie wird \"Up\" definiert? (Absolut oder Quantil)\n",
                "EPSILON    = float(C.get(\"epsilon\", 0.0005)) # Der Schwellwert für \"Up\" (Standard: 0.0005)\n",
                "\n",
                "# Ergebnis-Ordner vorbereiten.\n",
                "# Jedes Training bekommt einen eigenen Unterordner mit Zeitstempel, damit nichts überschrieben wird.\n",
                "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\"))\n",
                "# Erstelle das Verzeichnis, falls es nicht existiert (auch übergeordnete Ordner)\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Generiere Zeitstempel für den aktuellen Lauf: YYYY-MM-DD_HH-MM-SS_lstm\n",
                "run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S_lstm\")\n",
                "# Kombiniere Ergebnis-Ordner und Lauf-Name zum vollen Pfad\n",
                "RUN_DIR  = RESULTS_DIR / run_name\n",
                "# Erstelle den Lauf-Ordner\n",
                "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Ausgabe des Pfades zur Kontrolle\n",
                "print(\"RUN_DIR:\", RUN_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "27a371be-faf9-43d2-8bd0-53399c30ed00",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Ablations] shuffle_train=True | no_recurrent_dropout=False | ln_layout=both\n"
                    ]
                }
            ],
            "source": [
                "# === ABLATIONS STUDIEN (EXPERIMENTELLE SCHALTER) ===\n",
                "# \"Ablation Studies\" bedeuten, dass man gezielt Teile des Modells weglässt oder ändert,\n",
                "# um zu verstehen, welchen Beitrag sie zur Leistung liefern.\n",
                "\n",
                "AB = C.get(\"ablations\", {}) # Lädt Ablation-Settings aus der Config (falls vorhanden, sonst leeres Dict)\n",
                "\n",
                "# Hilfsfunktion, um Boolesche Werte sicher zu lesen (auch aus Umgebungsvariablen).\n",
                "def _get_bool(key, default):\n",
                "    # Versuche, den Wert aus den Umgebungsvariablen zu lesen\n",
                "    env = os.getenv(key)\n",
                "    if env is not None:\n",
                "        # Prüft auf verschiedene Schreibweisen für \"Wahr\" (1, true, yes, y, on)\n",
                "        return env.strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
                "    # Wenn nicht in Env, hole aus Config oder nimm Default\n",
                "    return bool(AB.get(key.lower(), default))\n",
                "\n",
                "# 1. Shuffle Train: Sollen die Trainingsdaten zufällig gemischt werden?\n",
                "# Standard: True (wichtig, damit das Modell nicht die Reihenfolge auswendig lernt)\n",
                "ABL_SHUFFLE_TRAIN = _get_bool(\"ABLATION_SHUFFLE_TRAIN\", True)\n",
                "\n",
                "# 2. Recurrent Dropout: Soll Dropout innerhalb der LSTM-Zellen deaktiviert werden?\n",
                "# Dropout hilft gegen Overfitting, macht das Training aber auf GPUs oft langsamer.\n",
                "# Standard: False (also Dropout benutzen)\n",
                "ABL_NO_RECURRENT_DROPOUT = _get_bool(\"ABLATION_NO_RECURRENT_DROPOUT\", False)\n",
                "\n",
                "# 3. LayerNormalization Layout: Wo sollen die Normalisierungs-Schichten hin?\n",
                "# \"both\" = Nach jeder RNN-Schicht.\n",
                "# \"after_second\" = Nur nach der zweiten Schicht.\n",
                "# Hole Wert aus Env oder Config, Standard ist \"both\"\n",
                "ABL_LN_LAYOUT = os.getenv(\"ABLATION_LN_LAYOUT\", AB.get(\"ln_layout\", \"both\")).lower()\n",
                "# Validierung des Wertes, Fallback auf \"both\" bei unbekannten Werten\n",
                "if ABL_LN_LAYOUT not in {\"both\",\"after_second\"}:\n",
                "    ABL_LN_LAYOUT = \"both\"\n",
                "\n",
                "# Ausgabe der aktuellen Ablation-Konfiguration\n",
                "print(f\"[Ablations] shuffle_train={ABL_SHUFFLE_TRAIN} | no_recurrent_dropout={ABL_NO_RECURRENT_DROPOUT} | ln_layout={ABL_LN_LAYOUT}\")\n",
                "\n",
                "# Wir konstruieren den erwarteten Dateinamen für die Trainingsdaten.\n",
                "# Dies dient nur zur Info und Vorbereitung des Strings, geladen wird später dynamischer.\n",
                "eps_tag   = f\"{EPS_MODE}{str(EPSILON).replace('.','p')}\" # Erstellt Tag wie \"abs0p0005\"\n",
                "TRAIN_CSV = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{HORIZON}_{eps_tag}.csv\" # Pfad zur CSV\n",
                "\n",
                "# Import von TensorFlow und Keras für das Deep Learning\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "# Metriken zur Bewertung der Klassifikation aus scikit-learn importieren\n",
                "from sklearn.metrics import (\n",
                "    classification_report,     # Erzeugt detaillierten Bericht (Precision, Recall, F1)\n",
                "    confusion_matrix,          # Erzeugt Verwirrungsmatrix (TP, TN, FP, FN)\n",
                "    balanced_accuracy_score,   # Accuracy gewichtet nach Klassengröße\n",
                "    matthews_corrcoef,         # MCC: Robuste Metrik für binäre Klassifikation\n",
                "    average_precision_score,   # AUPRC: Fläche unter der Precision-Recall-Kurve\n",
                "    roc_auc_score              # AUROC: Fläche unter der ROC-Kurve\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "2b82cb57-3a32-480c-8945-d35214679cdf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gefunden best_config.json: ..\\results\\2026-01-07_21-25-52_wfcv\\best_config.json\n",
                        "[Block3 Setup] cell=GRU width=64/32 rd=0.2 dp_cfg=0.2 lr=0.0005 lookback=60 | features_used=all | L2(Dense)=1e-05\n"
                    ]
                }
            ],
            "source": [
                "# === 1) BESTE CONFIG LADEN (Aus Hyperparameter-Optimierung) ===\n",
                "# Falls wir vorher einen Optimierungslauf (WFCV) gemacht haben, wollen wir dessen\n",
                "# Ergebnisse (die besten Parameter) hier nutzen.\n",
                "\n",
                "# Funktion zum Finden der neuesten 'best_config.json'\n",
                "def _latest_best_config(results_dir=\"../results\"):\n",
                "    # Wir suchen nach allen 'best_config.json' Dateien in WFCV-Ordnern\n",
                "    # Muster: ../results/*_wfcv/best_config.json\n",
                "    pattern = os.path.join(results_dir, \"*_wfcv\", \"best_config.json\")\n",
                "    cands = glob.glob(pattern)\n",
                "    # Wenn keine gefunden wurden, gib None zurück\n",
                "    if not cands:\n",
                "        return None, None\n",
                "    # Wir sortieren nach Änderungsdatum (os.path.getmtime), um die neueste zu finden\n",
                "    cands = sorted(cands, key=os.path.getmtime)\n",
                "    best_path = cands[-1] # Der letzte Eintrag ist der neueste\n",
                "    # Datei öffnen und laden\n",
                "    with open(best_path, \"r\") as f:\n",
                "        best_cfg = json.load(f)\n",
                "    return best_cfg, best_path\n",
                "\n",
                "# Aufruf der Suchfunktion\n",
                "BEST_CFG, BEST_CFG_PATH = _latest_best_config(RESULTS_DIR)\n",
                "\n",
                "# Fallback: Wenn keine Optimierung lief, nehmen wir Standardwerte.\n",
                "if BEST_CFG is None:\n",
                "    print(\"[INFO] Keine best_config.json gefunden — nutze Fallback (Config.json-Defaults).\")\n",
                "    BEST_CFG = {\n",
                "        \"features_used\": \"all\",    # Alle Features nutzen\n",
                "        \"lookback\": LOOKBACK,      # Lookback aus Config\n",
                "        \"cell\": \"GRU\",             # GRU als Standard-Zelle\n",
                "        \"width1\": 32,              # 32 Neuronen in Layer 1\n",
                "        \"width2\": 16,              # 16 Neuronen in Layer 2\n",
                "        \"dropout\": 0.10,           # 10% Dropout\n",
                "        \"lr\": 5e-4                 # Lernrate 0.0005\n",
                "    }\n",
                "else:\n",
                "    print(\"Gefunden best_config.json:\", BEST_CFG_PATH)\n",
                "\n",
                "# Wir setzen die Hyperparameter basierend auf der geladenen Config (oder Fallback).\n",
                "CELL    = str(BEST_CFG.get(\"cell\", \"GRU\")).upper() # Typ der RNN-Zelle: GRU oder LSTM\n",
                "WIDTH1  = int(BEST_CFG.get(\"width1\", 32))          # Anzahl Neuronen in 1. Schicht\n",
                "WIDTH2  = int(BEST_CFG.get(\"width2\", 16))          # Anzahl Neuronen in 2. Schicht\n",
                "DROPOUT = float(BEST_CFG.get(\"dropout\", 0.10))     # Dropout-Rate (z.B. 0.10 = 10% Neuronen abschalten)\n",
                "LR      = float(BEST_CFG.get(\"lr\", 5e-4))          # Lernrate für den Optimizer\n",
                "LB_FROM_BEST = int(BEST_CFG.get(\"lookback\", LOOKBACK)) # Wichtiger Parameter: Lookback\n",
                "\n",
                "# Wir nutzen den Lookback aus der Optimierung, falls vorhanden, sonst den aus der config.json\n",
                "USE_LOOKBACK = LB_FROM_BEST if LB_FROM_BEST > 0 else LOOKBACK\n",
                "# Tag für verwendete Features (z.B. \"all\" oder \"mom_only\")\n",
                "FEATURES_USED_TAG = str(BEST_CFG.get(\"features_used\", \"all\"))\n",
                "\n",
                "# Ablation-Logik anwenden:\n",
                "# Wenn \"no_recurrent_dropout\" aktiv ist, setzen wir Dropout in den RNN-Zellen auf 0.\n",
                "# Dafür erhöhen wir ggf. die L2-Regularisierung im Dense-Layer, um Overfitting anders zu bekämpfen.\n",
                "if ABL_NO_RECURRENT_DROPOUT:\n",
                "    RDROP = 0.0        # Kein Recurrent Dropout\n",
                "    L2_DENSE = 1e-4    # Stärkere L2-Regularisierung (1e-4)\n",
                "else:\n",
                "    RDROP = DROPOUT    # Nutze konfigurierten Dropout\n",
                "    L2_DENSE = 1e-5    # Schwächere L2-Regularisierung (1e-5)\n",
                "\n",
                "# Ausgabe der finalen Konfiguration für diesen Lauf\n",
                "print(f\"[Block3 Setup] cell={CELL} width={WIDTH1}/{WIDTH2} rd={RDROP} dp_cfg={DROPOUT} lr={LR} lookback={USE_LOOKBACK} | features_used={FEATURES_USED_TAG} | L2(Dense)={L2_DENSE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "07b91fc5-71ec-46b3-b625-f02038552a01",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded TRAIN_CSV: ../data/AAPL_1d_2010-01-01_2026-01-01_cls_h1_abs0p0005.csv\n",
                        "[Label] using horizon=1 | mode=abs | epsilon=0.0005\n",
                        "FEATURES (final): ['logret_1d', 'logret_3d', 'logret_5d', 'realized_vol_10', 'bb_pos', 'rsi_14', 'macd', 'macd_sig', 'macd_diff', 'vol_z_20', 'sma_diff']\n"
                    ]
                }
            ],
            "source": [
                "# === 2) DATEN & FEATURES LADEN ===\n",
                "# Hier laden wir die vorbereiteten Trainingsdaten (CSV).\n",
                "\n",
                "import yaml, glob, os, re\n",
                "\n",
                "# Pfad zur Feature-Metadaten-DAtei (YAML)\n",
                "yaml_path = f\"../data/features_{FEATURESET}.yml\"\n",
                "meta = {}\n",
                "# Wenn die Datei existiert, laden wir sie\n",
                "if os.path.exists(yaml_path):\n",
                "    with open(yaml_path, \"r\") as f:\n",
                "        meta = yaml.safe_load(f) or {}\n",
                "\n",
                "# Funktion, um die korrekte CSV-Datei zu finden.\n",
                "# Da der Dateiname viele Parameter enthält (Epsilon, Horizon etc.), müssen wir flexibel suchen.\n",
                "def _resolve_train_csv():\n",
                "    # 1. Versuch: Exakter Pfad basierend auf Konfiguration und Variablen\n",
                "    eps_tag_cfg = f\"{EPS_MODE}{str(EPSILON).replace('.','p')}\"\n",
                "    exact = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{HORIZON}_{eps_tag_cfg}.csv\"\n",
                "    if os.path.exists(exact):\n",
                "        return exact\n",
                "        \n",
                "    # 2. Versuch: Pfad basierend auf den Einträgen im YAML (falls dort abweichend definiert)\n",
                "    lab = (meta or {}).get(\"label\", {})\n",
                "    h_yaml   = int(lab.get(\"horizon\", HORIZON))\n",
                "    mode_yaml= str(lab.get(\"mode\", EPS_MODE))\n",
                "    eps_yaml = float(lab.get(\"epsilon\", EPSILON))\n",
                "    eps_tag_yaml = f\"{mode_yaml}{str(eps_yaml).replace('.','p')}\"\n",
                "    by_yaml = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{h_yaml}_{eps_tag_yaml}.csv\"\n",
                "    if os.path.exists(by_yaml):\n",
                "        return by_yaml\n",
                "        \n",
                "    # 3. Versuch: Irgendeine passende Datei finden (Fallback)\n",
                "    # Suche nach Dateien die mit den Basisparametern übereinstimmen\n",
                "    pat_any = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h*.csv\"\n",
                "    cands = sorted(glob.glob(pat_any), key=os.path.getmtime)\n",
                "    if cands:\n",
                "        return cands[-1] # Die neueste nehmen\n",
                "        \n",
                "    # Wenn nichts gefunden wurde, Fehler werfen\n",
                "    raise FileNotFoundError(\"Kein TRAIN_CSV gefunden. Bitte Block 2 mit Label-Definition laufen lassen.\")\n",
                "\n",
                "# Datei bestimmen und Pfad ausgeben\n",
                "TRAIN_CSV = _resolve_train_csv()\n",
                "print(\"Loaded TRAIN_CSV:\", TRAIN_CSV)\n",
                "\n",
                "# Wir analysieren den Dateinamen, um sicherzugehen, welche Label-Parameter wirklich drinstecken.\n",
                "def _infer_label_from(meta_dict, train_csv_path, fallback_h_from_root):\n",
                "    h, mode, eps = None, None, None\n",
                "    # Regex sucht nach Muster wie \"_cls_h5_abs0p0005.csv\" am Ende des Dateinamens\n",
                "    m = re.search(r\"_cls_h(\\d+)_([a-zq]+)([0-9p.]+)\\.csv$\", str(train_csv_path))\n",
                "    if m:\n",
                "        h    = int(m.group(1)) # Horizon\n",
                "        mode = m.group(2)      # Mode (z.B. abs)\n",
                "        eps_str = m.group(3)   # Epsilon als String (z.B. 0p0005)\n",
                "        eps = float(str(eps_str).replace(\"p\", \".\")) # Epsilon als Float\n",
                "    \n",
                "    if h is None: h = int(fallback_h_from_root)\n",
                "    return h, mode, eps\n",
                "\n",
                "# Parameter aus Dateinamen extrahieren\n",
                "H_DATA, MODE_DATA, EPS_DATA = _infer_label_from(meta, TRAIN_CSV, HORIZON)\n",
                "HORIZON  = int(H_DATA)\n",
                "if MODE_DATA is not None:   EPS_MODE = str(MODE_DATA)\n",
                "if EPS_DATA  is not None:   EPSILON  = float(EPS_DATA)\n",
                "print(f\"[Label] using horizon={HORIZON} | mode={EPS_MODE} | epsilon={EPSILON}\")\n",
                "\n",
                "# Daten einlesen. Index ist das Datum (Spalte 0), Parsing aktivieren.\n",
                "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
                "\n",
                "# Feature-Auswahl: Welche Spalten nutzen wir als Input (X)?\n",
                "# Wir holen uns alle Features aus der YAML-Liste, die auch im DataFrame sind.\n",
                "ALL_FEATURES = [c for c in (meta.get(\"features\", []) if meta else []) if c in df.columns]\n",
                "\n",
                "# Fallback, falls YAML leer: Alles außer OHLCV (Preise) und Target.\n",
                "if not ALL_FEATURES:\n",
                "    OHLCV = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
                "    # Setze alle Spalten als Features, die nicht OHLCV oder 'target' sind\n",
                "    ALL_FEATURES = [c for c in df.columns if c not in (OHLCV | {\"target\"})]\n",
                "\n",
                "# Wir filtern die Features ggf. noch weiter (z.B. \"mom_only\" für Momentum-Strategie).\n",
                "if FEATURES_USED_TAG == \"mom_only\":\n",
                "    # Behalte nur Features, die bestimmte Schlüsselwörter enthalten\n",
                "    FEATURES = [c for c in ALL_FEATURES\n",
                "                if (\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"})]\n",
                "else:\n",
                "    # Sonst nutze alle verfügbaren Features\n",
                "    FEATURES = ALL_FEATURES\n",
                "\n",
                "# Definition von Input (X) und Zielvariable (y)\n",
                "TARGET = \"target\"\n",
                "X = df[FEATURES].copy()       # Features als DataFrame\n",
                "y = df[TARGET].astype(int).copy() # Target als Integer (0/1)\n",
                "print(\"FEATURES (final):\", FEATURES)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "af24474a-6165-44f4-9243-a688d6372c68",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Split sizes → train 3489 (2010-02-22 - 2023-12-29)\n",
                        "              val   252   (2024-01-02 - 2024-12-31)\n",
                        "              test  250  (2025-01-02 - 2025-12-31)\n"
                    ]
                }
            ],
            "source": [
                "# === 3) CHRONOLOGISCHER SPLIT (Train/Val/Test) ===\n",
                "# Finanzdaten dürfen NICHT zufällig gesplittet werden (Zeitabhängigkeit!).\n",
                "# Wir definieren feste Zeiträume für die Sets.\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Train: Alles VOR 2024\n",
                "train_mask = X.index < \"2024-01-01\"\n",
                "\n",
                "# Validation: Das Jahr 2024 (zum Tunen der Hyperparameter und Early Stopping)\n",
                "val_mask   = (X.index >= \"2024-01-01\") & (X.index < \"2025-01-01\")\n",
                "\n",
                "# Test: Das Jahr 2025 (echter \"Out-of-Sample\" Test für finale Bewertung)\n",
                "test_mask  = X.index >= \"2025-01-01\"\n",
                "\n",
                "# Daten aufteilen basierend auf den Masken\n",
                "X_train, y_train = X.loc[train_mask], y.loc[train_mask]\n",
                "X_val,   y_val   = X.loc[val_mask],   y.loc[val_mask]\n",
                "X_test,  y_test  = X.loc[test_mask],  y.loc[test_mask]\n",
                "\n",
                "# Ausgabe der Größen zur Kontrolle\n",
                "print(f\"Split sizes → train {len(X_train)} ({X_train.index.min().date()} - {X_train.index.max().date()})\")\n",
                "print(f\"              val   {len(X_val)}   ({X_val.index.min().date()} - {X_val.index.max().date()})\")\n",
                "print(f\"              test  {len(X_test)}  ({X_test.index.min().date()} - {X_test.index.max().date()})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "5c6197b0-c082-48ba-9dd5-6de13a80ed3d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4) SKALIERUNG (StandardScaler) ===\n",
                "# Neuronale Netze funktionieren am besten, wenn die Inputs klein sind (etwa um -1 bis 1).\n",
                "# Wir nutzen StandardScaler, um (Wert - Mittelwert) / Standardabweichung zu rechnen.\n",
                "\n",
                "# WICHTIG: Der Scaler darf NUR auf das Training-Set \"gefittet\" (berechnet) werden.\n",
                "# Wir dürfen keine Informationen aus Val/Test (Zukunft) nutzen (Data Leakage!).\n",
                "scaler = StandardScaler(with_mean=True, with_std=True)\n",
                "\n",
                "# Fit auf Train (Mittelwert/Std berechnen), Transform auf Train (Werte skalieren)\n",
                "X_train_s = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=FEATURES)\n",
                "# Transform auf Val und Test (mit den in Train berechneten Parametern!)\n",
                "X_val_s   = pd.DataFrame(scaler.transform(X_val),       index=X_val.index,   columns=FEATURES)\n",
                "X_test_s  = pd.DataFrame(scaler.transform(X_test),      index=X_test.index,  columns=FEATURES)\n",
                "\n",
                "# Wir speichern den Scaler, damit wir später neue Daten genau gleich skalieren können.\n",
                "import joblib\n",
                "joblib.dump(scaler, RUN_DIR / \"scaler.joblib\")\n",
                "\n",
                "# Diagnose: Drift-Check\n",
                "# Ändern sich die statistischen Eigenschaften der Daten zwischen Train und Test stark?\n",
                "# Das nennt man \"Dataset Shift\" oder \"Concept Drift\".\n",
                "def drift_summary(Xa: pd.DataFrame, Xb: pd.DataFrame):\n",
                "    out = []\n",
                "    # Für jedes Feature\n",
                "    for c in Xa.columns:\n",
                "        # Mittelwert und Standardabweichung für Set A und Set B berechnen\n",
                "        m1, s1 = Xa[c].mean(), Xa[c].std(ddof=1)\n",
                "        m2, s2 = Xb[c].mean(), Xb[c].std(ddof=1)\n",
                "        # Verhältnisse der Standardabweichungen und Differenz der Mittelwerte berechnen\n",
                "        ratio_std = float((s2 + 1e-9) / (s1 + 1e-9))\n",
                "        diff_mean = float(m2 - m1)\n",
                "        # Ergebnis speichern\n",
                "        out.append({\"feature\": c, \"mean_diff\": diff_mean, \"std_ratio\": ratio_std})\n",
                "    # DataFrame zurückgeben, sortiert nach Standardabweichungs-Verhältnis\n",
                "    return pd.DataFrame(out).sort_values(\"std_ratio\", ascending=False)\n",
                "\n",
                "# Ergebnisse in CSV speichern zur späteren Analyse\n",
                "drift_summary(X_train_s, X_test_s).to_csv(RUN_DIR / \"drift_train_vs_test.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "de930d62-f80e-408a-9471-fcdbb3c2b9ef",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 5) WINDOWING (Zeitreihen-Daten vorbereiten) ===\n",
                "# LSTMs und GRUs benötigen keine Einzelbilder (Vektoren), sondern Sequenzen (Filme).\n",
                "# Wir müssen die Daten umformen: Aus [Samples, Features] wird [Samples, Timesteps, Features].\n",
                "\n",
                "def make_windows(X_df: pd.DataFrame, y_ser: pd.Series, lookback: int):\n",
                "    # Konvertierung zu Numpy Arrays für Speed (float32 für Features, int32 für Labels)\n",
                "    X_values = X_df.values.astype(np.float32)\n",
                "    y_values = y_ser.values.astype(np.int32)\n",
                "    n = len(X_df)\n",
                "    xs, ys = [], []\n",
                "    # Wir gleiten mit einem Fenster der Größe 'lookback' über die Daten\n",
                "    # Start bei lookback-1, da wir erst dann genug Daten für das erste Fenster haben\n",
                "    for i in range(lookback-1, n):\n",
                "        # Das Input-Fenster geht von i-lookback+1 bis i (einschließlich)\n",
                "        xs.append(X_values[i - lookback + 1 : i + 1])\n",
                "        # Das Label gehört zum Zeitpunkt i (dem letzten Punkt im Fenster)\n",
                "        ys.append(y_values[i])\n",
                "    # Stacken der Listen zu Numpy Arrays\n",
                "    return np.stack(xs, axis=0), np.array(ys)\n",
                "\n",
                "# Anwendung der Window-Funktion auf alle Splits (Train, Val, Test)\n",
                "Xtr_win, ytr = make_windows(X_train_s, y_train, USE_LOOKBACK)\n",
                "Xva_win, yva = make_windows(X_val_s,   y_val,   USE_LOOKBACK)\n",
                "Xte_win, yte = make_windows(X_test_s,  y_test,  USE_LOOKBACK)\n",
                "\n",
                "# Seeds für Zufallsgeneratoren setzen (für Reproduzierbarkeit der Ergebnisse)\n",
                "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
                "\n",
                "# TensorFlow Datasets (tf.data.Dataset) erstellen.\n",
                "# Diese laden Daten effizient \"on demand\" und mischen sie im RAM.\n",
                "def to_ds(X, y, batch, shuffle):\n",
                "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
                "    if shuffle:\n",
                "        # Nur Trainingsdaten sollten gemischt werden\n",
                "        ds = ds.shuffle(buffer_size=len(X), seed=SEED, reshuffle_each_iteration=True)\n",
                "    # Batching und Prefetching für Performance\n",
                "    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "# Erstellen der Dataset-Objekte für Training, Validation und Test\n",
                "ds_train = to_ds(Xtr_win, ytr, BATCH, shuffle=ABL_SHUFFLE_TRAIN)\n",
                "ds_val   = to_ds(Xva_win, yva, BATCH, shuffle=False)\n",
                "ds_test  = to_ds(Xte_win, yte, BATCH, shuffle=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "63ab8c75-36ad-464b-b8a6-7d87c4b519d7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Diag] LogReg AUROC val/test = 0.437/0.535\n"
                    ]
                }
            ],
            "source": [
                "# === 6) DIAGNOSE: LOGISTIC REGRESSION BASELINE ===\n",
                "# Bevor wir das komplexe LSTM trainieren, testen wir ein simples lineares Modell.\n",
                "# Wenn unser LSTM schlechter abschneidet als das hier, wissen wir, dass das Problem nicht \"fehlende Komplexität\" ist.\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import roc_auc_score\n",
                "\n",
                "# Initialisierung der logistischen Regression\n",
                "logit = LogisticRegression(max_iter=200)\n",
                "# Wir trainieren nur auf den features, ohne Zeitreihen-Struktur.\n",
                "# Um vergleichbar zu bleiben, müssen wir die Indices anpassen (wegen Lookback-Verlust am Anfang)\n",
                "# .iloc[USE_LOOKBACK-1:] ignoriert die ersten Zeilen, für die wir keine LSTMs-Fenster haben.\n",
                "logit.fit(X_train_s.iloc[USE_LOOKBACK-1:], y_train.iloc[USE_LOOKBACK-1:])\n",
                "\n",
                "# Vorhersagen auf Validation und Test\n",
                "y_proba_val_lr = logit.predict_proba(X_val_s.iloc[USE_LOOKBACK-1:])[:,1]\n",
                "y_proba_test_lr = logit.predict_proba(X_test_s.iloc[USE_LOOKBACK-1:])[:,1]\n",
                "\n",
                "# Score berechnen (AUROC - Area Under ROC Curve)\n",
                "res_val  = roc_auc_score(y_val.iloc[USE_LOOKBACK-1:], y_proba_val_lr)\n",
                "res_test = roc_auc_score(y_test.iloc[USE_LOOKBACK-1:], y_proba_test_lr)\n",
                "\n",
                "# Ausgabe der Ergebnisse\n",
                "print(f\"[Diag] LogReg AUROC val/test = {res_val:.3f}/{res_test:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "76d25d1b-8215-40c9-9b57-e469739b9fe8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
                        "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
                    ]
                }
            ],
            "source": [
                "# === 7) MODELLBAU (LSTM/GRU ARCHITEKTUR) ===\n",
                "# Hier wird das neuronale Netz definiert.\n",
                "\n",
                "from tensorflow.keras import layers, regularizers, optimizers, callbacks, models\n",
                "\n",
                "# Wahl der Zelle: GRU ist oft schneller und gleich gut wie LSTM.\n",
                "# Wir nutzen den Typ, der in der Config (bzw. Best Config) steht.\n",
                "rnn_cell = layers.GRU if CELL == \"GRU\" else layers.LSTM\n",
                "\n",
                "# Wir bauen das Modell schichtweise auf (Sequential API Liste).\n",
                "model_layers = [\n",
                "    # Input Layer: Definiert die Form der Daten (Lookback x Features)\n",
                "    layers.Input(shape=(USE_LOOKBACK, len(FEATURES))),\n",
                "\n",
                "    # 1. Rekurrente Schicht (Hidden Layer 1)\n",
                "    # return_sequences=True gibt die gesamte Sequenz weiter (wichtig für Stacked RNNs)\n",
                "    # recurrent_dropout hilft gegen Overfitting, ist aber langsam auf GPUs (daher in Ablation abschaltbar)\n",
                "    rnn_cell(WIDTH1, return_sequences=True, recurrent_dropout=RDROP),\n",
                "]\n",
                "\n",
                "# Ablation Layer-Norm: Normalisierung zwischen den Schichten stabilisiert das Training.\n",
                "# Wenn \"both\", fügen wir nach der ersten Schicht eine Normalisierung ein.\n",
                "if ABL_LN_LAYOUT == \"both\":\n",
                "    model_layers.append(layers.LayerNormalization())\n",
                "\n",
                "# 2. Rekurrente Schicht (Hidden Layer 2)\n",
                "# return_sequences=False (Standard) gibt nur den letzten Zustand am Ende der Sequenz aus.\n",
                "# Das komprimiert die Zeitinformation in einen Vektor.\n",
                "model_layers += [\n",
                "    rnn_cell(WIDTH2, recurrent_dropout=RDROP),\n",
                "    layers.LayerNormalization(), # Immer nach der letzten RNN Schicht normalisieren\n",
                "\n",
                "    # Dense Layer (Fully Connected) zur Merkmals-Verarbeitung\n",
                "    # activation=\"relu\" (Rectified Linear Unit) für Nichtlinearität\n",
                "    # L2-Regularisierung bestraft große Gewichte (gegen Overfitting)\n",
                "    layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(L2_DENSE)),\n",
                "\n",
                "    # Output Layer: Ein einzelnes Neuron mit Sigmoid-Aktivierung.\n",
                "    # Gibt eine Wahrscheinlichkeit zwischen 0 und 1 aus.\n",
                "    layers.Dense(1, activation=\"sigmoid\"),\n",
                "]\n",
                "\n",
                "# Modell aus der Liste der Schichten zusammensetzen\n",
                "model = models.Sequential(model_layers)\n",
                "\n",
                "# Kompilieren: Festlegen von Optimizer und Fehlerfunktion (Loss)\n",
                "model.compile(\n",
                "    optimizer=optimizers.Adam(learning_rate=LR), # Adam Optimizer mit definierter Lernrate\n",
                "    loss=\"binary_crossentropy\", # Standard-Loss für Ja/Nein Klassifikation\n",
                "    metrics=[\n",
                "        tf.keras.metrics.AUC(name=\"auc\"),             # ROC-AUC\n",
                "        tf.keras.metrics.AUC(name=\"auprc\", curve=\"PR\"), # Precision-Recall AUC (wichtiger bei unbalancierten Daten)\n",
                "        tf.keras.metrics.BinaryAccuracy(name=\"acc\"),  # Einfache Genauigkeit\n",
                "        tf.keras.metrics.Precision(name=\"prec\"),      # Precision: Wie viele Treffer waren wirklich Treffer?\n",
                "        tf.keras.metrics.Recall(name=\"rec\"),          # Recall: Wie viele aller Treffer haben wir gefunden?\n",
                "    ],\n",
                ")\n",
                "\n",
                "# Callbacks steuern den Trainingsablauf\n",
                "ckpt_path = RUN_DIR / \"best.keras\" # Pfad zum Speichern des besten Modells\n",
                "cbs = [\n",
                "    # Checkpoint: Speichert das Modell immer dann, wenn \"val_auprc\" besser wird.\n",
                "    callbacks.ModelCheckpoint(filepath=str(ckpt_path),\n",
                "                              monitor=\"val_auprc\", mode=\"max\",\n",
                "                              save_best_only=True, verbose=1),\n",
                "    \n",
                "    # Early Stopping: Bricht ab, wenn \"val_auprc\" sich 12 Epochen lang nicht verbessert.\n",
                "    # restore_best_weights=True stellt sicher, dass wir am Ende das beste Modell haben, nicht das letzte.\n",
                "    callbacks.EarlyStopping(monitor=\"val_auprc\", mode=\"max\",\n",
                "                            patience=12, restore_best_weights=True),\n",
                "    \n",
                "    # ReduceLROnPlateau: Halbiert die Lernrate, wenn 6 Epochen lang nichts passiert.\n",
                "    # Das hilft, im Minimum genauer zu konvergieren.\n",
                "    callbacks.ReduceLROnPlateau(monitor=\"val_auprc\", mode=\"max\",\n",
                "                                factor=0.5, patience=6, min_lr=1e-5, verbose=1),\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "803e0244-31a6-4f91-b404-29e8f58c6e79",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.7086 - auc: 0.4897 - auprc: 0.5073 - acc: 0.4936 - prec: 0.5043 - rec: 0.4729\n",
                        "Epoch 1: val_auprc improved from -inf to 0.60240, saving model to ..\\results\\2026-01-07_21-29-55_lstm\\best.keras\n",
                        "54/54 [==============================] - 17s 269ms/step - loss: 0.7086 - auc: 0.4897 - auprc: 0.5073 - acc: 0.4936 - prec: 0.5043 - rec: 0.4729 - val_loss: 0.7032 - val_auc: 0.5395 - val_auprc: 0.6024 - val_acc: 0.4560 - val_prec: 0.5690 - val_rec: 0.2920 - lr: 5.0000e-04\n",
                        "Epoch 2/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6917 - auc: 0.5434 - auprc: 0.5558 - acc: 0.5300 - prec: 0.5401 - rec: 0.5346\n",
                        "Epoch 2: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 15s 271ms/step - loss: 0.6917 - auc: 0.5434 - auprc: 0.5558 - acc: 0.5300 - prec: 0.5401 - rec: 0.5346 - val_loss: 0.7164 - val_auc: 0.5025 - val_auprc: 0.5822 - val_acc: 0.4456 - val_prec: 0.5600 - val_rec: 0.2478 - lr: 5.0000e-04\n",
                        "Epoch 3/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6892 - auc: 0.5578 - auprc: 0.5541 - acc: 0.5408 - prec: 0.5547 - rec: 0.5094\n",
                        "Epoch 3: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 14s 256ms/step - loss: 0.6892 - auc: 0.5578 - auprc: 0.5541 - acc: 0.5408 - prec: 0.5547 - rec: 0.5094 - val_loss: 0.7147 - val_auc: 0.5027 - val_auprc: 0.5691 - val_acc: 0.4870 - val_prec: 0.6129 - val_rec: 0.3363 - lr: 5.0000e-04\n",
                        "Epoch 4/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6833 - auc: 0.5787 - auprc: 0.5855 - acc: 0.5583 - prec: 0.5658 - rec: 0.5797\n",
                        "Epoch 4: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 13s 248ms/step - loss: 0.6833 - auc: 0.5787 - auprc: 0.5855 - acc: 0.5583 - prec: 0.5658 - rec: 0.5797 - val_loss: 0.6967 - val_auc: 0.4991 - val_auprc: 0.5822 - val_acc: 0.5337 - val_prec: 0.5920 - val_rec: 0.6549 - lr: 5.0000e-04\n",
                        "Epoch 5/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6813 - auc: 0.5878 - auprc: 0.5892 - acc: 0.5557 - prec: 0.5666 - rec: 0.5511\n",
                        "Epoch 5: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 14s 262ms/step - loss: 0.6813 - auc: 0.5878 - auprc: 0.5892 - acc: 0.5557 - prec: 0.5666 - rec: 0.5511 - val_loss: 0.7074 - val_auc: 0.4829 - val_auprc: 0.5560 - val_acc: 0.4974 - val_prec: 0.5645 - val_rec: 0.6195 - lr: 5.0000e-04\n",
                        "Epoch 6/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6777 - auc: 0.6008 - auprc: 0.5999 - acc: 0.5673 - prec: 0.5683 - rec: 0.6345\n",
                        "Epoch 6: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 15s 276ms/step - loss: 0.6777 - auc: 0.6008 - auprc: 0.5999 - acc: 0.5673 - prec: 0.5683 - rec: 0.6345 - val_loss: 0.7073 - val_auc: 0.4813 - val_auprc: 0.5564 - val_acc: 0.5181 - val_prec: 0.5806 - val_rec: 0.6372 - lr: 5.0000e-04\n",
                        "Epoch 7/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6750 - auc: 0.6052 - auprc: 0.6148 - acc: 0.5659 - prec: 0.5736 - rec: 0.5831\n",
                        "Epoch 7: val_auprc did not improve from 0.60240\n",
                        "\n",
                        "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
                        "54/54 [==============================] - 14s 257ms/step - loss: 0.6750 - auc: 0.6052 - auprc: 0.6148 - acc: 0.5659 - prec: 0.5736 - rec: 0.5831 - val_loss: 0.7288 - val_auc: 0.4625 - val_auprc: 0.5480 - val_acc: 0.4508 - val_prec: 0.5538 - val_rec: 0.3186 - lr: 5.0000e-04\n",
                        "Epoch 8/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6693 - auc: 0.6281 - auprc: 0.6318 - acc: 0.5930 - prec: 0.6083 - rec: 0.5694\n",
                        "Epoch 8: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 14s 266ms/step - loss: 0.6693 - auc: 0.6281 - auprc: 0.6318 - acc: 0.5930 - prec: 0.6083 - rec: 0.5694 - val_loss: 0.7174 - val_auc: 0.4630 - val_auprc: 0.5543 - val_acc: 0.4663 - val_prec: 0.5658 - val_rec: 0.3805 - lr: 2.5000e-04\n",
                        "Epoch 9/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6677 - auc: 0.6349 - auprc: 0.6356 - acc: 0.5977 - prec: 0.6058 - rec: 0.6065\n",
                        "Epoch 9: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 15s 283ms/step - loss: 0.6677 - auc: 0.6349 - auprc: 0.6356 - acc: 0.5977 - prec: 0.6058 - rec: 0.6065 - val_loss: 0.7083 - val_auc: 0.4814 - val_auprc: 0.5644 - val_acc: 0.4819 - val_prec: 0.5586 - val_rec: 0.5487 - lr: 2.5000e-04\n",
                        "Epoch 10/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6655 - auc: 0.6386 - auprc: 0.6424 - acc: 0.5991 - prec: 0.6017 - rec: 0.6351\n",
                        "Epoch 10: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 15s 269ms/step - loss: 0.6655 - auc: 0.6386 - auprc: 0.6424 - acc: 0.5991 - prec: 0.6017 - rec: 0.6351 - val_loss: 0.7196 - val_auc: 0.4700 - val_auprc: 0.5553 - val_acc: 0.4508 - val_prec: 0.5422 - val_rec: 0.3982 - lr: 2.5000e-04\n",
                        "Epoch 11/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6645 - auc: 0.6394 - auprc: 0.6459 - acc: 0.5942 - prec: 0.6023 - rec: 0.6037\n",
                        "Epoch 11: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 14s 253ms/step - loss: 0.6645 - auc: 0.6394 - auprc: 0.6459 - acc: 0.5942 - prec: 0.6023 - rec: 0.6037 - val_loss: 0.7151 - val_auc: 0.4690 - val_auprc: 0.5577 - val_acc: 0.4715 - val_prec: 0.5679 - val_rec: 0.4071 - lr: 2.5000e-04\n",
                        "Epoch 12/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6638 - auc: 0.6432 - auprc: 0.6434 - acc: 0.6038 - prec: 0.6052 - rec: 0.6442\n",
                        "Epoch 12: val_auprc did not improve from 0.60240\n",
                        "54/54 [==============================] - 13s 250ms/step - loss: 0.6638 - auc: 0.6432 - auprc: 0.6434 - acc: 0.6038 - prec: 0.6052 - rec: 0.6442 - val_loss: 0.7176 - val_auc: 0.4654 - val_auprc: 0.5570 - val_acc: 0.4922 - val_prec: 0.5743 - val_rec: 0.5133 - lr: 2.5000e-04\n",
                        "Epoch 13/100\n",
                        "54/54 [==============================] - ETA: 0s - loss: 0.6609 - auc: 0.6500 - auprc: 0.6548 - acc: 0.6029 - prec: 0.6092 - rec: 0.6196\n",
                        "Epoch 13: val_auprc did not improve from 0.60240\n",
                        "\n",
                        "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
                        "54/54 [==============================] - 14s 259ms/step - loss: 0.6609 - auc: 0.6500 - auprc: 0.6548 - acc: 0.6029 - prec: 0.6092 - rec: 0.6196 - val_loss: 0.7272 - val_auc: 0.4565 - val_auprc: 0.5573 - val_acc: 0.4715 - val_prec: 0.5714 - val_rec: 0.3894 - lr: 2.5000e-04\n"
                    ]
                }
            ],
            "source": [
                "# === 8) TRAINING STARTEN ===\n",
                "# Jetzt geht's los. Wir fitten das Modell auf den Trainingsdaten.\n",
                "# validation_data sorgt dafür, dass wir nach jeder Epoche auf den Val-Daten prüfen.\n",
                "\n",
                "history = model.fit(ds_train, validation_data=ds_val, epochs=EPOCHS,\n",
                "                    callbacks=cbs, verbose=1)\n",
                "\n",
                "# Den Verlauf (History) speichern wir als CSV, um Lernkurven zu plotten.\n",
                "pd.DataFrame(history.history).to_csv(RUN_DIR / \"history.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "724d38b3-fbf9-4be4-bb3b-e4fd2f0c6da8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test (keras) metrics: {\n",
                        "  \"loss\": 0.7150263786315918,\n",
                        "  \"auc\": 0.48298200964927673,\n",
                        "  \"auprc\": 0.4813368320465088,\n",
                        "  \"acc\": 0.481675386428833,\n",
                        "  \"prec\": 0.5,\n",
                        "  \"rec\": 0.34343433380126953\n",
                        "}\n",
                        "[Diag] thr@val(max MCC) = 0.445 | val_MCC=0.128\n",
                        "\n",
                        "[Diag] Confusion (test):\n",
                        " [[33 59]\n",
                        " [28 71]]\n",
                        "\n",
                        "[Diag] Report (test):\n",
                        "               precision    recall  f1-score   support\n",
                        "\n",
                        "           0      0.541     0.359     0.431        92\n",
                        "           1      0.546     0.717     0.620        99\n",
                        "\n",
                        "    accuracy                          0.545       191\n",
                        "   macro avg      0.544     0.538     0.526       191\n",
                        "weighted avg      0.544     0.545     0.529       191\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# === 9) TEST-EVALUATION (Diagnose) ===\n",
                "# Wir prüfen sofort, wie das Modell auf den ungesehenen Testdaten abschneidet.\n",
                "\n",
                "# Bewertung des Modells auf dem Test-Set mit den Keras-Metriken\n",
                "test_metrics = model.evaluate(ds_test, return_dict=True, verbose=0)\n",
                "# Ausgabe der Metriken als formatierter JSON-String\n",
                "print(\"Test (keras) metrics:\", json.dumps(test_metrics, indent=2))\n",
                "\n",
                "# Wir müssen einen Schwellwert (Threshold) wählen, ab dem wir \"Wahr\" sagen.\n",
                "# Standard ist 0.5, aber bei Finanzdaten ist oft ein anderer Wert besser.\n",
                "# Wir optimieren den Threshold auf den Validierungsdaten (maximiere MCC).\n",
                "\n",
                "# Vorhersage der Wahrscheinlichkeiten für Validation\n",
                "val_proba = model.predict(ds_val, verbose=0).ravel()\n",
                "\n",
                "# Funktion zur Suche des besten Schwellwerts\n",
                "def choose_threshold(y_true, y_prob, bounds=(0.35, 0.65)):\n",
                "    # Wir testen alle Wahrscheinlichkeiten als mögliche Thresholds (plus 0 und 1)\n",
                "    uniq = np.unique(y_prob); cand = np.r_[0.0, uniq, 1.0]\n",
                "    best_t, best_s = 0.5, -1\n",
                "    for t in cand:\n",
                "        # Simuliere Prediction mit diesem Threshold\n",
                "        yp = (y_prob >= t).astype(int)\n",
                "        # Check ob Positive Rate plausibel ist (innerhalb bounds, z.B. nicht alles 0 oder alles 1)\n",
                "        pr = yp.mean()\n",
                "        if not (bounds[0] <= pr <= bounds[1]): \n",
                "            continue\n",
                "        # Berechne MCC Score (Matthews Correlation Coefficient)\n",
                "        s = matthews_corrcoef(y_true, yp)\n",
                "        # Speichere, wenn besser als bisheriges Maximum\n",
                "        if s > best_s: best_s, best_t = s, float(t)\n",
                "    return best_t, best_s\n",
                "\n",
                "# Besten Threshold auf den VALIDATION-Daten finden\n",
                "thr_diag, mcc_val_diag = choose_threshold(yva, val_proba, bounds=(0.35, 0.65))\n",
                "print(f\"[Diag] thr@val(max MCC) = {thr_diag:.3f} | val_MCC={mcc_val_diag:.3f}\")\n",
                "\n",
                "# Mit diesem optimalen Threshold testen wir nun auf dem Test-Set\n",
                "y_proba_test = model.predict(ds_test, verbose=0).ravel()\n",
                "y_pred_diag = (y_proba_test >= thr_diag).astype(int)\n",
                "\n",
                "# Zusätzliche Metriken berechnen, die Keras nicht standardmäßig ausgibt\n",
                "extra = {\"balanced_accuracy\": float(balanced_accuracy_score(yte, y_pred_diag)),\n",
                "         \"mcc\": float(matthews_corrcoef(yte, y_pred_diag)),\n",
                "         \"auprc\": float(average_precision_score(yte, y_proba_test))}\n",
                "\n",
                "# Zusätzliche Metriken speichern\n",
                "with open(RUN_DIR / \"extra_test_metrics_diag.json\", \"w\") as f:\n",
                "    json.dump(extra, f, indent=2)\n",
                "\n",
                "# Confusion Matrix und Klassifikations-Report ausgeben\n",
                "print(\"\\n[Diag] Confusion (test):\\n\", confusion_matrix(yte, y_pred_diag))\n",
                "print(\"\\n[Diag] Report (test):\\n\", classification_report(yte, y_pred_diag, digits=3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "819bd95d-c17c-4736-9d00-4143c55fbfd8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Artefakte gespeichert in: ..\\results\\2026-01-07_21-29-55_lstm\n"
                    ]
                }
            ],
            "source": [
                "# === 10) SAVE & EXPORT (Artefakte sichern) ===\n",
                "# Wir speichern alle relevanten Informationen über den Lauf in JSON-Dateien.\n",
                "# Das ist essenziell für Nachvollziehbarkeit und spätere Auswertung.\n",
                "\n",
                "env_info = {\n",
                "    \"python\": sys.version,\n",
                "    \"tensorflow\": tf.__version__,\n",
                "    \"seed\": SEED,\n",
                "    \"ticker\": TICKER, \"start\": START, \"end\": END, \"interval\": INTERVAL,\n",
                "    \"horizon\": HORIZON, \"epsilon_mode\": EPS_MODE, \"epsilon\": EPSILON,\n",
                "    \"featureset\": FEATURESET, \"features_used\": FEATURES_USED_TAG,\n",
                "    \"features_final\": FEATURES,\n",
                "    \"lookback\": USE_LOOKBACK, \"batch\": BATCH, \"epochs\": EPOCHS,\n",
                "    \"cell\": CELL, \"width1\": WIDTH1, \"width2\": WIDTH2,\n",
                "    \"dropout_cfg\": DROPOUT, \"recurrent_dropout_used\": RDROP,\n",
                "    \"ln_layout\": ABL_LN_LAYOUT,\n",
                "    \"lr\": LR,\n",
                "    \"loss\": \"BCE\",\n",
                "    \"train_csv\": TRAIN_CSV,\n",
                "    \"features_yaml\": yaml_path,\n",
                "    \"best_config_path\": BEST_CFG_PATH,\n",
                "    \"best_checkpoint_path\": str(ckpt_path),\n",
                "}\n",
                "\n",
                "# Spezifische Umgebungsinfos speichern\n",
                "with open(RUN_DIR / \"env_info.json\", \"w\") as f:\n",
                "    json.dump(env_info, f, indent=2)\n",
                "\n",
                "# Eine kompakte Config für 'config.json' im Run-Ordner, für schnelle Übersicht\n",
                "final_cfg_dump = {\n",
                "    \"ticker\": TICKER, \"start\": START, \"end\": END, \"interval\": INTERVAL,\n",
                "    \"horizon\": HORIZON, \"lookback\": USE_LOOKBACK,\n",
                "    \"featureset\": FEATURESET, \"features\": FEATURES,\n",
                "    \"scaler\": \"StandardScaler\", \"seed\": SEED, \"batch\": BATCH, \"epochs\": EPOCHS,\n",
                "    \"cell\": CELL, \"width1\": WIDTH1, \"width2\": WIDTH2,\n",
                "    \"dropout\": DROPOUT, \"recurrent_dropout_used\": RDROP,\n",
                "    \"ln_layout\": ABL_LN_LAYOUT,\n",
                "    \"lr\": LR,\n",
                "    \"loss\": \"BCE\",\n",
                "    \"epsilon_mode\": EPS_MODE, \"epsilon\": EPSILON,\n",
                "    \"train_csv\": TRAIN_CSV,\n",
                "    \"features_yaml\": yaml_path,\n",
                "    \"wfcv_best_config_source\": BEST_CFG_PATH,\n",
                "    \"ablations\": {\n",
                "        \"shuffle_train\": ABL_SHUFFLE_TRAIN,\n",
                "        \"no_recurrent_dropout\": ABL_NO_RECURRENT_DROPOUT,\n",
                "        \"ln_layout\": ABL_LN_LAYOUT,\n",
                "        \"l2_dense\": L2_DENSE\n",
                "    }\n",
                "}\n",
                "\n",
                "# Config speichern\n",
                "with open(RUN_DIR / \"config.json\", \"w\") as f:\n",
                "    json.dump(final_cfg_dump, f, indent=2)\n",
                "\n",
                "# Das trainierte Modell speichern (als .keras Datei)\n",
                "model.save(RUN_DIR / \"model.keras\")\n",
                "\n",
                "# Wahrscheinlichkeiten und Labels speichern (für Backtest-Notebooks)\n",
                "# Damit können wir später Strategien simulieren, ohne neu trainieren zu müssen.\n",
                "np.save(RUN_DIR / \"y_test.npy\", yte)\n",
                "np.save(RUN_DIR / \"y_proba.npy\", y_proba_test)\n",
                "\n",
                "print(f\"\\nArtefakte gespeichert in: {RUN_DIR}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
